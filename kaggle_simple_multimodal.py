"""
Kaggleç®€åŒ–å¤šæ¨¡æ€æµ‹è¯•è„šæœ¬
ç”¨äºåœ¨Kaggleç¯å¢ƒä¸­ç›´æ¥å¤„ç†å·²ä¸Šä¼ çš„PDFå’Œå›¾ç‰‡æ–‡ä»¶
"""

import os
import sys
import subprocess
import time
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/kaggle/working/adaptive_RAG')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from document_processor import DocumentProcessor
from main import AdaptiveRAGSystem
from config import ENABLE_MULTIMODAL, SUPPORTED_IMAGE_FORMATS

def setup_kaggle_environment():
    """è®¾ç½®Kaggleç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®Kaggleç¯å¢ƒ...")
    
    # å®‰è£…å¿…è¦çš„ä¾èµ–
    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 
                   'PyPDF2', 'pdfplumber', 'Pillow'])
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

def process_uploaded_files(pdf_path: str = None, image_paths: List[str] = None):
    """
    å¤„ç†å·²ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå‘é‡åŒ–å¹¶æŒä¹…åŒ–åˆ°é¡¹ç›®ç›®å½•
    æ”¯æŒæ–‡ä»¶å»é‡ï¼Œé¿å…é‡å¤å¤„ç†
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
    """
    import hashlib
    import json
    
    # è®¾ç½®å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    persist_dir = os.path.join(current_dir, 'chroma_db')
    metadata_file = os.path.join(current_dir, 'document_metadata.json')
    os.makedirs(persist_dir, exist_ok=True)
    
    print(f"ğŸ’¾ å‘é‡æ•°æ®åº“æŒä¹…åŒ–ç›®å½•: {persist_dir}")
    
    # åŠ è½½å·²å¤„ç†æ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆç”¨äºå»é‡ï¼‰
    processed_files = {}
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                processed_files = metadata.get('processed_files', {})
                print(f"ğŸ“Š å·²åŠ è½½å…ƒæ•°æ®ï¼Œå‘ç° {len(processed_files)} ä¸ªå·²å¤„ç†çš„æ–‡ä»¶")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
    
    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆç”¨äºå»é‡æ£€æµ‹ï¼‰
    def get_file_hash(file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        if not os.path.exists(file_path):
            return None
        try:
            with open(file_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            return file_hash
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
            return None
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‘é‡æ•°æ®åº“
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        print("âœ… æ£€æµ‹åˆ°å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ï¼ŒåŠ è½½ä¸­...")
        try:
            # åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from langchain_community.vectorstores import Chroma
            from config import EMBEDDING_MODEL, COLLECTION_NAME
            
            embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'}
            )
            
            vectorstore = Chroma(
                persist_directory=persist_dir,
                embedding_function=embeddings,
                collection_name=COLLECTION_NAME
            )
            
            retriever = vectorstore.as_retriever()
            print(f"âœ… å·²åŠ è½½æŒä¹…åŒ–çš„å‘é‡æ•°æ®åº“ï¼Œå…± {vectorstore._collection.count()} ä¸ªæ–‡æ¡£å—")
            
            # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
            doc_processor = DocumentProcessor()
            
            # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦éœ€è¦å¤„ç†
            if pdf_path and os.path.exists(pdf_path):
                file_hash = get_file_hash(pdf_path)
                if file_hash and file_hash in processed_files:
                    print(f"â­ï¸  PDFæ–‡ä»¶å·²å¤„ç†è¿‡ï¼ˆ{pdf_path}ï¼‰ï¼Œè·³è¿‡")
                else:
                    print(f"ğŸ†• æ£€æµ‹åˆ°æ–°PDFæ–‡ä»¶ï¼Œæ­£åœ¨æ·»åŠ : {pdf_path}")
                    try:
                        from langchain_community.document_loaders import PyPDFLoader
                        loader = PyPDFLoader(pdf_path)
                        docs = loader.load()
                        doc_splits = doc_processor.split_documents(docs)
                        
                        # æ·»åŠ åˆ°ç°æœ‰å‘é‡æ•°æ®åº“
                        vectorstore.add_documents(doc_splits)
                        print(f"âœ… å·²æ·»åŠ  {len(doc_splits)} ä¸ªæ–°æ–‡æ¡£å—")
                        
                        # æ›´æ–°å…ƒæ•°æ®
                        if file_hash:
                            processed_files[file_hash] = {
                                'path': pdf_path,
                                'type': 'pdf',
                                'chunks': len(doc_splits),
                                'processed_at': time.time()
                            }
                            with open(metadata_file, 'w', encoding='utf-8') as f:
                                json.dump({'processed_files': processed_files}, f, ensure_ascii=False, indent=2)
                            print(f"ğŸ’¾ å…ƒæ•°æ®å·²æ›´æ–°")
                    except Exception as e:
                        print(f"âš ï¸  æ·»åŠ æ–°PDFå¤±è´¥: {e}")
            
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}ï¼Œå°†é‡æ–°åˆ›å»º")
            vectorstore, retriever, doc_processor = None, None, None
    else:
        vectorstore, retriever, doc_processor = None, None, None
    
    # å¦‚æœæ²¡æœ‰åŠ è½½æˆåŠŸï¼Œåˆ™åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
    if vectorstore is None:
        print("ğŸ”§ æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        doc_processor = DocumentProcessor()
        
        # å¤„ç†PDFæ–‡ä»¶
        if pdf_path and os.path.exists(pdf_path):
            print(f"ğŸ“„ å¤„ç†PDFæ–‡ä»¶: {pdf_path}")
            try:
                from langchain_community.document_loaders import PyPDFLoader
                loader = PyPDFLoader(pdf_path)
                docs = loader.load()
                
                # åˆ†å‰²æ–‡æ¡£
                doc_splits = doc_processor.split_documents(docs)
                
                # åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆå¸¦æŒä¹…åŒ–ï¼‰
                from langchain_community.embeddings import HuggingFaceEmbeddings
                from langchain_community.vectorstores import Chroma
                from config import EMBEDDING_MODEL, COLLECTION_NAME
                
                embeddings = HuggingFaceEmbeddings(
                    model_name=EMBEDDING_MODEL,
                    model_kwargs={'device': 'cpu'}
                )
                
                vectorstore = Chroma.from_documents(
                    documents=doc_splits,
                    embedding=embeddings,
                    collection_name=COLLECTION_NAME,
                    persist_directory=persist_dir  # æŒä¹…åŒ–ç›®å½•
                )
                
                retriever = vectorstore.as_retriever()
                
                print(f"âœ… PDFå¤„ç†å®Œæˆï¼Œå…± {len(doc_splits)} ä¸ªæ–‡æ¡£å—")
                print(f"ğŸ’¾ å‘é‡æ•°æ®åº“å·²æŒä¹…åŒ–åˆ°: {persist_dir}")
                
                # ä¿å­˜å…ƒæ•°æ®
                file_hash = get_file_hash(pdf_path)
                if file_hash:
                    processed_files[file_hash] = {
                        'path': pdf_path,
                        'type': 'pdf',
                        'chunks': len(doc_splits),
                        'processed_at': time.time()
                    }
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump({'processed_files': processed_files}, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜")
                
            except Exception as e:
                print(f"âŒ PDFå¤„ç†å¤±è´¥: {e}")
                return None, None
        else:
            # ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“
            print("ğŸ“„ ä½¿ç”¨é»˜è®¤çŸ¥è¯†åº“...")
            try:
                vectorstore, retriever, doc_splits = doc_processor.setup_knowledge_base()
                
                # å°†é»˜è®¤çŸ¥è¯†åº“ä¹ŸæŒä¹…åŒ–
                if vectorstore and hasattr(vectorstore, '_persist_directory'):
                    vectorstore._persist_directory = persist_dir
                    print(f"ğŸ’¾ é»˜è®¤çŸ¥è¯†åº“å·²æŒä¹…åŒ–åˆ°: {persist_dir}")
                    
            except Exception as e:
                print(f"âŒ é»˜è®¤çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {e}")
                return None, None
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–è‡ªé€‚åº”RAGç³»ç»Ÿ...")
    rag_system = AdaptiveRAGSystem()
    
    # æ›´æ–°RAGç³»ç»Ÿçš„æ£€ç´¢å™¨
    rag_system.retriever = retriever
    rag_system.doc_processor = doc_processor
    rag_system.workflow_nodes.retriever = retriever
    rag_system.workflow_nodes.doc_processor = doc_processor
    
    return rag_system, doc_processor

def query_with_multimodal(rag_system: AdaptiveRAGSystem, query: str, image_paths: List[str] = None):
    """
    æ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢
    
    Args:
        rag_system: RAGç³»ç»Ÿå®ä¾‹
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
    """
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    
    try:
        # æ‰§è¡ŒæŸ¥è¯¢
        result = rag_system.query(query)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ¯ ç­”æ¡ˆ:")
        print(result['answer'])
        
        # æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
        if result.get('retrieval_metrics'):
            metrics = result['retrieval_metrics']
            print("\nğŸ“Š æ£€ç´¢è¯„ä¼°:")
            print(f"   - æ£€ç´¢è€—æ—¶: {metrics.get('latency', 0):.4f}ç§’")
            print(f"   - æ£€ç´¢æ–‡æ¡£æ•°: {metrics.get('retrieved_docs_count', 0)}")
            print(f"   - Precision@3: {metrics.get('precision_at_3', 0):.4f}")
            print(f"   - Recall@3: {metrics.get('recall_at_3', 0):.4f}")
            print(f"   - MAP: {metrics.get('map_score', 0):.4f}")
        
        return result
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        return None

def scan_and_copy_files():
    """æ‰«æ /kaggle/input/ å¹¶å¤åˆ¶æ–‡ä»¶åˆ° /kaggle/working/"""
    import shutil
    
    input_dir = '/kaggle/input'
    working_dir = '/kaggle/working'
    
    if not os.path.exists(input_dir):
        print("âš ï¸  /kaggle/input/ ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æ–‡ä»¶æ‰«æ")
        return
    
    print("ğŸ“‚ æ‰«æ /kaggle/input/ ç›®å½•...")
    
    copied_pdfs = []
    copied_images = []
    
    # é€’å½’æ‰«ææ‰€æœ‰æ–‡ä»¶
    for root, dirs, files in os.walk(input_dir):
        for file in files:
            # è·³è¿‡éšè—æ–‡ä»¶å’Œç©ºæ–‡ä»¶å
            if not file or file.startswith('.'):
                continue
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰æ–‡ä»¶
            print(f"   ğŸ” æ‰«æåˆ°: {file}")
                
            src = os.path.join(root, file)
            dst = os.path.join(working_dir, file)
            
            try:
                # ä¿®å¤ï¼šä½¿ç”¨å°å†™æ¯”è¾ƒï¼Œæ”¯æŒ .pdf, .PDF, .Pdf ç­‰
                if file.lower().endswith('.pdf'):
                    shutil.copy(src, dst)
                    copied_pdfs.append(file)
                    print(f"   âœ… å¤åˆ¶ PDF: {file}")
                elif any(file.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']):
                    shutil.copy(src, dst)
                    copied_images.append(file)
                    print(f"   âœ… å¤åˆ¶å›¾ç‰‡: {file}")
                else:
                    print(f"   âšª è·³è¿‡éç›®æ ‡æ–‡ä»¶: {file}")
            except Exception as e:
                print(f"   âš ï¸  å¤åˆ¶æ–‡ä»¶å¤±è´¥ {file}: {e}")
    
    if copied_pdfs or copied_images:
        print(f"\nğŸ“ å¤åˆ¶å®Œæˆ: {len(copied_pdfs)} ä¸ª PDF, {len(copied_images)} å¼ å›¾ç‰‡")
    else:
        print("âš ï¸  æœªæ‰¾åˆ° PDF æˆ–å›¾ç‰‡æ–‡ä»¶")
        print("\nğŸ” è¯·æ£€æŸ¥:")
        print("   1. æ–‡ä»¶æ˜¯å¦å·²ä¸Šä¼ åˆ° Kaggle")
        print("   2. æ–‡ä»¶æ˜¯å¦åœ¨ /kaggle/input/ ç›®å½•ä¸‹")
        print("   3. æ–‡ä»¶æ‰©å±•åæ˜¯å¦æ­£ç¡® (.pdf, .jpg, .png ç­‰)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Kaggleç®€åŒ–å¤šæ¨¡æ€æµ‹è¯•")
    print("="*50)
    
    # è®¾ç½®ç¯å¢ƒ
    setup_kaggle_environment()
    
    # ä» /kaggle/input/ å¤åˆ¶æ–‡ä»¶åˆ° /kaggle/working/
    scan_and_copy_files()
    
    # æ£€æŸ¥æ–‡ä»¶
    working_dir = '/kaggle/working'
    
    # è¿‡æ»¤æœ‰æ•ˆçš„PDFæ–‡ä»¶ï¼ˆæ’é™¤éšè—æ–‡ä»¶ï¼‰
    try:
        all_files = os.listdir(working_dir)
        
        # ä¿®å¤ï¼šç§»é™¤æ–‡ä»¶åé•¿åº¦é™åˆ¶ï¼Œæ”¯æŒ .pdf ç­‰çŸ­æ–‡ä»¶å
        pdf_files = [
            f for f in all_files 
            if f.lower().endswith('.pdf')  # å°å†™æ¯”è¾ƒ
            and not f.startswith('.')  # æ’é™¤éšè—æ–‡ä»¶
            and os.path.isfile(os.path.join(working_dir, f))  # ç¡®ä¿æ˜¯æ–‡ä»¶
        ]
        image_files = [
            f for f in all_files 
            if any(f.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp'])
            and not f.startswith('.')  # æ’é™¤éšè—æ–‡ä»¶
            and os.path.isfile(os.path.join(working_dir, f))  # ç¡®ä¿æ˜¯æ–‡ä»¶
        ]
    except Exception as e:
        print(f"âŒ æ‰«ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
        pdf_files = []
        image_files = []
        all_files = []
    
    print(f"\nğŸ“ /kaggle/working/ ä¸­çš„æ–‡ä»¶:")
    
    # è°ƒè¯•ï¼šè¯¦ç»†æ˜¾ç¤ºæ‰€æœ‰æ–‡ä»¶å’Œè¿‡æ»¤è¿‡ç¨‹
    print("\nğŸ” è¯¦ç»†è°ƒè¯•ä¿¡æ¯:")
    print(f"   ç›®å½•ä¸­æ€»å…± {len(all_files)} ä¸ªé¡¹ç›®")
    for f in all_files:
        f_path = os.path.join(working_dir, f)
        is_file = os.path.isfile(f_path)
        is_dir = os.path.isdir(f_path)
        f_lower = f.lower()
        
        # æ£€æŸ¥ PDF
        if f_lower.endswith('.pdf'):
            file_size = os.path.getsize(f_path) if is_file else 0
            print(f"   ğŸ“„ {f}: æ˜¯æ–‡ä»¶={is_file}, å¤§å°={file_size/1024:.1f}KB, é•¿åº¦={len(f)}")
        # æ£€æŸ¥å›¾ç‰‡
        elif any(f_lower.endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']):
            file_size = os.path.getsize(f_path) if is_file else 0
            print(f"   ğŸ–¼ï¸ {f}: æ˜¯æ–‡ä»¶={is_file}, å¤§å°={file_size/1024:.1f}KB")
        else:
            print(f"   âšª {f}: ç±»å‹={'[ç›®å½•]' if is_dir else '[æ–‡ä»¶]'}")
    
    print(f"\nğŸ“Š è¿‡æ»¤ç»“æœ:")
    print(f"   - PDFæ–‡ä»¶: {len(pdf_files)} ä¸ª")
    for pdf in pdf_files:
        pdf_path = os.path.join(working_dir, pdf)
        file_size = os.path.getsize(pdf_path) if os.path.exists(pdf_path) else 0
        print(f"     * {pdf} ({file_size/1024:.1f} KB)")
    
    print(f"   - å›¾ç‰‡æ–‡ä»¶: {len(image_files)} ä¸ª")
    for img in image_files:
        img_path = os.path.join(working_dir, img)
        file_size = os.path.getsize(img_path) if os.path.exists(img_path) else 0
        print(f"     * {img} ({file_size/1024:.1f} KB)")
    
    if not pdf_files and not image_files:
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("   1. åœ¨ Kaggle Notebook å³ä¾§ç‚¹å‡» '+ Add data'")
        print("   2. é€‰æ‹© 'Upload' æ ‡ç­¾")
        print("   3. ä¸Šä¼ ä½ çš„ PDF å’Œå›¾ç‰‡æ–‡ä»¶")
        print("   4. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        print("\nğŸ” å½“å‰ç›®å½•å†…å®¹:")
        try:
            print(f"   {os.listdir(working_dir)}")
        except:
            pass
        return
    
    # å¤„ç†æ–‡ä»¶ï¼ˆæ·»åŠ è·¯å¾„éªŒè¯ï¼‰
    if pdf_files:
        pdf_path = os.path.join(working_dir, pdf_files[0])
        if not os.path.exists(pdf_path) or not os.path.isfile(pdf_path):
            print(f"âŒ PDF æ–‡ä»¶è·¯å¾„æ— æ•ˆ: {pdf_path}")
            pdf_path = None
    else:
        pdf_path = None
    
    if image_files:
        image_paths = []
        for img in image_files:
            img_path = os.path.join(working_dir, img)
            if os.path.exists(img_path) and os.path.isfile(img_path):
                image_paths.append(img_path)
        image_paths = image_paths if image_paths else None
    else:
        image_paths = None
    
    rag_system, doc_processor = process_uploaded_files(pdf_path, image_paths)
    
    if not rag_system:
        print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return
    
    # ç¤ºä¾‹æŸ¥è¯¢
    print("\n" + "="*50)
    print("ğŸ§ª ç¤ºä¾‹æŸ¥è¯¢æµ‹è¯•")
    print("="*50)
    
    # æ–‡æœ¬æŸ¥è¯¢ç¤ºä¾‹
    query1 = "è¯·æ€»ç»“æ–‡æ¡£çš„ä¸»è¦å†…å®¹"
    query_with_multimodal(rag_system, query1, image_paths)
    
    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢
    if image_paths and ENABLE_MULTIMODAL:
        print("\n" + "="*50)
        print("ğŸ–¼ï¸ å¤šæ¨¡æ€æŸ¥è¯¢æµ‹è¯•")
        print("="*50)
        
        query2 = "è¯·ç»“åˆå›¾ç‰‡å†…å®¹ï¼Œè§£é‡Šæ–‡æ¡£ä¸­çš„ç›¸å…³æ¦‚å¿µ"
        query_with_multimodal(rag_system, query2, image_paths)
    
    print("\n" + "="*50)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*50)
    print("\nğŸ’¡ æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œè‡ªå®šä¹‰æŸ¥è¯¢:")
    print("```python")
    print("# è‡ªå®šä¹‰æŸ¥è¯¢")
    print("custom_query = 'æ‚¨çš„é—®é¢˜'")
    print("query_with_multimodal(rag_system, custom_query, image_paths)")
    print("```")

if __name__ == "__main__":
    main()