"""
åœ¨ Colab ä¸­ä»è¶…æ—¶å¤„ç»§ç»­å¤„ç†çš„å®Œæ•´è„šæœ¬
ç›´æ¥å¤åˆ¶åˆ° Colab ä»£ç å•å…ƒæ ¼è¿è¡Œ
"""

print("ğŸš€ GraphRAG è¶…æ—¶æ¢å¤è„šæœ¬")
print("="*60)

# ==================== æ­¥éª¤ 0: æ£€æŸ¥å‰ç½®æ¡ä»¶ ====================
print("\nğŸ“‹ æ­¥éª¤ 0: æ£€æŸ¥å‰ç½®æ¡ä»¶...")

import sys
import os

# æŒ‚è½½ Google Driveï¼ˆå¦‚æœè¿˜æ²¡æœ‰æŒ‚è½½ï¼‰
try:
    from google.colab import drive
    if not os.path.exists('/content/drive'):
        print("   æŒ‚è½½ Google Drive...")
        drive.mount('/content/drive')
    else:
        print("   âœ… Google Drive å·²æŒ‚è½½")
except:
    print("   âš ï¸ ä¸åœ¨ Colab ç¯å¢ƒä¸­")

# è®¾ç½®è·¯å¾„
project_path = '/content/drive/MyDrive/adaptive_RAG'
sys.path.insert(0, project_path)

print(f"   é¡¹ç›®è·¯å¾„: {project_path}")

# ==================== æ­¥éª¤ 1: é‡å¯ Ollama ====================
print("\nğŸ”„ æ­¥éª¤ 1: é‡å¯ Ollama æœåŠ¡...")

import subprocess
import time

# æ€æ‰æ—§è¿›ç¨‹
!pkill -9 ollama 2>/dev/null

time.sleep(2)

# å¯åŠ¨æ–°è¿›ç¨‹
print("   å¯åŠ¨ Ollama æœåŠ¡...")
ollama_process = subprocess.Popen(
    ["ollama", "serve"],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    preexec_fn=os.setpgrp
)

time.sleep(5)

# éªŒè¯æœåŠ¡
import requests
try:
    response = requests.get('http://localhost:11434/api/tags', timeout=5)
    if response.status_code == 200:
        print("   âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
    else:
        print(f"   âš ï¸ Ollama å“åº”å¼‚å¸¸: {response.status_code}")
except Exception as e:
    print(f"   âŒ Ollama æœåŠ¡æœªå“åº”: {e}")
    print("   è¯·æ£€æŸ¥ Ollama æ˜¯å¦æ­£ç¡®å®‰è£…")

# ==================== æ­¥éª¤ 2: åŠ è½½é…ç½®å’Œæ–‡æ¡£ ====================
print("\nğŸ“š æ­¥éª¤ 2: åŠ è½½é…ç½®å’Œæ–‡æ¡£...")

# å¯¼å…¥é…ç½®
from config import setup_environment

try:
    setup_environment()
    print("   âœ… ç¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ")
except Exception as e:
    print(f"   âš ï¸ ç¯å¢ƒé…ç½®è­¦å‘Š: {e}")

# æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ doc_splits å˜é‡
if 'doc_splits' in dir():
    print(f"   âœ… æ£€æµ‹åˆ°å·²æœ‰ doc_splits: {len(doc_splits)} ä¸ªæ–‡æ¡£")
    use_existing_docs = True
else:
    print("   âš ï¸ æœªæ£€æµ‹åˆ° doc_splitsï¼Œéœ€è¦é‡æ–°åŠ è½½æ–‡æ¡£")
    use_existing_docs = False

# å¦‚æœæ²¡æœ‰ doc_splitsï¼Œé‡æ–°åŠ è½½
if not use_existing_docs:
    print("\n   æ­£åœ¨åŠ è½½æ–‡æ¡£...")
    from document_processor import DocumentProcessor
    
    doc_processor = DocumentProcessor()
    
    # ä½¿ç”¨é»˜è®¤ URL æˆ–è‡ªå®šä¹‰ URL
    urls = [
        "https://lilianweng.github.io/posts/2023-06-23-agent/",
        "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
        "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"
    ]
    
    vectorstore, retriever, doc_splits = doc_processor.setup_knowledge_base(
        urls=urls,
        enable_graphrag=True
    )
    
    print(f"   âœ… æ–‡æ¡£åŠ è½½å®Œæˆ: {len(doc_splits)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

# ==================== æ­¥éª¤ 3: ä¿®å¤è¶…æ—¶é…ç½® ====================
print("\nâš™ï¸ æ­¥éª¤ 3: ä¿®å¤è¶…æ—¶é…ç½®...")

# æ–¹æ¡ˆï¼šç›´æ¥ä¿®æ”¹ entity_extractor.py æ–‡ä»¶å†…å®¹
entity_extractor_path = os.path.join(project_path, 'entity_extractor.py')

# è¯»å–åŸæ–‡ä»¶
with open(entity_extractor_path, 'r', encoding='utf-8') as f:
    content = f.read()

# æ£€æŸ¥æ˜¯å¦å·²ç»ä¿®æ”¹è¿‡
if 'timeout: int = 180' in content:
    print("   âœ… entity_extractor.py å·²ç»åŒ…å«è¶…æ—¶ä¿®å¤")
else:
    print("   ğŸ“ ä¿®æ”¹ entity_extractor.py...")
    
    # æ›¿æ¢åˆå§‹åŒ–æ–¹æ³•çš„ç­¾å
    content = content.replace(
        'def __init__(self, timeout: int = 60, max_retries: int = 3):',
        'def __init__(self, timeout: int = 180, max_retries: int = 5):'
    )
    
    # ä¿å­˜ä¿®æ”¹
    with open(entity_extractor_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("   âœ… å·²å°†é»˜è®¤è¶…æ—¶æ—¶é—´æ”¹ä¸º 180 ç§’ï¼Œé‡è¯•æ¬¡æ•°æ”¹ä¸º 5 æ¬¡")

# é‡æ–°åŠ è½½æ¨¡å—
import importlib

if 'entity_extractor' in sys.modules:
    importlib.reload(sys.modules['entity_extractor'])
    print("   ğŸ”„ entity_extractor æ¨¡å—å·²é‡æ–°åŠ è½½")

if 'graph_indexer' in sys.modules:
    importlib.reload(sys.modules['graph_indexer'])
    print("   ğŸ”„ graph_indexer æ¨¡å—å·²é‡æ–°åŠ è½½")

# ==================== æ­¥éª¤ 4: ç¡®å®šç»§ç»­å¤„ç†çš„èµ·ç‚¹ ====================
print("\nğŸ“Š æ­¥éª¤ 4: ç¡®å®šå¤„ç†èµ·ç‚¹...")

# è®©ç”¨æˆ·é€‰æ‹©ä»å“ªé‡Œå¼€å§‹
print("\nè¯·é€‰æ‹©ç»§ç»­å¤„ç†çš„æ–¹å¼:")
print("  1. ä»æ–‡æ¡£ #56 é‡æ–°å¼€å§‹ï¼ˆåŒ…å« #56ï¼‰")
print("  2. è·³è¿‡æ–‡æ¡£ #56ï¼Œä» #57 å¼€å§‹")
print("  3. ä»å¤´å¼€å§‹å¤„ç†æ‰€æœ‰æ–‡æ¡£")
print("  4. è‡ªå®šä¹‰èµ·å§‹ä½ç½®")

# é»˜è®¤é€‰é¡¹ï¼ˆå¯ä»¥ä¿®æ”¹ï¼‰
choice = 1  # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œæ¥é€‰æ‹©ä¸åŒçš„é€‰é¡¹

if choice == 1:
    start_index = 55  # æ–‡æ¡£ #56 çš„ç´¢å¼•
    print(f"\n   âœ… é€‰æ‹©: ä»æ–‡æ¡£ #56 å¼€å§‹ï¼ˆç´¢å¼• {start_index}ï¼‰")
elif choice == 2:
    start_index = 56  # è·³è¿‡ #56
    print(f"\n   âœ… é€‰æ‹©: è·³è¿‡æ–‡æ¡£ #56ï¼Œä» #57 å¼€å§‹ï¼ˆç´¢å¼• {start_index}ï¼‰")
elif choice == 3:
    start_index = 0
    print(f"\n   âœ… é€‰æ‹©: ä»å¤´å¼€å§‹å¤„ç†æ‰€æœ‰æ–‡æ¡£")
else:
    # è‡ªå®šä¹‰
    start_index = 55  # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œæ¥è‡ªå®šä¹‰èµ·å§‹ä½ç½®
    print(f"\n   âœ… é€‰æ‹©: è‡ªå®šä¹‰èµ·å§‹ä½ç½®ï¼ˆç´¢å¼• {start_index}ï¼‰")

remaining_docs = doc_splits[start_index:]
print(f"   å¾…å¤„ç†æ–‡æ¡£æ•°: {len(remaining_docs)} ä¸ª")

# ==================== æ­¥éª¤ 5: å¼€å§‹å¤„ç† ====================
print("\nğŸš€ æ­¥éª¤ 5: å¼€å§‹å¤„ç†æ–‡æ¡£...")
print("="*60)

from graph_indexer import GraphRAGIndexer

# åˆ›å»ºç´¢å¼•å™¨
indexer = GraphRAGIndexer()

# å¼€å§‹ç´¢å¼•
try:
    graph = indexer.index_documents(
        documents=remaining_docs,
        batch_size=3,  # ğŸ‘ˆ å¯ä»¥è°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼ˆ1-5 æ¨èï¼‰
        save_path=os.path.join(project_path, "knowledge_graph_recovered.pkl")
    )
    
    print("\n" + "="*60)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print("="*60)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = graph.get_statistics()
    print(f"\nğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
    print(f"   â€¢ èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
    print(f"   â€¢ è¾¹æ•°: {stats['num_edges']}")
    print(f"   â€¢ ç¤¾åŒºæ•°: {stats['num_communities']}")
    print(f"   â€¢ å›¾å¯†åº¦: {stats['density']:.4f}")
    
except KeyboardInterrupt:
    print("\nâš ï¸ å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")
    print("   å¯ä»¥è®°å½•å½“å‰è¿›åº¦ï¼Œç¨åç»§ç»­")
    
except Exception as e:
    print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:")
    print(f"   {type(e).__name__}: {e}")
    print("\nå»ºè®®:")
    print("   1. æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
    print("   2. å¦‚æœæ˜¯æŸä¸ªæ–‡æ¡£è¶…æ—¶ï¼Œå°è¯•è·³è¿‡å®ƒ")
    print("   3. å¦‚æœæ˜¯ Ollama é—®é¢˜ï¼Œé‡å¯æœåŠ¡")
    
    import traceback
    print("\nå®Œæ•´é”™è¯¯å †æ ˆ:")
    traceback.print_exc()

# ==================== å®Œæˆ ====================
print("\n" + "="*60)
print("è„šæœ¬æ‰§è¡Œå®Œæˆ")
print("="*60)
print("\nğŸ’¡ æç¤º:")
print("   â€¢ å¦‚æœé‡åˆ°è¶…æ—¶ï¼Œæ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
print("   â€¢ å¯ä»¥ä¿®æ”¹ choice å˜é‡æ¥è·³è¿‡é—®é¢˜æ–‡æ¡£")
print("   â€¢ å¯ä»¥ä¿®æ”¹ batch_size æ¥è°ƒæ•´å¤„ç†é€Ÿåº¦")
print("   â€¢ å›¾è°±å·²ä¿å­˜åˆ°: knowledge_graph_recovered.pkl")
