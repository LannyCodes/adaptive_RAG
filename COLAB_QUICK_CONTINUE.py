"""
Colab å¿«é€Ÿç»§ç»­è„šæœ¬ - ä»è¶…æ—¶å¤„æ¢å¤
å¤åˆ¶åˆ° Colab è¿è¡Œï¼Œä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ç»§ç»­å¤„ç†
"""

print("ğŸš€ GraphRAG æ¢å¤è„šæœ¬ v2.0")
print("="*60)

import sys
import os

# ==================== 1. è®¾ç½®ç¯å¢ƒ ====================
print("\n1ï¸âƒ£ è®¾ç½®ç¯å¢ƒ...")

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_path = '/content/drive/MyDrive/adaptive_RAG'
if project_path not in sys.path:
    sys.path.insert(0, project_path)
print(f"   âœ… é¡¹ç›®è·¯å¾„: {project_path}")

# ==================== 2. é‡å¯ Ollama ====================
print("\n2ï¸âƒ£ é‡å¯ Ollama...")

import subprocess
import time

subprocess.run(['pkill', '-9', 'ollama'], stderr=subprocess.DEVNULL)
time.sleep(2)

ollama_process = subprocess.Popen(
    ["ollama", "serve"],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)
time.sleep(5)

import requests
try:
    r = requests.get('http://localhost:11434/api/tags', timeout=5)
    print(f"   âœ… Ollama è¿è¡Œæ­£å¸¸" if r.status_code == 200 else f"   âš ï¸ çŠ¶æ€ç : {r.status_code}")
except:
    print("   âŒ Ollama æœªå“åº”")

# ==================== 3. åŠ è½½æ–‡æ¡£ ====================
print("\n3ï¸âƒ£ åŠ è½½æ–‡æ¡£...")

from config import setup_environment
from document_processor import DocumentProcessor

setup_environment()

# åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
doc_processor = DocumentProcessor()

# åŠ è½½æ–‡æ¡£ï¼ˆä½¿ç”¨é»˜è®¤ URLsï¼‰
vectorstore, retriever, doc_splits = doc_processor.setup_knowledge_base(
    enable_graphrag=True
)

print(f"   âœ… å·²åŠ è½½ {len(doc_splits)} ä¸ªæ–‡æ¡£")

# ==================== 4. ä¿®æ”¹è¶…æ—¶é…ç½® ====================
print("\n4ï¸âƒ£ å¢åŠ è¶…æ—¶æ—¶é—´...")

entity_file = os.path.join(project_path, 'entity_extractor.py')
with open(entity_file, 'r', encoding='utf-8') as f:
    content = f.read()

# ä¿®æ”¹é»˜è®¤å‚æ•°
if 'timeout: int = 60' in content:
    content = content.replace(
        'timeout: int = 60, max_retries: int = 3',
        'timeout: int = 180, max_retries: int = 5'
    )
    with open(entity_file, 'w', encoding='utf-8') as f:
        f.write(content)
    print("   âœ… è¶…æ—¶å·²æ”¹ä¸º 180 ç§’ï¼Œé‡è¯•æ”¹ä¸º 5 æ¬¡")
else:
    print("   â„¹ï¸ å·²ç»æ˜¯ä¿®æ”¹åçš„é…ç½®")

# é‡æ–°åŠ è½½æ¨¡å—
import importlib
for mod in ['entity_extractor', 'graph_indexer']:
    if mod in sys.modules:
        importlib.reload(sys.modules[mod])

# ==================== 5. ç»§ç»­å¤„ç† ====================
print("\n5ï¸âƒ£ ç»§ç»­å¤„ç†æ–‡æ¡£...")
print("="*60)

from graph_indexer import GraphRAGIndexer

# é…ç½®èµ·å§‹ä½ç½®
START_INDEX = 55  # ğŸ‘ˆ ä»æ–‡æ¡£ #56 å¼€å§‹ï¼Œä¿®æ”¹è¿™é‡Œå¯ä»¥è·³è¿‡æŸäº›æ–‡æ¡£
BATCH_SIZE = 3    # ğŸ‘ˆ æ‰¹æ¬¡å¤§å°ï¼Œå¯ä»¥æ”¹ä¸º 1-5

print(f"\n   èµ·å§‹ä½ç½®: æ–‡æ¡£ #{START_INDEX + 1}")
print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
print(f"   å¾…å¤„ç†: {len(doc_splits) - START_INDEX} ä¸ªæ–‡æ¡£\n")

remaining_docs = doc_splits[START_INDEX:]

indexer = GraphRAGIndexer()

try:
    graph = indexer.index_documents(
        documents=remaining_docs,
        batch_size=BATCH_SIZE,
        save_path=f"{project_path}/knowledge_graph_recovered.pkl"
    )
    
    print("\nâœ… å¤„ç†å®Œæˆï¼")
    stats = graph.get_statistics()
    print(f"ğŸ“Š èŠ‚ç‚¹: {stats['num_nodes']}, è¾¹: {stats['num_edges']}, ç¤¾åŒº: {stats['num_communities']}")
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    print("\nå»ºè®®:")
    print("   â€¢ å¦‚æœæ–‡æ¡£ #56 è¶…æ—¶ï¼Œä¿®æ”¹ START_INDEX = 56 è·³è¿‡å®ƒ")
    print("   â€¢ å¦‚æœ Ollama å´©æºƒï¼Œé‡æ–°è¿è¡Œæ­¤è„šæœ¬")
    print("   â€¢ å‡å° BATCH_SIZE åˆ° 1 æˆ– 2")
