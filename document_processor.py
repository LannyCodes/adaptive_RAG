"""
æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–æ¨¡å—
è´Ÿè´£æ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–å’Œå‘é‡æ•°æ®åº“åˆå§‹åŒ–
"""

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.document_loaders import WebBaseLoader

# å°è¯•å¯¼å…¥ langchain_milvusï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ° langchain_community å¹¶åº”ç”¨è¡¥ä¸
try:
    from langchain_milvus import MilvusVectorStore as Milvus
    print("âœ… ä½¿ç”¨ langchain-milvus (æ–°ç‰ˆ)")
except ImportError:
    try:
        from langchain_community.vectorstores import Milvus
        print("âš ï¸ ä½¿ç”¨ langchain_community.vectorstores.Milvus (æ—§ç‰ˆ)")
        
        # Monkeypatch: ä¿®å¤æ—§ç‰ˆ LangChain å¯¹ Milvus Lite æœ¬åœ°æ–‡ä»¶è·¯å¾„çš„æ ¡éªŒé—®é¢˜
        # æ—§ç‰ˆ _create_connection_alias å¼ºåˆ¶è¦æ±‚ URI ä»¥ http/https å¼€å¤´
        def _patched_create_connection_alias(self, connection_args):
            uri = connection_args.get("uri")
            # ä¸ºæœ¬åœ°æ–‡ä»¶ç”Ÿæˆå”¯ä¸€çš„ alias
            if uri:
                import hashlib
                return hashlib.md5(uri.encode()).hexdigest()
            return "default"
            
        # åº”ç”¨è¡¥ä¸
        Milvus._create_connection_alias = _patched_create_connection_alias
        print("ğŸ”§ å·²åº”ç”¨ Milvus Lite è·¯å¾„æ ¡éªŒè¡¥ä¸")
    except ImportError:
        pass

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever

from config import (
    KNOWLEDGE_BASE_URLS,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    COLLECTION_NAME,
    EMBEDDING_MODEL,
    # æ··åˆæ£€ç´¢é…ç½®
    ENABLE_HYBRID_SEARCH,
    HYBRID_SEARCH_WEIGHTS,
    KEYWORD_SEARCH_K,
    BM25_K1,
    BM25_B,
    # å‘é‡åº“é…ç½®
    VECTOR_STORE_TYPE,
    MILVUS_HOST,
    MILVUS_PORT,
    MILVUS_USER,
    MILVUS_PASSWORD,
    MILVUS_URI,
    MILVUS_INDEX_TYPE,
    MILVUS_INDEX_PARAMS,
    MILVUS_SEARCH_PARAMS,
    # æŸ¥è¯¢æ‰©å±•é…ç½®
    ENABLE_QUERY_EXPANSION,
    QUERY_EXPANSION_MODEL,
    QUERY_EXPANSION_PROMPT,
    MAX_EXPANDED_QUERIES,
    # å¤šæ¨¡æ€é…ç½®
    ENABLE_MULTIMODAL,
    MULTIMODAL_IMAGE_MODEL,
    SUPPORTED_IMAGE_FORMATS,
    IMAGE_EMBEDDING_DIM,
    MULTIMODAL_WEIGHTS
)
from reranker import create_reranker

# å¤šæ¨¡æ€æ”¯æŒç›¸å…³å¯¼å…¥
import base64
import io
from PIL import Image
import numpy as np
from typing import List, Dict, Any, Optional, Union

try:
    from langchain_core.documents import Document
except ImportError:
    try:
        from langchain_core.documents import Document
    except ImportError:
        from langchain.schema import Document


class CustomEnsembleRetriever:
    """è‡ªå®šä¹‰é›†æˆæ£€ç´¢å™¨ï¼Œç»“åˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢"""
    
    def __init__(self, retrievers, weights):
        self.retrievers = retrievers
        self.weights = weights
        
    def invoke(self, query):
        """æ‰§è¡Œæ£€ç´¢å¹¶åˆå¹¶ç»“æœ"""
        # è·å–å„æ£€ç´¢å™¨çš„ç»“æœ
        all_results = []
        for i, retriever in enumerate(self.retrievers):
            results = retriever.invoke(query)
            for doc in results:
                # æ·»åŠ æ£€ç´¢å™¨ç´¢å¼•å’Œæƒé‡ä¿¡æ¯
                doc.metadata["retriever_index"] = i
                doc.metadata["retriever_weight"] = self.weights[i]
                all_results.append(doc)
        
        return self._process_results(all_results)
    
    async def ainvoke(self, query):
        """å¼‚æ­¥æ‰§è¡Œæ£€ç´¢å¹¶åˆå¹¶ç»“æœ"""
        import asyncio
        
        # å¹¶å‘è·å–å„æ£€ç´¢å™¨çš„ç»“æœ
        # æ³¨æ„ï¼šå‡è®¾æ‰€æœ‰ retriever éƒ½æ”¯æŒ ainvoke
        tasks = [retriever.ainvoke(query) for retriever in self.retrievers]
        results_list = await asyncio.gather(*tasks)
        
        all_results = []
        for i, results in enumerate(results_list):
            for doc in results:
                # æ·»åŠ æ£€ç´¢å™¨ç´¢å¼•å’Œæƒé‡ä¿¡æ¯
                doc.metadata["retriever_index"] = i
                doc.metadata["retriever_weight"] = self.weights[i]
                all_results.append(doc)
                
        return self._process_results(all_results)

    def _process_results(self, all_results):
        """æ’åºå’Œå»é‡å¤„ç†"""
        # æ ¹æ®æƒé‡æ’åºå¹¶å»é‡
        # ç®€å•å®ç°ï¼šå…ˆæŒ‰æ£€ç´¢å™¨ç´¢å¼•æ’åºï¼Œå†æŒ‰æƒé‡æ’åº
        all_results.sort(key=lambda x: (x.metadata["retriever_index"], -x.metadata["retriever_weight"]))
        
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        unique_results = []
        seen_content = set()
        for doc in all_results:
            content = doc.page_content
            if content not in seen_content:
                seen_content.add(content)
                unique_results.append(doc)
                
        return unique_results


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»ï¼Œè´Ÿè´£æ–‡æ¡£åŠ è½½ã€å¤„ç†å’Œå‘é‡åŒ–"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=CHUNK_SIZE, 
            chunk_overlap=CHUNK_OVERLAP
        )
        
        # Try to initialize embeddings with error handling
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"âœ… æ£€æµ‹åˆ°è®¾å¤‡: {device}")
            if device == 'cuda':
                print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
                print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",  # è½»é‡çº§åµŒå…¥æ¨¡å‹
                model_kwargs={'device': device},  # è‡ªåŠ¨é€‰æ‹©GPUæˆ–CPU
                encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–åµŒå…¥å‘é‡
            )
            print(f"âœ… HuggingFaceåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (è®¾å¤‡: {device})")
        except Exception as e:
            print(f"âš ï¸ HuggingFaceåµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
            print("æ­£åœ¨å°è¯•å¤‡ç”¨åµŒå…¥æ–¹æ¡ˆ...")
            # Fallback to OpenAI embeddings or other alternatives
            from langchain_community.embeddings import FakeEmbeddings
            self.embeddings = FakeEmbeddings(size=384)  # For testing purposes
            print("âœ… ä½¿ç”¨æµ‹è¯•åµŒå…¥æ¨¡å‹")
            
        self.vectorstore = None
        self.retriever = None
        self.bm25_retriever = None  # BM25æ£€ç´¢å™¨
        self.ensemble_retriever = None  # é›†æˆæ£€ç´¢å™¨
        
        # åˆå§‹åŒ–é‡æ’å™¨
        self.reranker = None
        self._setup_reranker()
        
        # åˆå§‹åŒ–å¤šæ¨¡æ€æ”¯æŒ
        self.image_embeddings_model = None
        self._setup_multimodal()
        
        # åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•
        self.query_expansion_model = None
        self._setup_query_expansion()
    
    def _setup_reranker(self):
        """
        è®¾ç½®é‡æ’å™¨
        ä½¿ç”¨ CrossEncoder æå‡é‡æ’å‡†ç¡®ç‡
        """
        try:
            # ä½¿ç”¨ CrossEncoder é‡æ’å™¨ (å‡†ç¡®ç‡æœ€é«˜) â­
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– CrossEncoder é‡æ’å™¨...")
            self.reranker = create_reranker(
                'crossencoder',
                model_name='cross-encoder/ms-marco-MiniLM-L-6-v2',  # è½»é‡çº§æ¨¡å‹
                max_length=512
            )
            print("âœ… CrossEncoder é‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ CrossEncoder åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•å›é€€åˆ°æ··åˆé‡æ’å™¨...")
            try:
                # å›é€€åˆ°æ··åˆé‡æ’å™¨
                self.reranker = create_reranker('hybrid', self.embeddings)
                print("âœ… æ··åˆé‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                print(f"âš ï¸ é‡æ’å™¨åˆå§‹åŒ–å®Œå…¨å¤±è´¥: {e2}")
                print("âš ï¸ å°†ä½¿ç”¨åŸºç¡€æ£€ç´¢ï¼Œä¸è¿›è¡Œé‡æ’")
    
    def _setup_multimodal(self):
        """è®¾ç½®å¤šæ¨¡æ€æ”¯æŒ"""
        if not ENABLE_MULTIMODAL:
            print("âš ï¸ å¤šæ¨¡æ€æ”¯æŒå·²ç¦ç”¨")
            return
            
        try:
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å¤šæ¨¡æ€æ”¯æŒ...")
            from transformers import CLIPProcessor, CLIPModel
            import torch
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.image_embeddings_model = CLIPModel.from_pretrained(MULTIMODAL_IMAGE_MODEL).to(device)
            self.image_processor = CLIPProcessor.from_pretrained(MULTIMODAL_IMAGE_MODEL)
            print(f"âœ… å¤šæ¨¡æ€æ”¯æŒåˆå§‹åŒ–æˆåŠŸ (è®¾å¤‡: {device})")
        except Exception as e:
            print(f"âš ï¸ å¤šæ¨¡æ€æ”¯æŒåˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å°†ä»…ä½¿ç”¨æ–‡æœ¬æ£€ç´¢")
            self.image_embeddings_model = None
    
    def _setup_query_expansion(self):
        """è®¾ç½®æŸ¥è¯¢æ‰©å±•"""
        if not ENABLE_QUERY_EXPANSION:
            print("âš ï¸ æŸ¥è¯¢æ‰©å±•å·²ç¦ç”¨")
            return
            
        try:
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•...")
            from langchain_community.llms import Ollama
            
            self.query_expansion_model = Ollama(model=QUERY_EXPANSION_MODEL)
            print(f"âœ… æŸ¥è¯¢æ‰©å±•åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {QUERY_EXPANSION_MODEL})")
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢æ‰©å±•åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å°†ä¸ä½¿ç”¨æŸ¥è¯¢æ‰©å±•")
            self.query_expansion_model = None
    
    def load_documents(self, urls=None):
        """ä»URLåŠ è½½æ–‡æ¡£"""
        if urls is None:
            urls = KNOWLEDGE_BASE_URLS
        
        print(f"æ­£åœ¨åŠ è½½ {len(urls)} ä¸ªURLçš„æ–‡æ¡£...")
        docs = [WebBaseLoader(url).load() for url in urls]
        docs_list = [item for sublist in docs for item in sublist]
        print(f"æˆåŠŸåŠ è½½ {len(docs_list)} ä¸ªæ–‡æ¡£")
        return docs_list
    
    def split_documents(self, docs):
        """å°†æ–‡æ¡£åˆ†å‰²æˆå—"""
        print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        doc_splits = self.text_splitter.split_documents(docs)
        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(doc_splits)} ä¸ªæ–‡æ¡£å—")
        return doc_splits
    
    def initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥"""
        if self.vectorstore:
            return

        print("æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...")
        
        # å¼ºåˆ¶ä½¿ç”¨ Milvus
        try:
            # å‡†å¤‡è¿æ¥å‚æ•°
            connection_args = {}
            is_local_file = False
            
            # ä¼˜å…ˆä½¿ç”¨ URI
            if MILVUS_URI and len(MILVUS_URI.strip()) > 0:
                is_local_file = not (MILVUS_URI.startswith("http://") or MILVUS_URI.startswith("https://"))
                
                real_uri = MILVUS_URI
                if is_local_file:
                    import os
                    # Milvus Lite requires absolute path in some versions/environments
                    if not os.path.isabs(real_uri):
                        real_uri = os.path.abspath(real_uri)
                        print(f"ğŸ“‚ å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„: {real_uri}")
                    
                    # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                    parent_dir = os.path.dirname(real_uri)
                    if parent_dir and not os.path.exists(parent_dir):
                        print(f"ğŸ“‚ åˆ›å»º Milvus å­˜å‚¨ç›®å½•: {parent_dir}")
                        os.makedirs(parent_dir, exist_ok=True)
                
                mode_name = "Lite (Local File)" if is_local_file else "Cloud (HTTP)"
                print(f"ğŸ”„ æ­£åœ¨è¿æ¥ Milvus {mode_name} ({real_uri})...")
                connection_args["uri"] = real_uri
                
                if not is_local_file and MILVUS_PASSWORD:
                        connection_args["token"] = MILVUS_PASSWORD
            else:
                print(f"ğŸ”„ æ­£åœ¨è¿æ¥ Milvus Server ({MILVUS_HOST}:{MILVUS_PORT})...")
                connection_args = {
                    "host": MILVUS_HOST,
                    "port": MILVUS_PORT,
                    "user": MILVUS_USER,
                    "password": MILVUS_PASSWORD
                }

            # æ˜¾å¼å»ºç«‹å…¨å±€è¿æ¥ (ä¿®å¤ ConnectionNotExistException)
            try:
                from pymilvus import connections, utility
                print(f"ğŸ”Œ å°è¯•å»ºç«‹ pymilvus å…¨å±€è¿æ¥ (Alias: default)...")
                # ç§»é™¤æ—§è¿æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ä»¥é˜²å‚æ•°å˜æ›´
                if connections.has_connection("default"):
                    connections.disconnect("default")
                
                connections.connect(alias="default", **connection_args)
                print("âœ… pymilvus å…¨å±€è¿æ¥å»ºç«‹æˆåŠŸ")
                
                # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ (æå‰æ£€æŸ¥ï¼Œé¿å… LangChain å†…éƒ¨å‡ºé”™)
                if utility.has_collection(COLLECTION_NAME, using="default"):
                    print(f"âœ… é›†åˆ {COLLECTION_NAME} å·²å­˜åœ¨")
                else:
                    print(f"â„¹ï¸ é›†åˆ {COLLECTION_NAME} ä¸å­˜åœ¨ï¼Œå°†ç”± Milvus ç±»è‡ªåŠ¨åˆ›å»º")
                    
            except ImportError:
                print("âš ï¸ æœªæ‰¾åˆ° pymilvus åº“ï¼Œè·³è¿‡æ˜¾å¼è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ æ˜¾å¼è¿æ¥å°è¯•å¤±è´¥: {e}")
                # ç»§ç»­å°è¯•ï¼Œä¹Ÿè®¸ LangChain å†…éƒ¨èƒ½å¤„ç†

            # ç¡®å®šç´¢å¼•ç±»å‹
            # Milvus Lite (æœ¬åœ°æ¨¡å¼) ä»…æ”¯æŒ FLAT, IVF_FLAT, AUTOINDEXï¼Œä¸æ”¯æŒ HNSW
            final_index_type = MILVUS_INDEX_TYPE
            final_index_params = MILVUS_INDEX_PARAMS
            
            if is_local_file and MILVUS_INDEX_TYPE == "HNSW":
                print("âš ï¸ æ£€æµ‹åˆ° Milvus Lite (æœ¬åœ°æ¨¡å¼)ï¼ŒHNSW ç´¢å¼•ä¸å—æ”¯æŒï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º AUTOINDEX")
                final_index_type = "AUTOINDEX"
                final_index_params = {} # AUTOINDEX ä¸éœ€è¦å¤æ‚å‚æ•°

            # åˆå§‹åŒ– Milvus è¿æ¥ (ä¸åˆ é™¤æ—§æ•°æ®)
            # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬å·²ç»æ‰‹åŠ¨å»ºç«‹äº†å…¨å±€è¿æ¥ 'default'ï¼Œ
            # è¿™é‡Œæˆ‘ä»¬å°† connection_args ç®€åŒ–ä¸ºä»…æŒ‡å‘è¯¥ aliasï¼Œ
            # é¿å… LangChain å†æ¬¡å°è¯•è¿æ¥æˆ–å› å‚æ•°é—®é¢˜è¦†ç›–è¿æ¥ã€‚
            self.vectorstore = Milvus(
                embedding_function=self.embeddings,
                collection_name=COLLECTION_NAME,
                connection_args={"alias": "default"}, # âœ… å¤ç”¨å·²å»ºç«‹çš„è¿æ¥
                index_params={
                    "metric_type": "L2",
                    "index_type": final_index_type,
                    "params": final_index_params
                },
                search_params={
                    "metric_type": "L2", 
                    "params": MILVUS_SEARCH_PARAMS
                },
                drop_old=False,  # âœ… æŒä¹…åŒ–å…³é”®ï¼šä¸åˆ é™¤æ—§ç´¢å¼•
                auto_id=True
            )
            print("âœ… Milvus å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except ImportError:
            print("âŒ æœªå®‰è£… pymilvusï¼Œè¯·è¿è¡Œ: pip install pymilvus")
            raise
        except Exception as e:
            print(f"âŒ Milvus è¿æ¥å¤±è´¥: {e}")
            raise

        # é…ç½®æ£€ç´¢å™¨
        retriever_kwargs = {}
        # if ENABLE_MULTIMODAL:
            # retriever_kwargs["expr"] = "data_type == 'text'"
        self.retriever = self.vectorstore.as_retriever(search_kwargs=retriever_kwargs)

    def check_existing_urls(self, urls: List[str]) -> set:
        """æ£€æŸ¥å“ªäº›URLå·²ç»å­˜åœ¨äºå‘é‡åº“ä¸­"""
        if not self.vectorstore:
            return set()
            
        existing = set()
        print("æ­£åœ¨æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£...")
        try:
            # å°è¯•é€šè¿‡æ£€ç´¢æ¥æ£€æŸ¥
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ source å­—æ®µå¯ä»¥ä½œä¸ºè¿‡æ»¤æ¡ä»¶
            for url in urls:
                # ä½¿ç”¨ similarity_search ä½†å¸¦æœ‰ä¸¥æ ¼è¿‡æ»¤ï¼Œä¸”åªå–1æ¡
                # è¿™é‡Œçš„ query æ²¡å…³ç³»ï¼Œä¸»è¦çœ‹ filter
                try:
                    # æ³¨æ„ï¼šMilvus çš„ expr è¯­æ³•
                    expr = f'source == "{url}"'
                    res = self.vectorstore.similarity_search(
                        "test", 
                        k=1, 
                        expr=expr
                    )
                    if res:
                        existing.add(url)
                except Exception as e:
                    # å¦‚æœå¤±è´¥ï¼Œå¯èƒ½æ˜¯ schema é—®é¢˜ï¼Œå°è¯• metadata å­—æ®µ
                    try:
                        expr = f'metadata["source"] == "{url}"'
                        res = self.vectorstore.similarity_search(
                            "test", 
                            k=1, 
                            expr=expr
                        )
                        if res:
                            existing.add(url)
                    except:
                        pass
                        
            print(f"âœ… å‘ç° {len(existing)} ä¸ªå·²å­˜åœ¨çš„ URL")
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥ç°æœ‰URLå¤±è´¥: {e}")
            
        return existing

    def add_documents_to_vectorstore(self, doc_splits):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“"""
        if not doc_splits:
            return

        print(f"æ­£åœ¨æ·»åŠ  {len(doc_splits)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“...")
        if not self.vectorstore:
            self.initialize_vectorstore()
            
        # æ·»åŠ å…ƒæ•°æ®
        for doc in doc_splits:
            if 'source_type' not in doc.metadata:
                source = doc.metadata.get('source', '')
                if any(fmt in source.lower() for fmt in SUPPORTED_IMAGE_FORMATS):
                    doc.metadata['data_type'] = 'image'
                else:
                    doc.metadata['data_type'] = 'text'

        self.vectorstore.add_documents(doc_splits)
        print("âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ")
        
    def create_vectorstore(self, doc_splits, persist_directory=None):
        """(å·²å¼ƒç”¨) å…¼å®¹æ—§æ¥å£ï¼Œä½†ä½¿ç”¨æ–°é€»è¾‘"""
        print("âš ï¸ create_vectorstore å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ initialize_vectorstore å’Œ add_documents_to_vectorstore")
        self.initialize_vectorstore()
        if doc_splits:
            self.add_documents_to_vectorstore(doc_splits)
        return self.vectorstore, self.retriever

    def get_all_documents_from_vectorstore(self, limit: Optional[int] = None) -> List[Document]:
        """ä»å·²æŒä¹…åŒ–çš„å‘é‡æ•°æ®åº“è¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹å¹¶æ„é€  Document åˆ—è¡¨"""
        if not self.vectorstore:
            return []
        try:
            data = self.vectorstore._collection.get(include=["documents", "metadatas"])  # type: ignore
            docs_raw = data.get("documents") or []
            metas = data.get("metadatas") or []
            docs: List[Document] = []
            for i, content in enumerate(docs_raw):
                if content:
                    meta = metas[i] if i < len(metas) else {}
                    docs.append(Document(page_content=content, metadata=meta))
            if limit:
                return docs[:limit]
            return docs
        except Exception as e:
            print(f"âš ï¸ è¯»å–å‘é‡åº“æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    def setup_knowledge_base(self, urls=None, enable_graphrag=False):
        """è®¾ç½®å®Œæ•´çš„çŸ¥è¯†åº“ï¼ˆåŠ è½½ã€åˆ†å‰²ã€å‘é‡åŒ–ï¼‰
        
        Args:
            urls: æ–‡æ¡£URLåˆ—è¡¨
            enable_graphrag: æ˜¯å¦å¯ç”¨GraphRAGç´¢å¼•
            
        Returns:
            vectorstore, retriever, doc_splits
        """
        if urls is None:
            urls = KNOWLEDGE_BASE_URLS
            
        # 1. åˆå§‹åŒ–å‘é‡åº“è¿æ¥
        self.initialize_vectorstore()
        
        # 2. æ£€æŸ¥å·²å­˜åœ¨çš„ URL (å»é‡)
        existing_urls = self.check_existing_urls(urls)
        new_urls = [url for url in urls if url not in existing_urls]
        
        doc_splits = []
        if new_urls:
            print(f"ğŸ”„ å‘ç° {len(new_urls)} ä¸ªæ–° URLï¼Œå¼€å§‹å¤„ç†...")
            docs = self.load_documents(new_urls)
            doc_splits = self.split_documents(docs)
            self.add_documents_to_vectorstore(doc_splits)
        else:
            print("âœ… æ‰€æœ‰ URL å·²å­˜åœ¨ï¼Œè·³è¿‡æ–‡æ¡£åŠ è½½å’Œå‘é‡åŒ–")
            
        # 3. åˆå§‹åŒ–æ··åˆæ£€ç´¢ (BM25)
        if ENABLE_HYBRID_SEARCH:
            print("æ­£åœ¨åˆå§‹åŒ–æ··åˆæ£€ç´¢ (BM25)...")
            try:
                bm25_docs = []
                # å¦‚æœæœ‰æ—§æ•°æ®ä¸”è¿™æ¬¡æ²¡æœ‰åŠ è½½å…¨éƒ¨æ•°æ®ï¼Œå¿…é¡»ä» DB åŠ è½½æ‰€æœ‰æ–‡æ¡£ä»¥é‡å»º BM25
                # æ³¨æ„ï¼šå¦‚æœåªæœ‰æ–°æ–‡æ¡£ï¼ŒBM25 åªä¼šåŒ…å«æ–°æ–‡æ¡£ï¼Œè¿™æ˜¯ä¸å¯¹çš„ã€‚
                # åªè¦æœ‰ existing_urlsï¼Œè¯´æ˜åº“é‡Œæœ‰æ—§æ•°æ®ã€‚
                if len(existing_urls) > 0:
                    print("ğŸ”„ æ­£åœ¨ä»å‘é‡åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£ä»¥é‡å»º BM25 ç´¢å¼•...")
                    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾å†…å­˜å¤Ÿå¤§
                    all_docs = self.get_all_documents_from_vectorstore()
                    bm25_docs = all_docs
                else:
                    # å…¨æ–°æ„å»º
                    bm25_docs = doc_splits
                
                if bm25_docs:
                    self.bm25_retriever = BM25Retriever.from_documents(
                        bm25_docs, 
                        k=KEYWORD_SEARCH_K,
                        k1=BM25_K1,
                        b=BM25_B
                    )
                    
                    self.ensemble_retriever = CustomEnsembleRetriever(
                        retrievers=[self.retriever, self.bm25_retriever],
                        weights=[HYBRID_SEARCH_WEIGHTS["vector"], HYBRID_SEARCH_WEIGHTS["keyword"]]
                    )
                    print("âœ… æ··åˆæ£€ç´¢åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("âš ï¸ æ²¡æœ‰æ–‡æ¡£ç”¨äºåˆå§‹åŒ– BM25")
            except Exception as e:
                print(f"âš ï¸ æ··åˆæ£€ç´¢åˆå§‹åŒ–å¤±è´¥: {e}")
                self.ensemble_retriever = None
        
        # è¿”å› doc_splitsç”¨äºGraphRAGç´¢å¼• (æ³¨æ„ï¼šè¿™é‡Œåªè¿”å›äº†æ–°å¢çš„)
        return self.vectorstore, self.retriever, doc_splits
    
    async def async_expand_query(self, query: str) -> List[str]:
        """å¼‚æ­¥æ‰©å±•æŸ¥è¯¢"""
        if not self.query_expansion_model:
            return [query]
            
        try:
            # ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
            prompt = QUERY_EXPANSION_PROMPT.format(query=query)
            expanded_queries_text = await self.query_expansion_model.ainvoke(prompt)
            
            # è§£ææ‰©å±•æŸ¥è¯¢
            expanded_queries = [query]  # åŒ…å«åŸå§‹æŸ¥è¯¢
            for line in expanded_queries_text.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('//'):
                    # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
                    if line[0].isdigit() and '.' in line[:5]:
                        line = line.split('.', 1)[1].strip()
                    expanded_queries.append(line)
            
            # é™åˆ¶æ‰©å±•æŸ¥è¯¢æ•°é‡
            return expanded_queries[:MAX_EXPANDED_QUERIES + 1]
        except Exception as e:
            print(f"âš ï¸ å¼‚æ­¥æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [query]

    async def async_hybrid_retrieve(self, query: str, top_k: int = 5, filter_type: str = "text") -> List:
        """å¼‚æ­¥æ··åˆæ£€ç´¢
        
        Args:
            filter_type: æ•°æ®ç±»å‹è¿‡æ»¤ï¼Œ"text" (é»˜è®¤), "image", æˆ– "all" (ä¸è¿‡æ»¤)
        """
        # æ„å»ºæœç´¢å‚æ•°
        search_kwargs = {}
        if filter_type != "all" and ENABLE_MULTIMODAL:
            search_kwargs["expr"] = f"data_type == '{filter_type}'"

        if not ENABLE_HYBRID_SEARCH or not self.ensemble_retriever:
            # çº¯å‘é‡æ£€ç´¢ï¼Œç›´æ¥æ”¯æŒ search_kwargs
            if self.vectorstore:
                return await self.vectorstore.asimilarity_search(query, k=top_k, **search_kwargs)
            return await self.retriever.ainvoke(query)
            
        try:
            # æ··åˆæ£€ç´¢
            # æ³¨æ„ï¼šç›®å‰ CustomEnsembleRetriever çš„ invoke/ainvoke å°šæœªé€ä¼  search_kwargs
            # ä¸ºäº†è®©æ··åˆæ£€ç´¢ä¹Ÿäº«å—åˆ°è¿‡æ»¤ä¼˜åŒ–ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ CustomEnsembleRetriever æˆ–è€…åœ¨è¿™é‡Œå¤„ç†
            # é‰´äº CustomEnsembleRetriever æ¯”è¾ƒç®€å•ï¼Œæˆ‘ä»¬å‡è®¾å®ƒä¸»è¦ç”¨äºæ–‡æœ¬
            # å¦‚æœéœ€è¦ä¸¥æ ¼è¿‡æ»¤ï¼Œæœ€å¥½åœ¨ vectorstore å±‚é¢å¤„ç†
            
            # ä¸´æ—¶æ–¹æ¡ˆï¼šå¦‚æœæ˜¯æ··åˆæ£€ç´¢ä¸”éœ€è¦è¿‡æ»¤ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¼ é€’ç»™ retriever
            # ä½†æ ‡å‡† retriever æ¥å£ä¸æ”¯æŒåŠ¨æ€ä¼ å‚ã€‚
            # ç­–ç•¥ï¼šå¦‚æœ filter_type æ˜¯ text (é»˜è®¤)ï¼Œä¸”æˆ‘ä»¬åœ¨ init æ—¶å·²ç»è®¾ç½®äº†é»˜è®¤ä¸ä¸¥æ ¼è¿‡æ»¤ï¼Œ
            # è¿™é‡Œå…¶å®æ— æ³•åŠ¨æ€æ”¹å˜ retriever çš„è¡Œä¸ºï¼Œé™¤éæˆ‘ä»¬é‡æ–°ç”Ÿæˆä¸€ä¸ª retriever æˆ–è€…ä¿®æ”¹ retriever.search_kwargs
            
            # åŠ¨æ€ä¿®æ”¹ retriever çš„ search_kwargs (è¿™æ˜¯ LangChain retriever çš„ç‰¹æ€§)
            if filter_type != "all" and ENABLE_MULTIMODAL:
                self.retriever.search_kwargs["expr"] = f"data_type == '{filter_type}'"
            else:
                self.retriever.search_kwargs.pop("expr", None)
                
            results = await self.ensemble_retriever.ainvoke(query)
            return results[:top_k]
        except Exception as e:
            print(f"âš ï¸ å¼‚æ­¥æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°å‘é‡æ£€ç´¢")
            if self.vectorstore:
                return await self.vectorstore.asimilarity_search(query, k=top_k, **search_kwargs)
            return await self.retriever.ainvoke(query)

    async def async_enhanced_retrieve(self, query: str, top_k: int = 5, rerank_candidates: int = 20, 
                         image_paths: List[str] = None, use_query_expansion: bool = None):
        """å¼‚æ­¥å¢å¼ºæ£€ç´¢"""
        import asyncio
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
        if use_query_expansion is None:
            use_query_expansion = ENABLE_QUERY_EXPANSION
            
        # å¦‚æœå¯ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        if use_query_expansion:
            expanded_queries = await self.async_expand_query(query)
            print(f"æŸ¥è¯¢æ‰©å±•: {len(expanded_queries)} ä¸ªæŸ¥è¯¢")
        else:
            expanded_queries = [query]
            
        # å¤šæ¨¡æ€æ£€ç´¢ï¼ˆæš‚æ—¶ä¿æŒåŒæ­¥ï¼Œä½¿ç”¨çº¿ç¨‹æ± ï¼‰
        if image_paths and ENABLE_MULTIMODAL:
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(None, self.multimodal_retrieve, query, image_paths, top_k)
            
        # æ··åˆæ£€ç´¢æˆ–å‘é‡æ£€ç´¢
        all_candidate_docs = []
        
        # å†³å®šè¿‡æ»¤ç­–ç•¥
        # é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœåªæ˜¯æ–‡æœ¬æŸ¥è¯¢ï¼Œä¸ºäº†æ€§èƒ½ä¼˜åŒ–ï¼Œæˆ‘ä»¬åªæ£€ç´¢æ–‡æœ¬æ•°æ®
        # å¦‚æœæä¾›äº†å›¾åƒï¼Œæˆ–è€…ç”¨æˆ·æ˜¾å¼è¦æ±‚ï¼Œå¯ä»¥æ”¾å¼€é™åˆ¶
        filter_type = "text" # é»˜è®¤åªæœæ–‡æœ¬ï¼Œå®ç°ç™¾ä¸‡çº§æ•°æ®çš„æ€§èƒ½ä¼˜åŒ–
        if image_paths:
            filter_type = "all" # è·¨æ¨¡æ€æ—¶æœæ‰€æœ‰
            
        # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼ (ä»…ç”¨äºç›´æ¥è°ƒç”¨ vectorstore çš„æƒ…å†µï¼Œasync_hybrid_retrieve å†…éƒ¨å·²å¤„ç†)
        search_kwargs = {}
        if filter_type != "all" and ENABLE_MULTIMODAL:
             search_kwargs["expr"] = f"data_type == '{filter_type}'"

        async def retrieve_single(q):
            if ENABLE_HYBRID_SEARCH:
                # ä½¿ç”¨æ”¯æŒåŠ¨æ€è¿‡æ»¤çš„ hybrid retrieve
                 docs = await self.async_hybrid_retrieve(q, rerank_candidates, filter_type=filter_type)
            else:
                # ä½¿ç”¨å¸¦æœ‰è¿‡æ»¤æ¡ä»¶çš„æ£€ç´¢
                if self.vectorstore:
                    docs = await self.vectorstore.asimilarity_search(
                        q, 
                        k=rerank_candidates,
                        **search_kwargs # ä¼ å…¥ expr
                    )
                else:
                    # Fallback
                    docs = await self.retriever.ainvoke(q)
                
                if len(docs) > rerank_candidates:
                    docs = docs[:rerank_candidates]
            return docs

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢çš„æ£€ç´¢
        results = await asyncio.gather(*[retrieve_single(q) for q in expanded_queries])
        
        for docs in results:
            all_candidate_docs.extend(docs)
            
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        unique_docs = []
        seen_content = set()
        for doc in all_candidate_docs:
            content = doc.page_content
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)
                
        print(f"æ£€ç´¢è·å¾— {len(unique_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # é‡æ’ï¼ˆå¦‚æœé‡æ’å™¨å¯ç”¨ï¼‰
        # æ³¨æ„ï¼šé‡æ’é€šå¸¸æ˜¯è®¡ç®—å¯†é›†å‹ï¼Œå»ºè®®æ”¾å…¥çº¿ç¨‹æ± 
        if self.reranker and len(unique_docs) > top_k:
            try:
                loop = asyncio.get_running_loop()
                # rerank æ–¹æ³•å†…éƒ¨å¯èƒ½ä¹Ÿæ¯”è¾ƒè€—æ—¶
                reranked_results = await loop.run_in_executor(
                    None, 
                    self.reranker.rerank, 
                    query, unique_docs, top_k
                )
                final_docs = [doc for doc, score in reranked_results]
                scores = [score for doc, score in reranked_results]
                
                print(f"é‡æ’åè¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
                print(f"é‡æ’åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
                
                return final_docs
            except Exception as e:
                print(f"âš ï¸ é‡æ’å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                return unique_docs[:top_k]
        else:
            return unique_docs[:top_k]
    
    def expand_query(self, query: str) -> List[str]:
        """æ‰©å±•æŸ¥è¯¢ï¼Œç”Ÿæˆç›¸å…³æŸ¥è¯¢"""
        if not self.query_expansion_model:
            return [query]
            
        try:
            # ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
            prompt = QUERY_EXPANSION_PROMPT.format(query=query)
            expanded_queries_text = self.query_expansion_model.invoke(prompt)
            
            # è§£ææ‰©å±•æŸ¥è¯¢
            expanded_queries = [query]  # åŒ…å«åŸå§‹æŸ¥è¯¢
            for line in expanded_queries_text.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('//'):
                    # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
                    if line[0].isdigit() and '.' in line[:5]:
                        line = line.split('.', 1)[1].strip()
                    expanded_queries.append(line)
            
            # é™åˆ¶æ‰©å±•æŸ¥è¯¢æ•°é‡
            return expanded_queries[:MAX_EXPANDED_QUERIES + 1]  # +1 å› ä¸ºåŒ…å«åŸå§‹æŸ¥è¯¢
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [query]
    
    def encode_image(self, image_path: str) -> np.ndarray:
        """ç¼–ç å›¾åƒä¸ºåµŒå…¥å‘é‡"""
        if not self.image_embeddings_model:
            raise ValueError("å¤šæ¨¡æ€æ”¯æŒæœªåˆå§‹åŒ–")
            
        try:
            # åŠ è½½å¹¶å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            inputs = self.image_processor(images=image, return_tensors="pt")
            
            # è·å–å›¾åƒåµŒå…¥
            with torch.no_grad():
                image_features = self.image_embeddings_model.get_image_features(**inputs)
                # æ ‡å‡†åŒ–åµŒå…¥å‘é‡
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
                
            return image_features.cpu().numpy().flatten()
        except Exception as e:
            print(f"âš ï¸ å›¾åƒç¼–ç å¤±è´¥: {e}")
            raise
    
    def multimodal_retrieve(self, query: str, image_paths: List[str] = None, top_k: int = 5) -> List:
        """å¤šæ¨¡æ€æ£€ç´¢ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒ"""
        if not ENABLE_MULTIMODAL or not self.image_embeddings_model:
            # å¦‚æœå¤šæ¨¡æ€æœªå¯ç”¨ï¼Œå›é€€åˆ°æ–‡æœ¬æ£€ç´¢
            return self.hybrid_retrieve(query, top_k) if ENABLE_HYBRID_SEARCH else self.retriever.invoke(query)[:top_k]
        
        # 1. æ–‡æœ¬æŸ¥è¯¢ (Text-to-Text & Text-to-Image)
        # å¦‚æœæä¾›äº†æ–‡æœ¬æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒèƒ½æ£€ç´¢åˆ°æ–‡æœ¬å’Œç›¸å…³å›¾åƒ
        # æ­¤æ—¶ä¸åº”è¯¥é™åˆ¶ data_typeï¼Œæˆ–è€…åº”è¯¥æ˜¾å¼åŒ…å«ä¸¤è€…
        
        # å¦‚æœæ²¡æœ‰æä¾›å›¾åƒï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æŸ¥è¯¢ï¼Œä½†ä¹Ÿå¯èƒ½æƒ³æœå›¾
        # è¿™é‡Œæˆ‘ä»¬è®© self.retriever (æˆ– hybrid) è´Ÿè´£æ‰€æœ‰æ¨¡æ€çš„æ£€ç´¢
        # (å‰ææ˜¯å®ƒä»¬éƒ½åœ¨åŒä¸€ä¸ªå‘é‡ç©ºé—´ï¼ŒCLIP å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹)
        text_docs = []
        if query:
             text_docs = self.hybrid_retrieve(query, top_k) if ENABLE_HYBRID_SEARCH else self.retriever.invoke(query)[:top_k]
        
        # å¦‚æœæ²¡æœ‰æä¾›å›¾åƒè¾“å…¥ï¼Œç›´æ¥è¿”å›æ–‡æœ¬æŸ¥è¯¢çš„ç»“æœ
        if not image_paths:
            return text_docs
            
        try:
            # 2. å›¾åƒæŸ¥è¯¢ (Image-to-Text & Image-to-Image)
            image_results = []
            for image_path in image_paths:
                # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
                file_ext = image_path.split('.')[-1].lower()
                if file_ext not in SUPPORTED_IMAGE_FORMATS:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {file_ext}")
                    continue
                    
                # ç¼–ç å›¾åƒ
                image_embedding = self.encode_image(image_path)
                
                # ä½¿ç”¨å›¾åƒåµŒå…¥è¿›è¡Œæ£€ç´¢
                if self.vectorstore:
                    # å›¾åƒå¯ä»¥æ£€ç´¢æ–‡æœ¬æè¿°ï¼Œä¹Ÿå¯ä»¥æ£€ç´¢ç›¸ä¼¼å›¾åƒ
                    # è¿™é‡Œæˆ‘ä»¬ä¸åšé™åˆ¶ï¼Œæ£€ç´¢æ‰€æœ‰ç±»å‹
                    img_docs = self.vectorstore.similarity_search_by_vector(
                        embedding=image_embedding,
                        k=top_k
                    )
                    image_results.extend(img_docs)
                
            # åˆå¹¶æ–‡æœ¬æŸ¥è¯¢ç»“æœå’Œå›¾åƒæŸ¥è¯¢ç»“æœ
            # ç®€å•åˆå¹¶å¹¶å»é‡
            all_docs = text_docs + image_results
            
            # å»é‡
            unique_docs = []
            seen_content = set()
            for doc in all_docs:
                content = doc.page_content
                if content not in seen_content:
                    seen_content.add(content)
                    unique_docs.append(doc)
            
            final_docs = unique_docs[:top_k]
            
            print(f"âœ… å¤šæ¨¡æ€æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_docs)} ä¸ªç»“æœ")
            return final_docs
        except Exception as e:
            print(f"âš ï¸ å¤šæ¨¡æ€æ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°æ–‡æœ¬æ£€ç´¢")
            return text_docs
    
    def hybrid_retrieve(self, query: str, top_k: int = 5) -> List:
        """æ··åˆæ£€ç´¢ï¼Œç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢"""
        if not ENABLE_HYBRID_SEARCH or not self.ensemble_retriever:
            # å¦‚æœæ··åˆæ£€ç´¢æœªå¯ç”¨ï¼Œå›é€€åˆ°å‘é‡æ£€ç´¢
            return self.retriever.invoke(query)[:top_k]
            
        try:
            # ä½¿ç”¨é›†æˆæ£€ç´¢å™¨è¿›è¡Œæ··åˆæ£€ç´¢
            results = self.ensemble_retriever.invoke(query)
            return results[:top_k]
        except Exception as e:
            print(f"âš ï¸ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°å‘é‡æ£€ç´¢")
            return self.retriever.invoke(query)[:top_k]
    
    def enhanced_retrieve(self, query: str, top_k: int = 5, rerank_candidates: int = 20, 
                         image_paths: List[str] = None, use_query_expansion: bool = None):
        """å¢å¼ºæ£€ç´¢ï¼šå…ˆæ£€ç´¢æ›´å¤šå€™é€‰ï¼Œç„¶åé‡æ’ï¼Œæ”¯æŒæŸ¥è¯¢æ‰©å±•å’Œå¤šæ¨¡æ€
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            rerank_candidates: é‡æ’å‰çš„å€™é€‰æ–‡æ¡£æ•°é‡
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œç”¨äºå¤šæ¨¡æ€æ£€ç´¢
            use_query_expansion: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
        """
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
        if use_query_expansion is None:
            use_query_expansion = ENABLE_QUERY_EXPANSION
            
        # å¦‚æœå¯ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        if use_query_expansion:
            expanded_queries = self.expand_query(query)
            print(f"æŸ¥è¯¢æ‰©å±•: {len(expanded_queries)} ä¸ªæŸ¥è¯¢")
        else:
            expanded_queries = [query]
            
        # å¤šæ¨¡æ€æ£€ç´¢ï¼ˆå¦‚æœæä¾›äº†å›¾åƒï¼‰
        if image_paths and ENABLE_MULTIMODAL:
            return self.multimodal_retrieve(query, image_paths, top_k)
            
        # æ··åˆæ£€ç´¢æˆ–å‘é‡æ£€ç´¢
        all_candidate_docs = []
        for expanded_query in expanded_queries:
            if ENABLE_HYBRID_SEARCH:
                # ä½¿ç”¨æ··åˆæ£€ç´¢
                docs = self.hybrid_retrieve(expanded_query, rerank_candidates)
            else:
                # ä½¿ç”¨å‘é‡æ£€ç´¢
                docs = self.retriever.invoke(expanded_query)
                if len(docs) > rerank_candidates:
                    docs = docs[:rerank_candidates]
            
            all_candidate_docs.extend(docs)
            
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        unique_docs = []
        seen_content = set()
        for doc in all_candidate_docs:
            content = doc.page_content
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)
                
        print(f"æ£€ç´¢è·å¾— {len(unique_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # é‡æ’ï¼ˆå¦‚æœé‡æ’å™¨å¯ç”¨ï¼‰
        if self.reranker and len(unique_docs) > top_k:
            try:
                reranked_results = self.reranker.rerank(query, unique_docs, top_k)
                final_docs = [doc for doc, score in reranked_results]
                scores = [score for doc, score in reranked_results]
                
                print(f"é‡æ’åè¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
                print(f"é‡æ’åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
                
                return final_docs
            except Exception as e:
                print(f"âš ï¸ é‡æ’å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                return unique_docs[:top_k]
        else:
            # ä¸é‡æ’æˆ–å€™é€‰æ•°é‡ä¸è¶³
            return unique_docs[:top_k]
    
    def compare_retrieval_methods(self, query: str, top_k: int = 5, image_paths: List[str] = None):
        """æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ"""
        if not self.retriever:
            return {}
        
        results = {
            'query': query,
            'image_paths': image_paths
        }
        
        # åŸå§‹æ£€ç´¢ (ä½¿ç”¨ invoke æ›¿ä»£ get_relevant_documents)
        original_docs = self.retriever.invoke(query)[:top_k]
        results['vector_retrieval'] = {
            'count': len(original_docs),
            'documents': [{
                'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                'metadata': getattr(doc, 'metadata', {})
            } for doc in original_docs]
        }
        
        # æ··åˆæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ENABLE_HYBRID_SEARCH and self.ensemble_retriever:
            hybrid_docs = self.hybrid_retrieve(query, top_k)
            results['hybrid_retrieval'] = {
                'count': len(hybrid_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in hybrid_docs]
            }
        
        # æŸ¥è¯¢æ‰©å±•æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ENABLE_QUERY_EXPANSION and self.query_expansion_model:
            expanded_docs = self.enhanced_retrieve(query, top_k, use_query_expansion=True)
            results['expanded_query_retrieval'] = {
                'count': len(expanded_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in expanded_docs]
            }
        
        # å¤šæ¨¡æ€æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰å›¾åƒï¼‰
        if ENABLE_MULTIMODAL and image_paths:
            multimodal_docs = self.multimodal_retrieve(query, image_paths, top_k)
            results['multimodal_retrieval'] = {
                'count': len(multimodal_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in multimodal_docs]
            }
        
        # å¢å¼ºæ£€ç´¢ï¼ˆå¸¦é‡æ’ï¼‰
        enhanced_docs = self.enhanced_retrieve(query, top_k)
        results['enhanced_retrieval'] = {
            'count': len(enhanced_docs),
            'documents': [{
                'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                'metadata': getattr(doc, 'metadata', {})
            } for doc in enhanced_docs]
        }
        
        # æ·»åŠ é…ç½®ä¿¡æ¯
        results['configuration'] = {
            'hybrid_search_enabled': ENABLE_HYBRID_SEARCH,
            'query_expansion_enabled': ENABLE_QUERY_EXPANSION,
            'multimodal_enabled': ENABLE_MULTIMODAL,
            'reranker_used': self.reranker is not None,
            'hybrid_weights': HYBRID_SEARCH_WEIGHTS if ENABLE_HYBRID_SEARCH else None,
            'multimodal_weights': MULTIMODAL_WEIGHTS if ENABLE_MULTIMODAL else None
        }
        
        return results

    def format_docs(self, docs):
        """æ ¼å¼åŒ–æ–‡æ¡£ç”¨äºç”Ÿæˆ"""
        return "\n\n".join(doc.page_content for doc in docs)


def initialize_document_processor():
    """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨å¹¶è®¾ç½®çŸ¥è¯†åº“"""
    print("ğŸš€ åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ (Milvus ç‰ˆ)...")
    processor = DocumentProcessor()
    
    # ç›´æ¥è®¾ç½®çŸ¥è¯†åº“
    # Milvus çš„è¿æ¥å’Œç´¢å¼•é€»è¾‘åœ¨ DocumentProcessor.create_vectorstore ä¸­å¤„ç†
    vectorstore, retriever, doc_splits = processor.setup_knowledge_base()
    
    return processor, vectorstore, retriever, doc_splits
