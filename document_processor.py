"""
æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–æ¨¡å—
è´Ÿè´£æ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–å’Œå‘é‡æ•°æ®åº“åˆå§‹åŒ–
"""

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

from config import (
    KNOWLEDGE_BASE_URLS,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    COLLECTION_NAME,
    EMBEDDING_MODEL
)
from reranker import create_reranker


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»ï¼Œè´Ÿè´£æ–‡æ¡£åŠ è½½ã€å¤„ç†å’Œå‘é‡åŒ–"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=CHUNK_SIZE, 
            chunk_overlap=CHUNK_OVERLAP
        )
        
        # Try to initialize embeddings with error handling
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"âœ… æ£€æµ‹åˆ°è®¾å¤‡: {device}")
            if device == 'cuda':
                print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
                print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",  # è½»é‡çº§åµŒå…¥æ¨¡å‹
                model_kwargs={'device': device},  # è‡ªåŠ¨é€‰æ‹©GPUæˆ–CPU
                encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–åµŒå…¥å‘é‡
            )
            print(f"âœ… HuggingFaceåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (è®¾å¤‡: {device})")
        except Exception as e:
            print(f"âš ï¸ HuggingFaceåµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
            print("æ­£åœ¨å°è¯•å¤‡ç”¨åµŒå…¥æ–¹æ¡ˆ...")
            # Fallback to OpenAI embeddings or other alternatives
            from langchain_community.embeddings import FakeEmbeddings
            self.embeddings = FakeEmbeddings(size=384)  # For testing purposes
            print("âœ… ä½¿ç”¨æµ‹è¯•åµŒå…¥æ¨¡å‹")
            
        self.vectorstore = None
        self.retriever = None
        
        # åˆå§‹åŒ–é‡æ’å™¨
        self.reranker = None
        self._setup_reranker()
    
    def _setup_reranker(self):
        """
        è®¾ç½®é‡æ’å™¨
        ä½¿ç”¨ CrossEncoder æå‡é‡æ’å‡†ç¡®ç‡
        """
        try:
            # ä½¿ç”¨ CrossEncoder é‡æ’å™¨ (å‡†ç¡®ç‡æœ€é«˜) â­
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– CrossEncoder é‡æ’å™¨...")
            self.reranker = create_reranker(
                'crossencoder',
                model_name='cross-encoder/ms-marco-MiniLM-L-6-v2',  # è½»é‡çº§æ¨¡å‹
                max_length=512
            )
            print("âœ… CrossEncoder é‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ CrossEncoder åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•å›é€€åˆ°æ··åˆé‡æ’å™¨...")
            try:
                # å›é€€åˆ°æ··åˆé‡æ’å™¨
                self.reranker = create_reranker('hybrid', self.embeddings)
                print("âœ… æ··åˆé‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                print(f"âš ï¸ é‡æ’å™¨åˆå§‹åŒ–å®Œå…¨å¤±è´¥: {e2}")
                print("âš ï¸ å°†ä½¿ç”¨åŸºç¡€æ£€ç´¢ï¼Œä¸è¿›è¡Œé‡æ’")
    
    def load_documents(self, urls=None):
        """ä»URLåŠ è½½æ–‡æ¡£"""
        if urls is None:
            urls = KNOWLEDGE_BASE_URLS
        
        print(f"æ­£åœ¨åŠ è½½ {len(urls)} ä¸ªURLçš„æ–‡æ¡£...")
        docs = [WebBaseLoader(url).load() for url in urls]
        docs_list = [item for sublist in docs for item in sublist]
        print(f"æˆåŠŸåŠ è½½ {len(docs_list)} ä¸ªæ–‡æ¡£")
        return docs_list
    
    def split_documents(self, docs):
        """å°†æ–‡æ¡£åˆ†å‰²æˆå—"""
        print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        doc_splits = self.text_splitter.split_documents(docs)
        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(doc_splits)} ä¸ªæ–‡æ¡£å—")
        return doc_splits
    
    def create_vectorstore(self, doc_splits):
        """åˆ›å»ºå‘é‡æ•°æ®åº“"""
        print("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
        self.vectorstore = Chroma.from_documents(
            documents=doc_splits,
            collection_name=COLLECTION_NAME,
            embedding=self.embeddings,
        )
        self.retriever = self.vectorstore.as_retriever()
        print("å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ")
        return self.vectorstore, self.retriever
    
    def setup_knowledge_base(self, urls=None, enable_graphrag=False):
        """è®¾ç½®å®Œæ•´çš„çŸ¥è¯†åº“ï¼ˆåŠ è½½ã€åˆ†å‰²ã€å‘é‡åŒ–ï¼‰
        
        Args:
            urls: æ–‡æ¡£URLåˆ—è¡¨
            enable_graphrag: æ˜¯å¦å¯ç”¨GraphRAGç´¢å¼•
            
        Returns:
            vectorstore, retriever, doc_splits
        """
        docs = self.load_documents(urls)
        doc_splits = self.split_documents(docs)
        vectorstore, retriever = self.create_vectorstore(doc_splits)
        
        # è¿”å›doc_splitsç”¨äºGraphRAGç´¢å¼•
        return vectorstore, retriever, doc_splits
    
    def enhanced_retrieve(self, query: str, top_k: int = 5, rerank_candidates: int = 20):
        """å¢å¼ºæ£€ç´¢ï¼šå…ˆæ£€ç´¢æ›´å¤šå€™é€‰ï¼Œç„¶åé‡æ’"""
        if not self.retriever:
            print("âš ï¸ æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
            return []
        
        # 1. åˆå§‹æ£€ç´¢ï¼šè·å–æ›´å¤šå€™é€‰æ–‡æ¡£
        initial_docs = self.retriever.get_relevant_documents(query)
        
        # è·å–æ›´å¤šå€™é€‰ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        if hasattr(self.retriever, 'search_kwargs'):
            # ä¿®æ”¹æ£€ç´¢å‚æ•°ä»¥è·å–æ›´å¤šç»“æœ
            original_k = self.retriever.search_kwargs.get('k', 4)
            self.retriever.search_kwargs['k'] = min(rerank_candidates, len(initial_docs))
            candidate_docs = self.retriever.get_relevant_documents(query)
            self.retriever.search_kwargs['k'] = original_k  # æ¢å¤åŸè®¾ç½®
        else:
            candidate_docs = initial_docs
        
        print(f"åˆå§‹æ£€ç´¢è·å¾— {len(candidate_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # 2. é‡æ’ï¼ˆå¦‚æœé‡æ’å™¨å¯ç”¨ï¼‰
        if self.reranker and len(candidate_docs) > top_k:
            try:
                reranked_results = self.reranker.rerank(query, candidate_docs, top_k)
                final_docs = [doc for doc, score in reranked_results]
                scores = [score for doc, score in reranked_results]
                
                print(f"é‡æ’åè¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
                print(f"é‡æ’åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
                
                return final_docs
            except Exception as e:
                print(f"âš ï¸ é‡æ’å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                return candidate_docs[:top_k]
        else:
            # ä¸é‡æ’æˆ–å€™é€‰æ•°é‡ä¸è¶³
            return candidate_docs[:top_k]
    
    def compare_retrieval_methods(self, query: str, top_k: int = 5):
        """æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ"""
        if not self.retriever:
            return {}
        
        # åŸå§‹æ£€ç´¢
        original_docs = self.retriever.get_relevant_documents(query)[:top_k]
        
        # å¢å¼ºæ£€ç´¢ï¼ˆå¸¦é‡æ’ï¼‰
        enhanced_docs = self.enhanced_retrieve(query, top_k)
        
        return {
            'query': query,
            'original_retrieval': {
                'count': len(original_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in original_docs]
            },
            'enhanced_retrieval': {
                'count': len(enhanced_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in enhanced_docs]
            },
            'reranker_used': self.reranker is not None
        }

    def format_docs(self, docs):
        """æ ¼å¼åŒ–æ–‡æ¡£ç”¨äºç”Ÿæˆ"""
        return "\n\n".join(doc.page_content for doc in docs)


def initialize_document_processor():
    """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨å¹¶è®¾ç½®çŸ¥è¯†åº“"""
    processor: DocumentProcessor = DocumentProcessor()
    vectorstore, retriever, doc_splits = processor.setup_knowledge_base()
    return processor, vectorstore, retriever, doc_splits