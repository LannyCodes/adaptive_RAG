"""
æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–æ¨¡å—
è´Ÿè´£æ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ†å—ã€å‘é‡åŒ–å’Œå‘é‡æ•°æ®åº“åˆå§‹åŒ–
"""

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
# å°è¯•å¯¼å…¥ langchain_milvusï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ° langchain_community å¹¶åº”ç”¨è¡¥ä¸
try:
    from langchain_milvus import MilvusVectorStore as Milvus
    print("âœ… ä½¿ç”¨ langchain-milvus (æ–°ç‰ˆ)")
except ImportError:
    try:
        from langchain_community.vectorstores import Milvus
        print("âš ï¸ ä½¿ç”¨ langchain_community.vectorstores.Milvus (æ—§ç‰ˆ)")
        
        # Monkeypatch: ä¿®å¤æ—§ç‰ˆ LangChain å¯¹ Milvus Lite æœ¬åœ°æ–‡ä»¶è·¯å¾„çš„æ ¡éªŒé—®é¢˜
        # æ—§ç‰ˆ _create_connection_alias å¼ºåˆ¶è¦æ±‚ URI ä»¥ http/https å¼€å¤´
        def _patched_create_connection_alias(self, connection_args):
            uri = connection_args.get("uri")
            # ä¸ºæœ¬åœ°æ–‡ä»¶ç”Ÿæˆå”¯ä¸€çš„ alias
            if uri:
                import hashlib
                return hashlib.md5(uri.encode()).hexdigest()
            return "default"
            
        # åº”ç”¨è¡¥ä¸
        Milvus._create_connection_alias = _patched_create_connection_alias
        print("ğŸ”§ å·²åº”ç”¨ Milvus Lite è·¯å¾„æ ¡éªŒè¡¥ä¸")
    except ImportError:
        pass

from langchain_community.embeddings import HuggingFaceEmbeddings

from config import (
    KNOWLEDGE_BASE_URLS,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    COLLECTION_NAME,
    EMBEDDING_MODEL,
    # æ··åˆæ£€ç´¢é…ç½®
    ENABLE_HYBRID_SEARCH,
    HYBRID_SEARCH_WEIGHTS,
    KEYWORD_SEARCH_K,
    BM25_K1,
    BM25_B,
    # å‘é‡åº“é…ç½®
    VECTOR_STORE_TYPE,
    MILVUS_HOST,
    MILVUS_PORT,
    MILVUS_USER,
    MILVUS_PASSWORD,
    MILVUS_URI,
    MILVUS_INDEX_TYPE,
    MILVUS_INDEX_PARAMS,
    MILVUS_SEARCH_PARAMS,
    # Elasticsearch é…ç½®
    ES_URL,
    ES_USER,
    ES_PASSWORD,
    ES_INDEX_NAME,
    ES_VERIFY_CERTS,
    # æŸ¥è¯¢æ‰©å±•é…ç½®
    ENABLE_QUERY_EXPANSION,
    QUERY_EXPANSION_MODEL,
    QUERY_EXPANSION_PROMPT,
    MAX_EXPANDED_QUERIES,
    # å¤šæ¨¡æ€é…ç½®
    ENABLE_MULTIMODAL,
    MULTIMODAL_IMAGE_MODEL,
    SUPPORTED_IMAGE_FORMATS,
    IMAGE_EMBEDDING_DIM,
    MULTIMODAL_WEIGHTS
)
from reranker import create_reranker

# å¤šæ¨¡æ€æ”¯æŒç›¸å…³å¯¼å…¥
import base64
import io
from PIL import Image
import numpy as np
from typing import List, Dict, Any, Optional, Union

try:
    from langchain_core.documents import Document
except ImportError:
    try:
        from langchain_core.documents import Document
    except ImportError:
        from langchain.schema import Document


class CustomEnsembleRetriever:
    """è‡ªå®šä¹‰é›†æˆæ£€ç´¢å™¨ï¼Œç»“åˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢"""
    
    def __init__(self, retrievers, weights):
        self.retrievers = retrievers
        self.weights = weights
        
    def invoke(self, query):
        """æ‰§è¡Œæ£€ç´¢å¹¶åŠ æƒåˆå¹¶ç»“æœï¼Œæ”¯æŒç›¸é‚»å—åˆå¹¶"""
        all_results = []
        
        # 1. æ‰§è¡Œå„ä¸ªæ£€ç´¢å™¨
        for i, retriever in enumerate(self.retrievers):
            try:
                docs = retriever.invoke(query)
                weight = self.weights[i]
                for doc in docs:
                    # å°†æƒé‡æ³¨å…¥ metadataï¼Œæ–¹ä¾¿åç»­æ’åº
                    doc.metadata["retriever_weight"] = weight
                    doc.metadata["retriever_index"] = i
                    all_results.append(doc)
            except Exception as e:
                print(f"âš ï¸ Retriever {i} failed: {e}")

        # 2. é¢„å¤„ç†ï¼šå»é‡å’Œåˆæ­¥æ’åº
        unique_docs = self._process_results(all_results)
        
        # 3. å…³é”®æ­¥éª¤ï¼šåˆå¹¶ç›¸é‚»çš„æ–‡æœ¬å— (Context Merging)
        # å¦‚æœ doc A å’Œ doc B æ¥è‡ªåŒä¸€ä¸ªæ–‡ä»¶ï¼Œä¸”ç´¢å¼•å·ç›¸é‚»ï¼Œè¯´æ˜å®ƒä»¬æ˜¯è¿ç»­çš„æ®µè½
        # è¿™ä¸€æ­¥èƒ½æœ‰æ•ˆè§£å†³ "æ­¥éª¤1åœ¨Chunk1, æ­¥éª¤2åœ¨Chunk2" å¯¼è‡´ä¿¡æ¯æ–­è£‚çš„é—®é¢˜
        merged_docs = self._merge_adjacent_chunks(unique_docs)
        
        return merged_docs

    def _merge_adjacent_chunks(self, docs):
        """åˆå¹¶æ¥è‡ªåŒä¸€æ–‡æ¡£çš„ç›¸é‚»åˆ†å—"""
        if not docs:
            return []
            
        # æŒ‰ (source, start_index) æ’åºï¼Œä»¥ä¾¿å‘ç°ç›¸é‚»å—
        # å‡è®¾ metadata ä¸­æœ‰ 'source' å’Œ 'start_index' (è¿™é€šå¸¸ç”± TextSplitter æ·»åŠ )
        # å¦‚æœæ²¡æœ‰ start_indexï¼Œåˆ™å°è¯•ç”¨å†…å®¹é‡å åº¦åˆ¤æ–­ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        
        # ä¸ºäº†å®‰å…¨ï¼Œå…ˆæ£€æŸ¥ metadata æ˜¯å¦æœ‰å¿…è¦å­—æ®µ
        docs_with_meta = []
        for doc in docs:
            # å¦‚æœæ²¡æœ‰ start_indexï¼Œå°è¯•ç”¨ page_content çš„å“ˆå¸Œæˆ–ç®€å•é¡ºåºä½œä¸ºæ›¿è¡¥
            if "start_index" not in doc.metadata:
                doc.metadata["start_index"] = 0 
            docs_with_meta.append(doc)
            
        # æŒ‰æºæ–‡ä»¶å’Œèµ·å§‹ä½ç½®æ’åº
        sorted_docs = sorted(docs_with_meta, key=lambda x: (x.metadata.get("source", ""), x.metadata.get("start_index", 0)))
        
        merged = []
        current_doc = None
        
        for doc in sorted_docs:
            if current_doc is None:
                current_doc = doc
                continue
                
            # åˆ¤æ–­æ˜¯å¦ç›¸é‚»ï¼š
            # 1. åŒæº
            # 2. å½“å‰docçš„å¼€å§‹ä½ç½® <= ä¸Šä¸€ä¸ªdocçš„ç»“æŸä½ç½® + å®¹å·® (æ¯”å¦‚ 200 chars)
            is_same_source = current_doc.metadata.get("source") == doc.metadata.get("source")
            
            # è®¡ç®—ä¸Šä¸€ä¸ªdocçš„ç»“æŸä½ç½®
            prev_end = current_doc.metadata.get("start_index", 0) + len(current_doc.page_content)
            curr_start = doc.metadata.get("start_index", 0)
            
            # å¦‚æœé‡å æˆ–éå¸¸æ¥è¿‘ (è·ç¦» < 200 å­—ç¬¦)
            is_adjacent = (curr_start - prev_end) < 200
            
            if is_same_source and is_adjacent:
                # åˆå¹¶ï¼
                # print(f"ğŸ”— åˆå¹¶ç›¸é‚»å—: {current_doc.metadata.get('source')} (End: {prev_end}) + (Start: {curr_start})")
                new_content = current_doc.page_content + "\n" + doc.page_content
                # æ›´æ–° current_doc
                current_doc.page_content = new_content
                # ä¿ç•™å…ƒæ•°æ®
                current_doc.metadata["merged"] = True
            else:
                # ä¸ç›¸é‚»ï¼Œä¿å­˜ä¸Šä¸€ä¸ªï¼Œå¼€å§‹æ–°çš„
                merged.append(current_doc)
                current_doc = doc
                
        if current_doc:
            merged.append(current_doc)
            
        return merged
    
    async def ainvoke(self, query):
        """å¼‚æ­¥æ‰§è¡Œæ£€ç´¢å¹¶åˆå¹¶ç»“æœ"""
        import asyncio
        
        # å¹¶å‘è·å–å„æ£€ç´¢å™¨çš„ç»“æœ
        # æ³¨æ„ï¼šå‡è®¾æ‰€æœ‰ retriever éƒ½æ”¯æŒ ainvoke
        tasks = [retriever.ainvoke(query) for retriever in self.retrievers]
        results_list = await asyncio.gather(*tasks)
        
        all_results = []
        for i, results in enumerate(results_list):
            for doc in results:
                # æ·»åŠ æ£€ç´¢å™¨ç´¢å¼•å’Œæƒé‡ä¿¡æ¯
                doc.metadata["retriever_index"] = i
                doc.metadata["retriever_weight"] = self.weights[i]
                all_results.append(doc)
                
        return self._process_results(all_results)

    def _process_results(self, all_results):
        """æ’åºå’Œå»é‡å¤„ç†"""
        # æ ¹æ®æƒé‡æ’åºå¹¶å»é‡
        # ç®€å•å®ç°ï¼šå…ˆæŒ‰æ£€ç´¢å™¨ç´¢å¼•æ’åºï¼Œå†æŒ‰æƒé‡æ’åº
        all_results.sort(key=lambda x: (x.metadata["retriever_index"], -x.metadata["retriever_weight"]))
        
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        unique_results = []
        seen_content = set()
        for doc in all_results:
            content = doc.page_content
            if content not in seen_content:
                seen_content.add(content)
                unique_results.append(doc)
                
        return unique_results


from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun

class ElasticsearchBM25Retriever(BaseRetriever):
    """è‡ªå®šä¹‰ Elasticsearch BM25 æ£€ç´¢å™¨"""
    client: Any = None
    index_name: str = ""
    k: int = 5
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        if not self.client:
            return []
        try:
            # æ ‡å‡† BM25 æŸ¥è¯¢
            response = self.client.search(
                index=self.index_name,
                body={
                    "query": {
                        "match": {
                            "text": query
                        }
                    },
                    "size": self.k
                }
            )
            
            docs = []
            for hit in response["hits"]["hits"]:
                source = hit["_source"]
                # è¿˜åŸ Document å¯¹è±¡
                doc = Document(
                    page_content=source.get("text", ""),
                    metadata=source.get("metadata", {})
                )
                docs.append(doc)
            return docs
        except Exception as e:
            print(f"âš ï¸ Elasticsearch BM25 æ£€ç´¢å¤±è´¥: {e}")
            return []

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ç±»ï¼Œè´Ÿè´£æ–‡æ¡£åŠ è½½ã€å¤„ç†å’Œå‘é‡åŒ–"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            model_name="gpt-4",  # âœ… ä½¿ç”¨ cl100k_baseï¼Œå¯¹ä¸­æ–‡å‹ç¼©ç‡é«˜
            chunk_size=CHUNK_SIZE, 
            chunk_overlap=CHUNK_OVERLAP,
            add_start_index=True,  # âœ… å…³é”®ï¼šæ·»åŠ èµ·å§‹ç´¢å¼•ï¼Œç”¨äºåç»­çš„ç›¸é‚»å—åˆå¹¶
            # âœ… ä¼˜å…ˆåœ¨ä¸­æ–‡å¥å·ã€æ„Ÿå¹å·å¤„æ–­å¥ï¼Œè€Œä¸æ˜¯ç”Ÿç¡¬åœ°åˆ‡æ–­
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
        )
        
        # Try to initialize embeddings with error handling
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"âœ… æ£€æµ‹åˆ°è®¾å¤‡: {device}")
            if device == 'cuda':
                print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
                print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL,  # è½»é‡çº§åµŒå…¥æ¨¡å‹
                model_kwargs={'device': device},  # è‡ªåŠ¨é€‰æ‹©GPUæˆ–CPU
                encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–åµŒå…¥å‘é‡
            )
            print(f"âœ… HuggingFaceåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (è®¾å¤‡: {device})")
        except Exception as e:
            print(f"âš ï¸ HuggingFaceåµŒå…¥åˆå§‹åŒ–å¤±è´¥: {e}")
            print("æ­£åœ¨å°è¯•å¤‡ç”¨åµŒå…¥æ–¹æ¡ˆ...")
            # Fallback to OpenAI embeddings or other alternatives
            from langchain_community.embeddings import FakeEmbeddings
            self.embeddings = FakeEmbeddings(size=384)  # For testing purposes
            print("âœ… ä½¿ç”¨æµ‹è¯•åµŒå…¥æ¨¡å‹")
            
        self.vectorstore = None
        self.retriever = None
        self.bm25_retriever = None  # BM25æ£€ç´¢å™¨
        self.ensemble_retriever = None  # é›†æˆæ£€ç´¢å™¨
        
        # åˆå§‹åŒ–é‡æ’å™¨
        self.reranker = None
        self._setup_reranker()
        
        # åˆå§‹åŒ–å¤šæ¨¡æ€æ”¯æŒ
        self.image_embeddings_model = None
        self._setup_multimodal()
        
        # åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•
        self.query_expansion_model = None
        self._setup_query_expansion()

        # åˆå§‹åŒ– Elasticsearch (ç”¨äºç™¾ä¸‡çº§ BM25 æ£€ç´¢)
        self.es_client = None
        self._setup_elasticsearch()
    
    def _setup_elasticsearch(self):
        """è®¾ç½® Elasticsearch è¿æ¥"""
        if not ENABLE_HYBRID_SEARCH:
            return

        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Elasticsearch (BM25)...")
        try:
            from elasticsearch import Elasticsearch
            
            # æ„å»ºè¿æ¥å‚æ•°
            es_params = {"hosts": [ES_URL], "verify_certs": ES_VERIFY_CERTS}
            if ES_USER and ES_PASSWORD:
                es_params["basic_auth"] = (ES_USER, ES_PASSWORD)
            
            self.es_client = Elasticsearch(**es_params)
            
            # æµ‹è¯•è¿æ¥
            if not self.es_client.ping():
                print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Elasticsearch ({ES_URL})ï¼Œå°†æ— æ³•ä½¿ç”¨ BM25 æ£€ç´¢")
                self.es_client = None
            else:
                print(f"âœ… Elasticsearch è¿æ¥æˆåŠŸ: {ES_URL}")
                
                # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
                if not self.es_client.indices.exists(index=ES_INDEX_NAME):
                    print(f"â„¹ï¸ åˆ›å»º Elasticsearch ç´¢å¼•: {ES_INDEX_NAME}")
                    # å®šä¹‰ç®€å•çš„ mapping
                    mapping = {
                        "mappings": {
                            "properties": {
                                "text": {"type": "text", "analyzer": "standard"}, # ä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨ï¼Œæ”¯æŒä¸­æ–‡
                                "metadata": {"type": "object"}
                            }
                        }
                    }
                    self.es_client.indices.create(index=ES_INDEX_NAME, body=mapping)
                    print("âœ… Elasticsearch ç´¢å¼•åˆ›å»ºæˆåŠŸ")
                else:
                    print(f"âœ… Elasticsearch ç´¢å¼• {ES_INDEX_NAME} å·²å­˜åœ¨")
                    
        except ImportError:
            print("âš ï¸ æœªå®‰è£… elasticsearch åº“ï¼Œè¯·è¿è¡Œ 'pip install elasticsearch'")
            self.es_client = None
        except Exception as e:
            print(f"âš ï¸ Elasticsearch åˆå§‹åŒ–å¤±è´¥: {e}")
            self.es_client = None
    
    def _setup_reranker(self):
        """
        è®¾ç½®é‡æ’å™¨
        ä½¿ç”¨ CrossEncoder æå‡é‡æ’å‡†ç¡®ç‡
        """
        try:
            # ä½¿ç”¨ CrossEncoder é‡æ’å™¨ (å‡†ç¡®ç‡æœ€é«˜) â­
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– CrossEncoder é‡æ’å™¨...")
            self.reranker = create_reranker(
                'crossencoder',
                model_name='BAAI/bge-reranker-base',  # âœ… æ”¯æŒä¸­æ–‡çš„ SOTA é‡æ’å™¨
                max_length=1024  # âœ… åŒ¹é…æˆ‘ä»¬çš„ Chunk Size
            )
            print("âœ… CrossEncoder é‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ CrossEncoder åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•å›é€€åˆ°æ··åˆé‡æ’å™¨...")
            try:
                # å›é€€åˆ°æ··åˆé‡æ’å™¨
                self.reranker = create_reranker('hybrid', self.embeddings)
                print("âœ… æ··åˆé‡æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                print(f"âš ï¸ é‡æ’å™¨åˆå§‹åŒ–å®Œå…¨å¤±è´¥: {e2}")
                print("âš ï¸ å°†ä½¿ç”¨åŸºç¡€æ£€ç´¢ï¼Œä¸è¿›è¡Œé‡æ’")
    
    def _setup_multimodal(self):
        """è®¾ç½®å¤šæ¨¡æ€æ”¯æŒ"""
        if not ENABLE_MULTIMODAL:
            print("âš ï¸ å¤šæ¨¡æ€æ”¯æŒå·²ç¦ç”¨")
            return
            
        try:
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å¤šæ¨¡æ€æ”¯æŒ...")
            from transformers import CLIPProcessor, CLIPModel
            import torch
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.image_embeddings_model = CLIPModel.from_pretrained(MULTIMODAL_IMAGE_MODEL).to(device)
            self.image_processor = CLIPProcessor.from_pretrained(MULTIMODAL_IMAGE_MODEL)
            print(f"âœ… å¤šæ¨¡æ€æ”¯æŒåˆå§‹åŒ–æˆåŠŸ (è®¾å¤‡: {device})")
        except Exception as e:
            print(f"âš ï¸ å¤šæ¨¡æ€æ”¯æŒåˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å°†ä»…ä½¿ç”¨æ–‡æœ¬æ£€ç´¢")
            self.image_embeddings_model = None
    
    def _setup_query_expansion(self):
        """è®¾ç½®æŸ¥è¯¢æ‰©å±•"""
        if not ENABLE_QUERY_EXPANSION:
            print("âš ï¸ æŸ¥è¯¢æ‰©å±•å·²ç¦ç”¨")
            return
            
        try:
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æŸ¥è¯¢æ‰©å±•...")
            from langchain_community.llms import Ollama
            
            self.query_expansion_model = Ollama(model=QUERY_EXPANSION_MODEL)
            print(f"âœ… æŸ¥è¯¢æ‰©å±•åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {QUERY_EXPANSION_MODEL})")
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢æ‰©å±•åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å°†ä¸ä½¿ç”¨æŸ¥è¯¢æ‰©å±•")
            self.query_expansion_model = None
    
    def load_documents(self, urls=None):
        """ä»URLåŠ è½½æ–‡æ¡£"""
        if urls is None:
            urls = KNOWLEDGE_BASE_URLS
        
        print(f"æ­£åœ¨åŠ è½½ {len(urls)} ä¸ªURLçš„æ–‡æ¡£...")
        docs = [WebBaseLoader(url).load() for url in urls]
        docs_list = [item for sublist in docs for item in sublist]
        print(f"æˆåŠŸåŠ è½½ {len(docs_list)} ä¸ªæ–‡æ¡£")
        return docs_list
    
    def split_documents(self, docs):
        """å°†æ–‡æ¡£åˆ†å‰²æˆå—"""
        print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        doc_splits = self.text_splitter.split_documents(docs)
        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…± {len(doc_splits)} ä¸ªæ–‡æ¡£å—")
        return doc_splits
    
    def initialize_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥"""
        if self.vectorstore:
            return

        print("æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...")
        
        try:
            connection_args: Dict[str, Any] = {}
            is_local_file = False

            if MILVUS_URI and len(MILVUS_URI.strip()) > 0:
                is_local_file = not (MILVUS_URI.startswith("http://") or MILVUS_URI.startswith("https://"))
                real_uri = MILVUS_URI

                if is_local_file:
                    import os
                    if not os.path.isabs(real_uri):
                        real_uri = os.path.abspath(real_uri)
                        print(f"ğŸ“‚ å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„: {real_uri}")

                    parent_dir = os.path.dirname(real_uri)
                    if parent_dir and not os.path.exists(parent_dir):
                        print(f"ğŸ“‚ åˆ›å»º Milvus å­˜å‚¨ç›®å½•: {parent_dir}")
                        os.makedirs(parent_dir, exist_ok=True)

                mode_name = "Lite (Local File)" if is_local_file else "Cloud (HTTP)"
                print(f"ğŸ”„ æ­£åœ¨è¿æ¥ Milvus {mode_name} ({real_uri})...")
                connection_args["uri"] = real_uri

                if not is_local_file and MILVUS_PASSWORD:
                    connection_args["token"] = MILVUS_PASSWORD
            else:
                print(f"ğŸ”„ æ­£åœ¨è¿æ¥ Milvus Server ({MILVUS_HOST}:{MILVUS_PORT})...")
                connection_args = {
                    "host": MILVUS_HOST,
                    "port": MILVUS_PORT,
                    "user": MILVUS_USER,
                    "password": MILVUS_PASSWORD,
                }

            try:
                from pymilvus import connections, utility

                print("ğŸ”Œ å°è¯•å»ºç«‹ pymilvus å…¨å±€è¿æ¥ (Alias: default)...")
                if connections.has_connection("default"):
                    connections.disconnect("default")

                connections.connect(alias="default", **connection_args)
                print("âœ… pymilvus å…¨å±€è¿æ¥å»ºç«‹æˆåŠŸ")

                if utility.has_collection(COLLECTION_NAME, using="default"):
                    print(f"âœ… é›†åˆ {COLLECTION_NAME} å·²å­˜åœ¨")
                else:
                    print(f"â„¹ï¸ é›†åˆ {COLLECTION_NAME} ä¸å­˜åœ¨ï¼Œå°†ç”± Milvus ç±»è‡ªåŠ¨åˆ›å»º")
            except ImportError:
                print("âš ï¸ æœªæ‰¾åˆ° pymilvus åº“ï¼Œè·³è¿‡æ˜¾å¼è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ æ˜¾å¼è¿æ¥å°è¯•å¤±è´¥: {e}")

            final_index_type = MILVUS_INDEX_TYPE
            final_index_params = MILVUS_INDEX_PARAMS

            if is_local_file and MILVUS_INDEX_TYPE == "HNSW":
                print("âš ï¸ æ£€æµ‹åˆ° Milvus Lite (æœ¬åœ°æ¨¡å¼)ï¼ŒHNSW ç´¢å¼•ä¸å—æ”¯æŒï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º AUTOINDEX")
                final_index_type = "AUTOINDEX"
                final_index_params = {}

            self.vectorstore = Milvus(
                embedding_function=self.embeddings,
                collection_name=COLLECTION_NAME,
                connection_args={"alias": "default"},
                index_params={
                    "metric_type": "L2",
                    "index_type": final_index_type,
                    "params": final_index_params,
                },
                search_params={
                    "metric_type": "L2",
                    "params": MILVUS_SEARCH_PARAMS,
                },
                drop_old=False,
                auto_id=True,
            )
            print("âœ… Milvus å‘é‡æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except ImportError:
            print("âŒ æœªå®‰è£… pymilvusï¼Œè¯·è¿è¡Œ: pip install pymilvus")
            raise
        except Exception as e:
            print(f"âŒ Milvus è¿æ¥å¤±è´¥: {e}")
            raise

        retriever_kwargs: Dict[str, Any] = {}
        self.retriever = self.vectorstore.as_retriever(search_kwargs=retriever_kwargs)

    def check_existing_urls(self, urls: List[str]) -> set:
        """æ£€æŸ¥å“ªäº›URLå·²ç»å­˜åœ¨äºå‘é‡åº“ä¸­"""
        if not self.vectorstore:
            return set()
            
        existing = set()
        print("æ­£åœ¨æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£...")
        try:
            # å°è¯•é€šè¿‡æ£€ç´¢æ¥æ£€æŸ¥
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ source å­—æ®µå¯ä»¥ä½œä¸ºè¿‡æ»¤æ¡ä»¶
            for url in urls:
                # ä½¿ç”¨ similarity_search ä½†å¸¦æœ‰ä¸¥æ ¼è¿‡æ»¤ï¼Œä¸”åªå–1æ¡
                # è¿™é‡Œçš„ query æ²¡å…³ç³»ï¼Œä¸»è¦çœ‹ filter
                try:
                    # æ³¨æ„ï¼šMilvus çš„ expr è¯­æ³•
                    expr = f'source == "{url}"'
                    res = self.vectorstore.similarity_search(
                        "test", 
                        k=1, 
                        expr=expr
                    )
                    if res:
                        existing.add(url)
                except Exception as e:
                    # å¦‚æœå¤±è´¥ï¼Œå¯èƒ½æ˜¯ schema é—®é¢˜ï¼Œå°è¯• metadata å­—æ®µ
                    try:
                        expr = f'metadata["source"] == "{url}"'
                        res = self.vectorstore.similarity_search(
                            "test", 
                            k=1, 
                            expr=expr
                        )
                        if res:
                            existing.add(url)
                    except:
                        pass
                        
            print(f"âœ… å‘ç° {len(existing)} ä¸ªå·²å­˜åœ¨çš„ URL")
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥ç°æœ‰URLå¤±è´¥: {e}")
            
        return existing

    def add_documents_to_vectorstore(self, doc_splits):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“"""
        if not doc_splits:
            return

        print(f"æ­£åœ¨æ·»åŠ  {len(doc_splits)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“...")
        if not self.vectorstore:
            self.initialize_vectorstore()
            
        # æ·»åŠ å…ƒæ•°æ®
        for doc in doc_splits:
            if 'source_type' not in doc.metadata:
                source = doc.metadata.get('source', '')
                if any(fmt in source.lower() for fmt in SUPPORTED_IMAGE_FORMATS):
                    doc.metadata['data_type'] = 'image'
                else:
                    doc.metadata['data_type'] = 'text'

        # 1. æ·»åŠ åˆ° Milvus (Vector)
        self.vectorstore.add_documents(doc_splits)
        
        # 2. æ·»åŠ åˆ° Elasticsearch (BM25)
        if self.es_client:
            print(f"æ­£åœ¨åŒæ­¥ {len(doc_splits)} ä¸ªæ–‡æ¡£åˆ° Elasticsearch...")
            try:
                from elasticsearch import helpers
                actions = []
                for doc in doc_splits:
                    action = {
                        "_index": ES_INDEX_NAME,
                        "_source": {
                            "text": doc.page_content,
                            "metadata": doc.metadata
                        }
                    }
                    actions.append(action)
                
                # ä½¿ç”¨ bulk API æ‰¹é‡ç´¢å¼•
                success, failed = helpers.bulk(self.es_client, actions, stats_only=True)
                print(f"âœ… Elasticsearch åŒæ­¥å®Œæˆ: {success} ä¸ªæ–‡æ¡£æˆåŠŸ, {failed} ä¸ªå¤±è´¥")
                # å¼ºåˆ¶åˆ·æ–°ä»¥ç«‹å³å¯è§
                self.es_client.indices.refresh(index=ES_INDEX_NAME)
            except Exception as e:
                print(f"âš ï¸ Elasticsearch åŒæ­¥å¤±è´¥: {e}")

        print("âœ… æ–‡æ¡£æ·»åŠ å®Œæˆ")
        
    def create_vectorstore(self, doc_splits, persist_directory=None):
        """(å·²å¼ƒç”¨) å…¼å®¹æ—§æ¥å£ï¼Œä½†ä½¿ç”¨æ–°é€»è¾‘"""
        print("âš ï¸ create_vectorstore å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ initialize_vectorstore å’Œ add_documents_to_vectorstore")
        self.initialize_vectorstore()
        if doc_splits:
            self.add_documents_to_vectorstore(doc_splits)
        return self.vectorstore, self.retriever

    def get_all_documents_from_vectorstore(self, limit: Optional[int] = None) -> List[Document]:
        """ä»å·²æŒä¹…åŒ–çš„å‘é‡æ•°æ®åº“è¯»å–æ‰€æœ‰æ–‡æ¡£å†…å®¹å¹¶æ„é€  Document åˆ—è¡¨"""
        if not self.vectorstore:
            return []
        
        docs: List[Document] = []
        try:
            # 1. å°è¯•é€‚é… Milvus (LangChain å°è£…)
            # åˆ¤æ–­æ˜¯å¦æ˜¯ Milvus å®ä¾‹
            is_milvus = "Milvus" in self.vectorstore.__class__.__name__
            
            if is_milvus:
                try:
                    # è·å– pymilvus Collection å¯¹è±¡
                    # langchain_community ä½¿ç”¨ .col, langchain_milvus ä½¿ç”¨ .collection (æˆ–å…¶ä»–ï¼Œè§†ç‰ˆæœ¬è€Œå®š)
                    col = getattr(self.vectorstore, "col", None)
                    if not col:
                        col = getattr(self.vectorstore, "collection", None)
                    
                    if col:
                        # è·å– schema ä¿¡æ¯
                        pk_field = "pk"
                        if hasattr(col, "schema") and hasattr(col.schema, "primary_field"):
                            pk_field = col.schema.primary_field.name
                        
                        # ç¡®å®šæ–‡æœ¬å­—æ®µå
                        text_field = self.vectorstore._text_field if hasattr(self.vectorstore, "_text_field") else "text"
                        
                        # æ„é€ æŸ¥è¯¢
                        # æ³¨æ„ï¼šMilvus query limit é™åˆ¶ (é»˜è®¤ 16384)
                        # å¯¹äºç™¾ä¸‡çº§æ•°æ®ï¼Œåº”è¯¥ä½¿ç”¨ iteratorï¼Œä½† pymilvus çš„ iterator æ¥å£éšç‰ˆæœ¬å˜åŒ–
                        # è¿™é‡Œå…ˆå°è¯•è·å–å°½å¯èƒ½å¤šçš„æ•°æ® (æ¯”å¦‚ 10000 æ¡ä½œä¸ºç¤ºä¾‹ï¼Œæˆ–è€…åˆ†æ‰¹)
                        
                        query_limit = limit if limit else 16384
                        
                        # å°è¯•æŸ¥è¯¢
                        # å‡è®¾ PK æ˜¯ INT64ï¼Œä½¿ç”¨ >= 0 åŒ¹é…æ‰€æœ‰
                        expr = f"{pk_field} >= 0"
                        # å¦‚æœ PK æ˜¯å­—ç¬¦ä¸² (VARCHAR)ï¼Œä½¿ç”¨ != ""
                        # è¿™é‡Œæˆ‘ä»¬ç®€å•å°è¯•ï¼Œå¦‚æœä¸æˆåŠŸåˆ™æ•è·å¼‚å¸¸
                        
                        res = col.query(
                            expr=expr,
                            output_fields=[text_field, "source", "data_type"], # è·å–å¿…è¦çš„å­—æ®µ
                            limit=query_limit
                        )
                        
                        for item in res:
                            content = item.get(text_field, "")
                            # æ„é€  metadata (æ’é™¤æ–‡æœ¬å’ŒPK)
                            meta = {k: v for k, v in item.items() if k != text_field and k != pk_field}
                            docs.append(Document(page_content=content, metadata=meta))
                        
                        if len(docs) > 0:
                            print(f"âœ… ä» Milvus åŠ è½½äº† {len(docs)} æ¡æ–‡æ¡£ç”¨äºæ„å»º BM25")
                            return docs
                except Exception as milvus_e:
                    print(f"âš ï¸ å°è¯•ä» Milvus è¯»å–æ•°æ®å¤±è´¥: {milvus_e}")
                    # Fallthrough to other methods or return empty

            # 2. å°è¯•é€‚é… Chroma (åŸä»£ç é€»è¾‘)
            if hasattr(self.vectorstore, "_collection") and hasattr(self.vectorstore._collection, "get"):
                data = self.vectorstore._collection.get(include=["documents", "metadatas"])  # type: ignore
                docs_raw = data.get("documents") or []
                metas = data.get("metadatas") or []
                
                # Chroma å¯èƒ½ä¼šè¿”å› None
                if docs_raw:
                    for i, content in enumerate(docs_raw):
                        if content:
                            meta = metas[i] if metas and i < len(metas) else {}
                            docs.append(Document(page_content=content, metadata=meta))
                    
                    if limit:
                        return docs[:limit]
                    return docs

        except Exception as e:
            print(f"âš ï¸ ä»å‘é‡åº“åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            
        return docs
    
    def setup_knowledge_base(self, urls=None, enable_graphrag=False):
        """è®¾ç½®å®Œæ•´çš„çŸ¥è¯†åº“ï¼ˆåŠ è½½ã€åˆ†å‰²ã€å‘é‡åŒ–ï¼‰
        
        Args:
            urls: æ–‡æ¡£URLåˆ—è¡¨
            enable_graphrag: æ˜¯å¦å¯ç”¨GraphRAGç´¢å¼•
            
        Returns:
            vectorstore, retriever, doc_splits
        """
        if urls is None:
            urls = KNOWLEDGE_BASE_URLS
            
        # 1. åˆå§‹åŒ–å‘é‡åº“è¿æ¥
        self.initialize_vectorstore()
        
        # 2. æ£€æŸ¥å·²å­˜åœ¨çš„ URL (å»é‡)
        existing_urls = self.check_existing_urls(urls)
        new_urls = [url for url in urls if url not in existing_urls]
        
        doc_splits = []
        if new_urls:
            print(f"ğŸ”„ å‘ç° {len(new_urls)} ä¸ªæ–° URLï¼Œå¼€å§‹å¤„ç†...")
            docs = self.load_documents(new_urls)
            doc_splits = self.split_documents(docs)
            self.add_documents_to_vectorstore(doc_splits)
        else:
            print("âœ… æ‰€æœ‰ URL å·²å­˜åœ¨ï¼Œè·³è¿‡æ–‡æ¡£åŠ è½½å’Œå‘é‡åŒ–")
            
        # 3. åˆå§‹åŒ–æ··åˆæ£€ç´¢ (Elasticsearch BM25)
        # æ”¹è¿›æ–¹æ¡ˆï¼šä½¿ç”¨ Elasticsearch æ›¿ä»£å†…å­˜ç‰ˆ BM25ï¼Œæ”¯æŒç™¾ä¸‡çº§æ•°æ®
        if ENABLE_HYBRID_SEARCH:
            print("æ­£åœ¨åˆå§‹åŒ–æ··åˆæ£€ç´¢ (Elasticsearch BM25)...")
            try:
                if self.es_client:
                    # æ£€æŸ¥ ES æ˜¯å¦æœ‰æ•°æ®
                    count = 0
                    try:
                        if self.es_client.indices.exists(index=ES_INDEX_NAME):
                             count = self.es_client.count(index=ES_INDEX_NAME)["count"]
                    except Exception as e:
                        print(f"âš ï¸ æ£€æŸ¥ Elasticsearch ç´¢å¼•å¤±è´¥: {e}")
                        
                    print(f"ğŸ“Š Elasticsearch ç´¢å¼•å½“å‰åŒ…å« {count} ä¸ªæ–‡æ¡£")
                    
                    # è‡ªåŠ¨è¿ç§»é€»è¾‘ï¼šå¦‚æœ ES ä¸ºç©ºä½† VectorStore ä¸ä¸ºç©ºï¼Œä¸”æœ¬æ¬¡æ²¡æœ‰æ–°æ–‡æ¡£å…¥åº“(é¿å…é‡å¤)ï¼Œå°è¯•åŒæ­¥
                    # æ³¨æ„ï¼šå¦‚æœ doc_splits æœ‰å€¼ï¼Œå®ƒä»¬å·²ç»åœ¨ add_documents_to_vectorstore ä¸­è¢«åŒæ­¥äº†ï¼Œæ‰€ä»¥è¿™é‡Œåªéœ€å…³æ³¨æ—§æ•°æ®
                    if count == 0 and len(existing_urls) > 0:
                        print("âš ï¸ Elasticsearch ç´¢å¼•ä¸ºç©ºï¼Œä½†å‘é‡åº“ä¸­æœ‰æ•°æ®ã€‚æ­£åœ¨å°è¯•åŒæ­¥æ•°æ®...")
                        all_docs = self.get_all_documents_from_vectorstore()
                        if all_docs:
                             print(f"ğŸ”„ æ­£åœ¨å°† {len(all_docs)} ä¸ªå­˜é‡æ–‡æ¡£åŒæ­¥åˆ° Elasticsearch...")
                             from elasticsearch import helpers
                             actions = []
                             for doc in all_docs:
                                 action = {
                                     "_index": ES_INDEX_NAME,
                                     "_source": {
                                         "text": doc.page_content,
                                         "metadata": doc.metadata
                                     }
                                 }
                                 actions.append(action)
                             success, _ = helpers.bulk(self.es_client, actions, stats_only=True)
                             print(f"âœ… å·²åŒæ­¥ {success} ä¸ªæ–‡æ¡£åˆ° Elasticsearch")
                             self.es_client.indices.refresh(index=ES_INDEX_NAME)
                
                    self.bm25_retriever = ElasticsearchBM25Retriever(
                        client=self.es_client,
                        index_name=ES_INDEX_NAME,
                        k=KEYWORD_SEARCH_K
                    )
                    print("âœ… Elasticsearch BM25 æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("âš ï¸ Elasticsearch æœªè¿æ¥ï¼Œè·³è¿‡ BM25 åˆå§‹åŒ–")
                    self.bm25_retriever = None

                if self.bm25_retriever:
                    self.ensemble_retriever = CustomEnsembleRetriever(
                        retrievers=[self.retriever, self.bm25_retriever],
                        weights=[HYBRID_SEARCH_WEIGHTS["vector"], HYBRID_SEARCH_WEIGHTS["keyword"]]
                    )
                    print("âœ… æ··åˆæ£€ç´¢åˆå§‹åŒ–æˆåŠŸ")
                    
            except Exception as e:
                print(f"âš ï¸ æ··åˆæ£€ç´¢åˆå§‹åŒ–å¤±è´¥: {e}")
                self.ensemble_retriever = None
        
        # è¿”å› doc_splitsç”¨äºGraphRAGç´¢å¼• (æ³¨æ„ï¼šè¿™é‡Œåªè¿”å›äº†æ–°å¢çš„)
        return self.vectorstore, self.retriever, doc_splits
    
    async def async_expand_query(self, query: str) -> List[str]:
        """å¼‚æ­¥æ‰©å±•æŸ¥è¯¢"""
        if not self.query_expansion_model:
            return [query]
            
        try:
            # ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
            prompt = QUERY_EXPANSION_PROMPT.format(query=query)
            expanded_queries_text = await self.query_expansion_model.ainvoke(prompt)
            
            # è§£ææ‰©å±•æŸ¥è¯¢
            expanded_queries = [query]  # åŒ…å«åŸå§‹æŸ¥è¯¢
            for line in expanded_queries_text.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('//'):
                    # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
                    if line[0].isdigit() and '.' in line[:5]:
                        line = line.split('.', 1)[1].strip()
                    expanded_queries.append(line)
            
            # é™åˆ¶æ‰©å±•æŸ¥è¯¢æ•°é‡
            return expanded_queries[:MAX_EXPANDED_QUERIES + 1]
        except Exception as e:
            print(f"âš ï¸ å¼‚æ­¥æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [query]

    async def async_hybrid_retrieve(self, query: str, top_k: int = 5, filter_type: str = "text") -> List:
        """å¼‚æ­¥æ··åˆæ£€ç´¢
        
        Args:
            filter_type: æ•°æ®ç±»å‹è¿‡æ»¤ï¼Œ"text" (é»˜è®¤), "image", æˆ– "all" (ä¸è¿‡æ»¤)
        """
        # æ„å»ºæœç´¢å‚æ•°
        search_kwargs = {}
        if filter_type != "all" and ENABLE_MULTIMODAL:
            search_kwargs["expr"] = f"data_type == '{filter_type}'"

        if not ENABLE_HYBRID_SEARCH or not self.ensemble_retriever:
            # çº¯å‘é‡æ£€ç´¢ï¼Œç›´æ¥æ”¯æŒ search_kwargs
            if self.vectorstore:
                return await self.vectorstore.asimilarity_search(query, k=top_k, **search_kwargs)
            return await self.retriever.ainvoke(query)
            
        try:
            # æ··åˆæ£€ç´¢
            # æ³¨æ„ï¼šç›®å‰ CustomEnsembleRetriever çš„ invoke/ainvoke å°šæœªé€ä¼  search_kwargs
            # ä¸ºäº†è®©æ··åˆæ£€ç´¢ä¹Ÿäº«å—åˆ°è¿‡æ»¤ä¼˜åŒ–ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ CustomEnsembleRetriever æˆ–è€…åœ¨è¿™é‡Œå¤„ç†
            # é‰´äº CustomEnsembleRetriever æ¯”è¾ƒç®€å•ï¼Œæˆ‘ä»¬å‡è®¾å®ƒä¸»è¦ç”¨äºæ–‡æœ¬
            # å¦‚æœéœ€è¦ä¸¥æ ¼è¿‡æ»¤ï¼Œæœ€å¥½åœ¨ vectorstore å±‚é¢å¤„ç†
            
            # ä¸´æ—¶æ–¹æ¡ˆï¼šå¦‚æœæ˜¯æ··åˆæ£€ç´¢ä¸”éœ€è¦è¿‡æ»¤ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¼ é€’ç»™ retriever
            # ä½†æ ‡å‡† retriever æ¥å£ä¸æ”¯æŒåŠ¨æ€ä¼ å‚ã€‚
            # ç­–ç•¥ï¼šå¦‚æœ filter_type æ˜¯ text (é»˜è®¤)ï¼Œä¸”æˆ‘ä»¬åœ¨ init æ—¶å·²ç»è®¾ç½®äº†é»˜è®¤ä¸ä¸¥æ ¼è¿‡æ»¤ï¼Œ
            # è¿™é‡Œå…¶å®æ— æ³•åŠ¨æ€æ”¹å˜ retriever çš„è¡Œä¸ºï¼Œé™¤éæˆ‘ä»¬é‡æ–°ç”Ÿæˆä¸€ä¸ª retriever æˆ–è€…ä¿®æ”¹ retriever.search_kwargs
            
            # åŠ¨æ€ä¿®æ”¹ retriever çš„ search_kwargs (è¿™æ˜¯ LangChain retriever çš„ç‰¹æ€§)
            if filter_type != "all" and ENABLE_MULTIMODAL:
                self.retriever.search_kwargs["expr"] = f"data_type == '{filter_type}'"
            else:
                self.retriever.search_kwargs.pop("expr", None)
                
            results = await self.ensemble_retriever.ainvoke(query)
            return results[:top_k]
        except Exception as e:
            print(f"âš ï¸ å¼‚æ­¥æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°å‘é‡æ£€ç´¢")
            if self.vectorstore:
                return await self.vectorstore.asimilarity_search(query, k=top_k, **search_kwargs)
            return await self.retriever.ainvoke(query)

    async def async_enhanced_retrieve(self, query: str, top_k: int = 5, rerank_candidates: int = 20, 
                         image_paths: List[str] = None, use_query_expansion: bool = None):
        """å¼‚æ­¥å¢å¼ºæ£€ç´¢"""
        import asyncio
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
        if use_query_expansion is None:
            use_query_expansion = ENABLE_QUERY_EXPANSION
            
        # å¦‚æœå¯ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        if use_query_expansion:
            expanded_queries = await self.async_expand_query(query)
            print(f"æŸ¥è¯¢æ‰©å±•: {len(expanded_queries)} ä¸ªæŸ¥è¯¢")
        else:
            expanded_queries = [query]
            
        # å¤šæ¨¡æ€æ£€ç´¢ï¼ˆæš‚æ—¶ä¿æŒåŒæ­¥ï¼Œä½¿ç”¨çº¿ç¨‹æ± ï¼‰
        if image_paths and ENABLE_MULTIMODAL:
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(None, self.multimodal_retrieve, query, image_paths, top_k)
            
        # æ··åˆæ£€ç´¢æˆ–å‘é‡æ£€ç´¢
        all_candidate_docs = []
        
        # å†³å®šè¿‡æ»¤ç­–ç•¥
        # é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœåªæ˜¯æ–‡æœ¬æŸ¥è¯¢ï¼Œä¸ºäº†æ€§èƒ½ä¼˜åŒ–ï¼Œæˆ‘ä»¬åªæ£€ç´¢æ–‡æœ¬æ•°æ®
        # å¦‚æœæä¾›äº†å›¾åƒï¼Œæˆ–è€…ç”¨æˆ·æ˜¾å¼è¦æ±‚ï¼Œå¯ä»¥æ”¾å¼€é™åˆ¶
        filter_type = "text" # é»˜è®¤åªæœæ–‡æœ¬ï¼Œå®ç°ç™¾ä¸‡çº§æ•°æ®çš„æ€§èƒ½ä¼˜åŒ–
        if image_paths:
            filter_type = "all" # è·¨æ¨¡æ€æ—¶æœæ‰€æœ‰
            
        # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼ (ä»…ç”¨äºç›´æ¥è°ƒç”¨ vectorstore çš„æƒ…å†µï¼Œasync_hybrid_retrieve å†…éƒ¨å·²å¤„ç†)
        search_kwargs = {}
        if filter_type != "all" and ENABLE_MULTIMODAL:
             search_kwargs["expr"] = f"data_type == '{filter_type}'"

        async def retrieve_single(q):
            if ENABLE_HYBRID_SEARCH:
                # ä½¿ç”¨æ”¯æŒåŠ¨æ€è¿‡æ»¤çš„ hybrid retrieve
                 docs = await self.async_hybrid_retrieve(q, rerank_candidates, filter_type=filter_type)
            else:
                # ä½¿ç”¨å¸¦æœ‰è¿‡æ»¤æ¡ä»¶çš„æ£€ç´¢
                if self.vectorstore:
                    docs = await self.vectorstore.asimilarity_search(
                        q, 
                        k=rerank_candidates,
                        **search_kwargs # ä¼ å…¥ expr
                    )
                else:
                    # Fallback
                    docs = await self.retriever.ainvoke(q)
                
                if len(docs) > rerank_candidates:
                    docs = docs[:rerank_candidates]
            return docs

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢çš„æ£€ç´¢
        results = await asyncio.gather(*[retrieve_single(q) for q in expanded_queries])
        
        for docs in results:
            all_candidate_docs.extend(docs)
            
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        unique_docs = []
        seen_content = set()
        for doc in all_candidate_docs:
            content = doc.page_content
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)
                
        print(f"æ£€ç´¢è·å¾— {len(unique_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # é‡æ’ï¼ˆå¦‚æœé‡æ’å™¨å¯ç”¨ï¼‰
        # æ³¨æ„ï¼šé‡æ’é€šå¸¸æ˜¯è®¡ç®—å¯†é›†å‹ï¼Œå»ºè®®æ”¾å…¥çº¿ç¨‹æ± 
        if self.reranker and len(unique_docs) > top_k:
            try:
                loop = asyncio.get_running_loop()
                # rerank æ–¹æ³•å†…éƒ¨å¯èƒ½ä¹Ÿæ¯”è¾ƒè€—æ—¶
                reranked_results = await loop.run_in_executor(
                    None, 
                    self.reranker.rerank, 
                    query, unique_docs, top_k
                )
                final_docs = [doc for doc, score in reranked_results]
                scores = [score for doc, score in reranked_results]
                
                print(f"é‡æ’åè¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
                print(f"é‡æ’åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
                
                return final_docs
            except Exception as e:
                print(f"âš ï¸ é‡æ’å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                return unique_docs[:top_k]
        else:
            return unique_docs[:top_k]
    
    def expand_query(self, query: str) -> List[str]:
        """æ‰©å±•æŸ¥è¯¢ï¼Œç”Ÿæˆç›¸å…³æŸ¥è¯¢"""
        if not self.query_expansion_model:
            return [query]
            
        try:
            # ä½¿ç”¨LLMç”Ÿæˆæ‰©å±•æŸ¥è¯¢
            prompt = QUERY_EXPANSION_PROMPT.format(query=query)
            expanded_queries_text = self.query_expansion_model.invoke(prompt)
            
            # è§£ææ‰©å±•æŸ¥è¯¢
            expanded_queries = [query]  # åŒ…å«åŸå§‹æŸ¥è¯¢
            for line in expanded_queries_text.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('//'):
                    # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€
                    if line[0].isdigit() and '.' in line[:5]:
                        line = line.split('.', 1)[1].strip()
                    expanded_queries.append(line)
            
            # é™åˆ¶æ‰©å±•æŸ¥è¯¢æ•°é‡
            return expanded_queries[:MAX_EXPANDED_QUERIES + 1]  # +1 å› ä¸ºåŒ…å«åŸå§‹æŸ¥è¯¢
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
            return [query]
    
    def encode_image(self, image_path: str) -> np.ndarray:
        """ç¼–ç å›¾åƒä¸ºåµŒå…¥å‘é‡"""
        if not self.image_embeddings_model:
            raise ValueError("å¤šæ¨¡æ€æ”¯æŒæœªåˆå§‹åŒ–")
            
        try:
            # åŠ è½½å¹¶å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            inputs = self.image_processor(images=image, return_tensors="pt")
            
            # è·å–å›¾åƒåµŒå…¥
            with torch.no_grad():
                image_features = self.image_embeddings_model.get_image_features(**inputs)
                # æ ‡å‡†åŒ–åµŒå…¥å‘é‡
                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
                
            return image_features.cpu().numpy().flatten()
        except Exception as e:
            print(f"âš ï¸ å›¾åƒç¼–ç å¤±è´¥: {e}")
            raise
    
    def multimodal_retrieve(self, query: str, image_paths: List[str] = None, top_k: int = 5) -> List:
        """å¤šæ¨¡æ€æ£€ç´¢ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒ"""
        if not ENABLE_MULTIMODAL or not self.image_embeddings_model:
            # å¦‚æœå¤šæ¨¡æ€æœªå¯ç”¨ï¼Œå›é€€åˆ°æ–‡æœ¬æ£€ç´¢
            return self.hybrid_retrieve(query, top_k) if ENABLE_HYBRID_SEARCH else self.retriever.invoke(query)[:top_k]
        
        # 1. æ–‡æœ¬æŸ¥è¯¢ (Text-to-Text & Text-to-Image)
        # å¦‚æœæä¾›äº†æ–‡æœ¬æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒèƒ½æ£€ç´¢åˆ°æ–‡æœ¬å’Œç›¸å…³å›¾åƒ
        # æ­¤æ—¶ä¸åº”è¯¥é™åˆ¶ data_typeï¼Œæˆ–è€…åº”è¯¥æ˜¾å¼åŒ…å«ä¸¤è€…
        
        # å¦‚æœæ²¡æœ‰æä¾›å›¾åƒï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æŸ¥è¯¢ï¼Œä½†ä¹Ÿå¯èƒ½æƒ³æœå›¾
        # è¿™é‡Œæˆ‘ä»¬è®© self.retriever (æˆ– hybrid) è´Ÿè´£æ‰€æœ‰æ¨¡æ€çš„æ£€ç´¢
        # (å‰ææ˜¯å®ƒä»¬éƒ½åœ¨åŒä¸€ä¸ªå‘é‡ç©ºé—´ï¼ŒCLIP å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹)
        text_docs = []
        if query:
             text_docs = self.hybrid_retrieve(query, top_k) if ENABLE_HYBRID_SEARCH else self.retriever.invoke(query)[:top_k]
        
        # å¦‚æœæ²¡æœ‰æä¾›å›¾åƒè¾“å…¥ï¼Œç›´æ¥è¿”å›æ–‡æœ¬æŸ¥è¯¢çš„ç»“æœ
        if not image_paths:
            return text_docs
            
        try:
            # 2. å›¾åƒæŸ¥è¯¢ (Image-to-Text & Image-to-Image)
            image_results = []
            for image_path in image_paths:
                # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
                file_ext = image_path.split('.')[-1].lower()
                if file_ext not in SUPPORTED_IMAGE_FORMATS:
                    print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {file_ext}")
                    continue
                    
                # ç¼–ç å›¾åƒ
                image_embedding = self.encode_image(image_path)
                
                # ä½¿ç”¨å›¾åƒåµŒå…¥è¿›è¡Œæ£€ç´¢
                if self.vectorstore:
                    # å›¾åƒå¯ä»¥æ£€ç´¢æ–‡æœ¬æè¿°ï¼Œä¹Ÿå¯ä»¥æ£€ç´¢ç›¸ä¼¼å›¾åƒ
                    # è¿™é‡Œæˆ‘ä»¬ä¸åšé™åˆ¶ï¼Œæ£€ç´¢æ‰€æœ‰ç±»å‹
                    img_docs = self.vectorstore.similarity_search_by_vector(
                        embedding=image_embedding,
                        k=top_k
                    )
                    image_results.extend(img_docs)
                
            # åˆå¹¶æ–‡æœ¬æŸ¥è¯¢ç»“æœå’Œå›¾åƒæŸ¥è¯¢ç»“æœ
            # ç®€å•åˆå¹¶å¹¶å»é‡
            all_docs = text_docs + image_results
            
            # å»é‡
            unique_docs = []
            seen_content = set()
            for doc in all_docs:
                content = doc.page_content
                if content not in seen_content:
                    seen_content.add(content)
                    unique_docs.append(doc)
            
            final_docs = unique_docs[:top_k]
            
            print(f"âœ… å¤šæ¨¡æ€æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_docs)} ä¸ªç»“æœ")
            return final_docs
        except Exception as e:
            print(f"âš ï¸ å¤šæ¨¡æ€æ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°æ–‡æœ¬æ£€ç´¢")
            return text_docs
    
    def hybrid_retrieve(self, query: str, top_k: int = 5) -> List:
        """æ··åˆæ£€ç´¢ï¼Œç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢"""
        if not ENABLE_HYBRID_SEARCH or not self.ensemble_retriever:
            # å¦‚æœæ··åˆæ£€ç´¢æœªå¯ç”¨ï¼Œå›é€€åˆ°å‘é‡æ£€ç´¢
            return self.retriever.invoke(query)[:top_k]
            
        try:
            # ä½¿ç”¨é›†æˆæ£€ç´¢å™¨è¿›è¡Œæ··åˆæ£€ç´¢
            results = self.ensemble_retriever.invoke(query)
            return results[:top_k]
        except Exception as e:
            print(f"âš ï¸ æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°å‘é‡æ£€ç´¢")
            return self.retriever.invoke(query)[:top_k]
    
    def enhanced_retrieve(self, query: str, top_k: int = 5, rerank_candidates: int = 20, 
                         image_paths: List[str] = None, use_query_expansion: bool = None):
        """å¢å¼ºæ£€ç´¢ï¼šå…ˆæ£€ç´¢æ›´å¤šå€™é€‰ï¼Œç„¶åé‡æ’ï¼Œæ”¯æŒæŸ¥è¯¢æ‰©å±•å’Œå¤šæ¨¡æ€
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            rerank_candidates: é‡æ’å‰çš„å€™é€‰æ–‡æ¡£æ•°é‡
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨ï¼Œç”¨äºå¤šæ¨¡æ€æ£€ç´¢
            use_query_expansion: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
        """
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ‰©å±•
        if use_query_expansion is None:
            use_query_expansion = ENABLE_QUERY_EXPANSION
            
        # å¦‚æœå¯ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œç”Ÿæˆæ‰©å±•æŸ¥è¯¢
        if use_query_expansion:
            expanded_queries = self.expand_query(query)
            print(f"æŸ¥è¯¢æ‰©å±•: {len(expanded_queries)} ä¸ªæŸ¥è¯¢")
        else:
            expanded_queries = [query]
            
        # å¤šæ¨¡æ€æ£€ç´¢ï¼ˆå¦‚æœæä¾›äº†å›¾åƒï¼‰
        if image_paths and ENABLE_MULTIMODAL:
            return self.multimodal_retrieve(query, image_paths, top_k)
            
        # æ··åˆæ£€ç´¢æˆ–å‘é‡æ£€ç´¢
        all_candidate_docs = []
        for expanded_query in expanded_queries:
            if ENABLE_HYBRID_SEARCH:
                # ä½¿ç”¨æ··åˆæ£€ç´¢
                docs = self.hybrid_retrieve(expanded_query, rerank_candidates)
            else:
                # ä½¿ç”¨å‘é‡æ£€ç´¢
                docs = self.retriever.invoke(expanded_query)
                if len(docs) > rerank_candidates:
                    docs = docs[:rerank_candidates]
            
            all_candidate_docs.extend(docs)
            
        # å»é‡ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰
        unique_docs = []
        seen_content = set()
        for doc in all_candidate_docs:
            content = doc.page_content
            if content not in seen_content:
                seen_content.add(content)
                unique_docs.append(doc)
                
        print(f"æ£€ç´¢è·å¾— {len(unique_docs)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # é‡æ’ï¼ˆå¦‚æœé‡æ’å™¨å¯ç”¨ï¼‰
        if self.reranker and len(unique_docs) > top_k:
            try:
                reranked_results = self.reranker.rerank(query, unique_docs, top_k)
                final_docs = [doc for doc, score in reranked_results]
                scores = [score for doc, score in reranked_results]
                
                print(f"é‡æ’åè¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
                print(f"é‡æ’åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
                
                return final_docs
            except Exception as e:
                print(f"âš ï¸ é‡æ’å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                return unique_docs[:top_k]
        else:
            # ä¸é‡æ’æˆ–å€™é€‰æ•°é‡ä¸è¶³
            return unique_docs[:top_k]
    
    def compare_retrieval_methods(self, query: str, top_k: int = 5, image_paths: List[str] = None):
        """æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ"""
        if not self.retriever:
            return {}
        
        results = {
            'query': query,
            'image_paths': image_paths
        }
        
        # åŸå§‹æ£€ç´¢ (ä½¿ç”¨ invoke æ›¿ä»£ get_relevant_documents)
        original_docs = self.retriever.invoke(query)[:top_k]
        results['vector_retrieval'] = {
            'count': len(original_docs),
            'documents': [{
                'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                'metadata': getattr(doc, 'metadata', {})
            } for doc in original_docs]
        }
        
        # æ··åˆæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ENABLE_HYBRID_SEARCH and self.ensemble_retriever:
            hybrid_docs = self.hybrid_retrieve(query, top_k)
            results['hybrid_retrieval'] = {
                'count': len(hybrid_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in hybrid_docs]
            }
        
        # æŸ¥è¯¢æ‰©å±•æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ENABLE_QUERY_EXPANSION and self.query_expansion_model:
            expanded_docs = self.enhanced_retrieve(query, top_k, use_query_expansion=True)
            results['expanded_query_retrieval'] = {
                'count': len(expanded_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in expanded_docs]
            }
        
        # å¤šæ¨¡æ€æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰å›¾åƒï¼‰
        if ENABLE_MULTIMODAL and image_paths:
            multimodal_docs = self.multimodal_retrieve(query, image_paths, top_k)
            results['multimodal_retrieval'] = {
                'count': len(multimodal_docs),
                'documents': [{
                    'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                    'metadata': getattr(doc, 'metadata', {})
                } for doc in multimodal_docs]
            }
        
        # å¢å¼ºæ£€ç´¢ï¼ˆå¸¦é‡æ’ï¼‰
        enhanced_docs = self.enhanced_retrieve(query, top_k)
        results['enhanced_retrieval'] = {
            'count': len(enhanced_docs),
            'documents': [{
                'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content,
                'metadata': getattr(doc, 'metadata', {})
            } for doc in enhanced_docs]
        }
        
        # æ·»åŠ é…ç½®ä¿¡æ¯
        results['configuration'] = {
            'hybrid_search_enabled': ENABLE_HYBRID_SEARCH,
            'query_expansion_enabled': ENABLE_QUERY_EXPANSION,
            'multimodal_enabled': ENABLE_MULTIMODAL,
            'reranker_used': self.reranker is not None,
            'hybrid_weights': HYBRID_SEARCH_WEIGHTS if ENABLE_HYBRID_SEARCH else None,
            'multimodal_weights': MULTIMODAL_WEIGHTS if ENABLE_MULTIMODAL else None
        }
        
        return results

    def format_docs(self, docs):
        """æ ¼å¼åŒ–æ–‡æ¡£ç”¨äºç”Ÿæˆ"""
        return "\n\n".join(doc.page_content for doc in docs)


def initialize_document_processor():
    """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨å¹¶è®¾ç½®çŸ¥è¯†åº“"""
    print("ğŸš€ åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ (Milvus ç‰ˆ)...")
    processor = DocumentProcessor()
    
    # ç›´æ¥è®¾ç½®çŸ¥è¯†åº“
    # Milvus çš„è¿æ¥å’Œç´¢å¼•é€»è¾‘åœ¨ DocumentProcessor.create_vectorstore ä¸­å¤„ç†
    vectorstore, retriever, doc_splits = processor.setup_knowledge_base()
    
    return processor, vectorstore, retriever, doc_splits
