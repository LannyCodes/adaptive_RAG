"""
Kaggle å¿«é€Ÿå¯åŠ¨è„šæœ¬ - é¿å…é‡å¤ä¸‹è½½å¤§æ¨¡å‹
ä½¿ç”¨ä¼˜åŒ–çš„å°æ¨¡å‹é…ç½®ï¼Œå¤§å¹…å‡å°‘å¯åŠ¨æ—¶é—´

ä½¿ç”¨æ–¹æ³•:
åœ¨ Kaggle Notebook ç¬¬ä¸€ä¸ªå•å…ƒæ ¼è¿è¡Œ:
    exec(open('/kaggle/working/adaptive_RAG/KAGGLE_QUICK_START.py').read())
"""

import os
import subprocess
import sys
import time

print("ğŸš€ Kaggle å¿«é€Ÿå¯åŠ¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
print("="*70)

# ==================== é…ç½®åŒºåŸŸ ====================
REPO_URL = "https://github.com/LannyCodes/adaptive_RAG.git"
PROJECT_DIR = "/kaggle/working/adaptive_RAG"

# æ¨¡å‹é€‰æ‹©ï¼ˆæ ¹æ®éœ€æ±‚ä¿®æ”¹ï¼‰
# "phi"       - 1.6GB, 2-3åˆ†é’Ÿä¸‹è½½ï¼Œè´¨é‡å¥½ â­â­â­â­ ï¼ˆæ¨èï¼‰
# "tinyllama" - 600MB, 1åˆ†é’Ÿä¸‹è½½ï¼Œè´¨é‡ä¸­ç­‰ â­â­â­
# "qwen:0.5b" - 350MB, 30ç§’ä¸‹è½½ï¼Œè´¨é‡è¾ƒä½ â­â­
# "mistral"   - 4GB, 5-10åˆ†é’Ÿä¸‹è½½ï¼Œè´¨é‡æœ€å¥½ â­â­â­â­â­ ï¼ˆæ…¢ï¼‰

PREFERRED_MODEL = "phi"  # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œé€‰æ‹©æ¨¡å‹

print(f"\nğŸ“Œ é…ç½®:")
print(f"   â€¢ ä»“åº“: {REPO_URL}")
print(f"   â€¢ æ¨¡å‹: {PREFERRED_MODEL}")
print()

# ==================== æ­¥éª¤ 1: å…‹éš†é¡¹ç›® ====================
print("ğŸ“¦ æ­¥éª¤ 1/6: å…‹éš†é¡¹ç›®...")

os.chdir('/kaggle/working')

if os.path.exists(PROJECT_DIR):
    print("   âœ… é¡¹ç›®å·²å­˜åœ¨")
else:
    result = subprocess.run(['git', 'clone', REPO_URL], capture_output=True, text=True)
    if result.returncode == 0:
        print("   âœ… é¡¹ç›®å…‹éš†æˆåŠŸ")
    else:
        print(f"   âŒ å…‹éš†å¤±è´¥: {result.stderr}")
        sys.exit(1)

os.chdir(PROJECT_DIR)

# ==================== æ­¥éª¤ 2: ä¿®æ”¹é…ç½®ä½¿ç”¨å°æ¨¡å‹ ====================
print("\nâš™ï¸ æ­¥éª¤ 2/6: ä¼˜åŒ–æ¨¡å‹é…ç½®...")

config_file = 'config.py'

with open(config_file, 'r', encoding='utf-8') as f:
    content = f.read()

# æ›¿æ¢æ¨¡å‹é…ç½®
if 'LOCAL_LLM = "mistral"' in content:
    content = content.replace(
        'LOCAL_LLM = "mistral"',
        f'LOCAL_LLM = "{PREFERRED_MODEL}"  # Kaggleä¼˜åŒ–: ä½¿ç”¨æ›´å°çš„æ¨¡å‹'
    )
    
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"   âœ… å·²åˆ‡æ¢åˆ° {PREFERRED_MODEL} æ¨¡å‹")
else:
    print(f"   â„¹ï¸ é…ç½®å·²æ˜¯ä¼˜åŒ–æ¨¡å¼")

# ==================== æ­¥éª¤ 3: æ£€æŸ¥å¹¶å®‰è£… Ollama ====================
print("\nğŸ”§ æ­¥éª¤ 3/6: æ£€æŸ¥ Ollama...")

ollama_check = subprocess.run(['which', 'ollama'], capture_output=True)

if ollama_check.returncode == 0:
    print("   âœ… Ollama å·²å®‰è£…")
else:
    print("   ğŸ“¥ å®‰è£… Ollama...")
    subprocess.run('curl -fsSL https://ollama.com/install.sh | sh', shell=True)
    time.sleep(3)
    print("   âœ… Ollama å®‰è£…å®Œæˆ")

# éªŒè¯å®‰è£…
version_result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
if version_result.returncode == 0:
    print(f"   ğŸ“Œ {version_result.stdout.strip()}")

# ==================== æ­¥éª¤ 4: å¯åŠ¨ Ollama æœåŠ¡ ====================
print("\nğŸš€ æ­¥éª¤ 4/6: å¯åŠ¨ Ollama æœåŠ¡...")

# æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
ps_check = subprocess.run(['pgrep', '-f', 'ollama serve'], capture_output=True)

if ps_check.returncode == 0:
    print("   âœ… Ollama æœåŠ¡å·²è¿è¡Œ")
else:
    print("   ğŸ”„ å¯åŠ¨æœåŠ¡...")
    subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    time.sleep(15)
    
    # éªŒè¯
    import requests
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=10)
        if response.status_code == 200:
            print("   âœ… æœåŠ¡è¿è¡Œæ­£å¸¸")
    except:
        print("   âš ï¸ æœåŠ¡éªŒè¯å¤±è´¥ï¼Œä½†å¯èƒ½ä»åœ¨å¯åŠ¨ä¸­...")

# ==================== æ­¥éª¤ 5: ä¸‹è½½ä¼˜åŒ–çš„æ¨¡å‹ ====================
print(f"\nğŸ“¦ æ­¥éª¤ 5/6: ä¸‹è½½ {PREFERRED_MODEL} æ¨¡å‹...")

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
list_result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)

if PREFERRED_MODEL in list_result.stdout:
    print(f"   âœ… {PREFERRED_MODEL} æ¨¡å‹å·²å­˜åœ¨")
else:
    # æ˜¾ç¤ºé¢„è®¡æ—¶é—´
    time_estimates = {
        "qwen:0.5b": "çº¦30ç§’",
        "tinyllama": "çº¦1åˆ†é’Ÿ",
        "phi": "çº¦2-3åˆ†é’Ÿ",
        "mistral": "çº¦5-10åˆ†é’Ÿ"
    }
    
    estimated_time = time_estimates.get(PREFERRED_MODEL, "æœªçŸ¥")
    
    print(f"   ğŸ“¥ å¼€å§‹ä¸‹è½½ï¼ˆé¢„è®¡æ—¶é—´: {estimated_time}ï¼‰...")
    print(f"   â³ è¯·ç¨å€™...")
    
    start_time = time.time()
    
    pull_result = subprocess.run(
        ['ollama', 'pull', PREFERRED_MODEL],
        capture_output=True,
        text=True
    )
    
    elapsed = time.time() - start_time
    
    if pull_result.returncode == 0:
        print(f"   âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼ˆè€—æ—¶: {int(elapsed)}ç§’ï¼‰")
    else:
        print(f"   âš ï¸ ä¸‹è½½è­¦å‘Š: {pull_result.stderr[:200]}")

# ==================== æ­¥éª¤ 6: å®‰è£… Python ä¾èµ– ====================
print("\nğŸ“¦ æ­¥éª¤ 6/6: å®‰è£… Python ä¾èµ–...")

subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements_graphrag.txt', '-q'])
subprocess.run([sys.executable, '-m', 'pip', 'install', '-U', 
                'langchain', 'langchain-core', 'langchain-community', 
                'langchain-text-splitters', '-q'])

print("   âœ… ä¾èµ–å®‰è£…å®Œæˆ")

# ==================== è®¾ç½® Python è·¯å¾„ ====================
if PROJECT_DIR not in sys.path:
    sys.path.insert(0, PROJECT_DIR)

# ==================== å®Œæˆ ====================
print("\n" + "="*70)
print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼")
print("="*70)

print(f"\nğŸ“Š é…ç½®æ‘˜è¦:")
print(f"   â€¢ å·¥ä½œç›®å½•: {os.getcwd()}")
print(f"   â€¢ ä½¿ç”¨æ¨¡å‹: {PREFERRED_MODEL}")
print(f"   â€¢ Pythonè·¯å¾„: å·²æ·»åŠ ")

# æ˜¾ç¤ºæ¨¡å‹å¯¹æ¯”
print(f"\nğŸ“Œ æ¨¡å‹é€‰æ‹©è¯´æ˜:")
print("   â€¢ phi (å½“å‰) - å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ï¼Œæ¨èæ—¥å¸¸ä½¿ç”¨")
print("   â€¢ tinyllama - æœ€å¿«ä¸‹è½½ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•")
print("   â€¢ mistral - è´¨é‡æœ€é«˜ï¼Œä½†ä¸‹è½½æ…¢ï¼ˆä¸æ¨èKaggleï¼‰")

print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
print("   1. å¼€å§‹ GraphRAG ç´¢å¼•:")
print("      from document_processor import DocumentProcessor")
print("      from graph_indexer import GraphRAGIndexer")
print("      ")
print("      doc_processor = DocumentProcessor()")
print("      vectorstore, retriever, doc_splits = doc_processor.setup_knowledge_base(enable_graphrag=True)")
print("      ")
print("      indexer = GraphRAGIndexer()")
print("      graph = indexer.index_documents(doc_splits, batch_size=3)")
print()
print("   2. å¦‚éœ€åˆ‡æ¢æ¨¡å‹ï¼Œä¿®æ”¹è„šæœ¬é¡¶éƒ¨çš„ PREFERRED_MODEL å˜é‡")

print("\nâš ï¸ æç¤º:")
print(f"   â€¢ å½“å‰ä½¿ç”¨ {PREFERRED_MODEL} æ¨¡å‹ï¼Œæ¯” Mistral å¿« {2 if PREFERRED_MODEL == 'phi' else 5}x")
print("   â€¢ ä¼šè¯ç»“æŸåä»éœ€é‡æ–°ä¸‹è½½ï¼ˆä½†é€Ÿåº¦å·²å¤§å¹…æå‡ï¼‰")
print("   â€¢ å¦‚éœ€æœ€ä½³è´¨é‡ï¼Œæœ¬åœ°å¼€å‘æ—¶å¯ç”¨ Mistral")
