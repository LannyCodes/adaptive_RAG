"""
VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰æ¨¡å‹å®Œæ•´ç»“æ„è§£æ
åŒ…å«ç¼–ç å™¨ã€è§£ç å™¨ã€é‡å‚æ•°åŒ–æŠ€å·§å’ŒæŸå¤±å‡½æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class Encoder(nn.Module):
    """ç¼–ç å™¨ï¼šå°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´çš„å‡å€¼å’Œæ–¹å·®"""
    
    def __init__(self, input_dim=784, hidden_dims=[512, 256], latent_dim=20):
        super(Encoder, self).__init__()
        
        # ç¬¬1å±‚ï¼šè¾“å…¥å±‚ â†’ ç¬¬ä¸€ä¸ªéšè—å±‚
        self.fc1 = nn.Linear(input_dim, hidden_dims[0])
        
        # ç¬¬2å±‚ï¼šç¬¬ä¸€ä¸ªéšè—å±‚ â†’ ç¬¬äºŒä¸ªéšè—å±‚
        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])
        
        # ç¬¬3å±‚ï¼šç¬¬äºŒä¸ªéšè—å±‚ â†’ æ½œåœ¨ç©ºé—´å‡å€¼
        self.fc_mu = nn.Linear(hidden_dims[1], latent_dim)
        
        # ç¬¬4å±‚ï¼šç¬¬äºŒä¸ªéšè—å±‚ â†’ æ½œåœ¨ç©ºé—´å¯¹æ•°æ–¹å·®
        self.fc_logvar = nn.Linear(hidden_dims[1], latent_dim)
        
    def forward(self, x):
        print("\nğŸ” ç¼–ç å™¨å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼š")
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # Layer 1: è¾“å…¥ â†’ éšè—å±‚1
        h1 = F.relu(self.fc1(x))
        print(f"Layer 1 å: {h1.shape}")
        
        # Layer 2: éšè—å±‚1 â†’ éšè—å±‚2
        h2 = F.relu(self.fc2(h1))
        print(f"Layer 2 å: {h2.shape}")
        
        # Layer 3: è®¡ç®—å‡å€¼ Î¼
        mu = self.fc_mu(h2)
        print(f"å‡å€¼ Î¼ å½¢çŠ¶: {mu.shape}")
        
        # Layer 4: è®¡ç®—å¯¹æ•°æ–¹å·® log(ÏƒÂ²)
        logvar = self.fc_logvar(h2)
        print(f"å¯¹æ•°æ–¹å·® logvar å½¢çŠ¶: {logvar.shape}")
        
        return mu, logvar


class Decoder(nn.Module):
    """è§£ç å™¨ï¼šä»æ½œåœ¨ç©ºé—´é‡å»ºåŸå§‹æ•°æ®"""
    
    def __init__(self, latent_dim=20, hidden_dims=[256, 512], output_dim=784):
        super(Decoder, self).__init__()
        
        # ç¬¬1å±‚ï¼šæ½œåœ¨ç©ºé—´ â†’ ç¬¬ä¸€ä¸ªéšè—å±‚
        self.fc1 = nn.Linear(latent_dim, hidden_dims[0])
        
        # ç¬¬2å±‚ï¼šç¬¬ä¸€ä¸ªéšè—å±‚ â†’ ç¬¬äºŒä¸ªéšè—å±‚
        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])
        
        # ç¬¬3å±‚ï¼šç¬¬äºŒä¸ªéšè—å±‚ â†’ è¾“å‡ºå±‚
        self.fc3 = nn.Linear(hidden_dims[1], output_dim)
        
    def forward(self, z):
        print("\nğŸ”§ è§£ç å™¨å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼š")
        print(f"æ½œåœ¨å˜é‡ z å½¢çŠ¶: {z.shape}")
        
        # Layer 1: æ½œåœ¨ç©ºé—´ â†’ éšè—å±‚1
        h1 = F.relu(self.fc1(z))
        print(f"Layer 1 å: {h1.shape}")
        
        # Layer 2: éšè—å±‚1 â†’ éšè—å±‚2
        h2 = F.relu(self.fc2(h1))
        print(f"Layer 2 å: {h2.shape}")
        
        # Layer 3: éšè—å±‚2 â†’ è¾“å‡ºï¼ˆä½¿ç”¨sigmoidç¡®ä¿å€¼åœ¨[0,1]ï¼‰
        recon_x = torch.sigmoid(self.fc3(h2))
        print(f"é‡å»ºè¾“å‡ºå½¢çŠ¶: {recon_x.shape}")
        
        return recon_x


class Reparameterization:
    """é‡å‚æ•°åŒ–æŠ€å·§ï¼šä»N(Î¼, ÏƒÂ²)é‡‡æ ·ï¼ŒåŒæ—¶ä¿æŒæ¢¯åº¦å¯ä¼ æ’­"""
    
    @staticmethod
    def reparameterize(mu, logvar):
        print("\nğŸ”„ é‡å‚æ•°åŒ–è¿‡ç¨‹ï¼š")
        print(f"è¾“å…¥å‡å€¼ Î¼: {mu.shape}, å¯¹æ•°æ–¹å·® logvar: {logvar.shape}")
        
        # è®¡ç®—æ ‡å‡†å·® Ïƒ = exp(0.5 * log(ÏƒÂ²))
        std = torch.exp(0.5 * logvar)
        print(f"æ ‡å‡†å·® Ïƒ å½¢çŠ¶: {std.shape}")
        
        # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ · Îµ ~ N(0, I)
        eps = torch.randn_like(std)
        print(f"å™ªå£° Îµ å½¢çŠ¶: {eps.shape}")
        
        # é‡å‚æ•°åŒ–ï¼šz = Î¼ + Ïƒ âŠ™ Îµ
        z = mu + eps * std
        print(f"é‡‡æ ·ç»“æœ z å½¢çŠ¶: {z.shape}")
        
        return z


class VAELoss:
    """VAEæŸå¤±å‡½æ•°ï¼šé‡å»ºæŸå¤± + KLæ•£åº¦"""
    
    @staticmethod
    def loss_function(recon_x, x, mu, logvar):
        print("\nğŸ“Š æŸå¤±è®¡ç®—è¿‡ç¨‹ï¼š")
        
        # 1. é‡å»ºæŸå¤±ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰
        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')
        print(f"é‡å»ºæŸå¤± BCE: {BCE.item():.2f}")
        
        # 2. KLæ•£åº¦ï¼ˆæ½œåœ¨åˆ†å¸ƒä¸æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„å·®å¼‚ï¼‰
        # KL = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        print(f"KLæ•£åº¦ KLD: {KLD.item():.2f}")
        
        # 3. æ€»æŸå¤±
        total_loss = BCE + KLD
        print(f"æ€»æŸå¤±: {total_loss.item():.2f} (BCE + KLD)")
        
        return total_loss


class VAE(nn.Module):
    """å®Œæ•´çš„VAEæ¨¡å‹"""
    
    def __init__(self, input_dim=784, hidden_dims=[512, 256], latent_dim=20):
        super(VAE, self).__init__()
        
        self.encoder = Encoder(input_dim, hidden_dims, latent_dim)
        self.decoder = Decoder(latent_dim, hidden_dims[::-1], input_dim)
        
    def forward(self, x):
        print("=" * 60)
        print("ğŸš€ VAE å®Œæ•´å‰å‘ä¼ æ’­")
        print("=" * 60)
        
        # ç¼–ç å™¨ï¼šx â†’ (Î¼, logvar)
        mu, logvar = self.encoder(x)
        
        # é‡å‚æ•°åŒ–ï¼šä»N(Î¼, ÏƒÂ²)é‡‡æ ·z
        z = Reparameterization.reparameterize(mu, logvar)
        
        # è§£ç å™¨ï¼šz â†’ é‡å»ºæ•°æ®
        recon_x = self.decoder(z)
        
        return recon_x, mu, logvar


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

def test_vae():
    """æµ‹è¯•VAEæ¨¡å‹ç»“æ„"""
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•VAEæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆbatch_size=4, è¾“å…¥ç»´åº¦784ï¼‰
    batch_size = 4
    input_dim = 784
    x = torch.randn(batch_size, input_dim)
    
    print(f"\nğŸ“¦ è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}")
    
    # åˆå§‹åŒ–VAEæ¨¡å‹
    model = VAE(input_dim=input_dim)
    
    # å‰å‘ä¼ æ’­
    recon_x, mu, logvar = model(x)
    
    # è®¡ç®—æŸå¤±
    loss = VAELoss.loss_function(recon_x, x, mu, logvar)
    
    print("\nâœ… VAEæ¨¡å‹æµ‹è¯•å®Œæˆï¼")
    
    return model, loss


if __name__ == "__main__":
    test_vae()