# ä½¿ç”¨ Python 3.11 ä½œä¸ºåŸºç¡€é•œåƒ
FROM python:3.11-slim

# è®¾ç½®éäº¤äº’å¼å‰ç«¯
ENV DEBIAN_FRONTEND=noninteractive

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
# curl: ä¸‹è½½ Ollama
# build-essential: ç¼–è¯‘ä¾èµ–
# procps: è¿›ç¨‹ç®¡ç†
RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    procps \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# å¤åˆ¶ä¾èµ–æ–‡ä»¶å¹¶å®‰è£…
COPY requirements.txt .
# ä½¿ç”¨é˜¿é‡Œäº‘é•œåƒæºåŠ é€Ÿ pip å®‰è£…
RUN pip install --no-cache-dir -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# åˆ›å»ºå¯åŠ¨è„šæœ¬
# 1. æ˜¾å¼è®¾ç½® OLLAMA_HOST ä¸ºæœ¬åœ°
# 2. å¢åŠ æ—¥å¿—è¾“å‡º
# 3. å¢åŠ é‡è¯•æœºåˆ¶
RUN echo '#!/bin/bash\n\
export OLLAMA_MODELS=/home/user/.ollama/models\n\
export OLLAMA_HOST=127.0.0.1:11434\n\
\n\
echo "ğŸš€ Starting application on ModelScope..."\n\
\n\
# å¯åŠ¨ Ollama\n\
echo "ğŸ”´ Starting Ollama..."\n\
ollama serve > ollama.log 2>&1 &\n\
\n\
echo "â³ Waiting for Ollama to start..."\n\
sleep 5\n\
\n\
# å°è¯•æ‹‰å–æ¨¡å‹
echo "â¬‡ï¸  Pulling model (qwen2:1.5b)..."
# åœ¨åå°æ‹‰å–ï¼Œä¸é˜»å¡æœåŠ¡å¯åŠ¨
(ollama pull qwen2:1.5b && echo "âœ… Model pulled successfully") || echo "âš ï¸ Model pull failed" &\n\
\n\
# å¯åŠ¨ FastAPI\n\
echo "ğŸŸ¢ Starting FastAPI Server..."\n\
# ç»‘å®š 0.0.0.0:7860\n\
uvicorn server:app --host 0.0.0.0 --port 7860\n\
' > start.sh && chmod +x start.sh

# åˆ›å»ºé root ç”¨æˆ·
RUN useradd -m -u 1000 user
RUN mkdir -p /home/user/.ollama/models && chown -R user:user /home/user/.ollama
RUN mkdir -p /app && chown -R user:user /app

# åˆ‡æ¢ç”¨æˆ·
USER user

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV HOME=/home/user
ENV PATH=$HOME/.local/bin:$PATH
ENV OLLAMA_MODELS=$HOME/.ollama/models
ENV OLLAMA_HOST=127.0.0.1:11434

# æš´éœ²ç«¯å£
EXPOSE 7860

# å¯åŠ¨å‘½ä»¤
CMD ["/bin/bash", "/app/start.sh"]
