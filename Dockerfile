# ä½¿ç”¨ Python 3.11 ä½œä¸ºåŸºç¡€é•œåƒ
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
# curl: ä¸‹è½½ Ollama
# build-essential: ç¼–è¯‘æŸäº› Python åº“å¯èƒ½éœ€è¦
RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# å¤åˆ¶ä¾èµ–æ–‡ä»¶å¹¶å®‰è£…
COPY requirements.txt .
# ç¨å¾®æ”¾å®½ç‰ˆæœ¬é™åˆ¶ä»¥é¿å…å®‰è£…å¤±è´¥
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# åˆ›å»ºå¯åŠ¨è„šæœ¬
# 1. å¯åŠ¨ Ollama æœåŠ¡åå°è¿è¡Œ
# 2. ä¸‹è½½éœ€è¦çš„æ¨¡å‹ (è¿™é‡Œç”¨ tinyllama ä»¥ä¾¿å¿«é€Ÿæ¼”ç¤ºï¼Œä½ å¯ä»¥æ”¹ä¸º mistral æˆ– llama3)
# 3. å¯åŠ¨ FastAPI åº”ç”¨ (Hugging Face Spaces è¦æ±‚ç›‘å¬ 7860 ç«¯å£)
RUN echo '#!/bin/bash\n\
echo "ğŸ”´ Starting Ollama..."\n\
ollama serve &\n\
echo "â³ Waiting for Ollama to start..."\n\
sleep 5\n\
echo "â¬‡ï¸  Pulling model..."\n\
ollama pull tinyllama\n\
echo "ğŸŸ¢ Starting FastAPI Server..."\n\
uvicorn server:app --host 0.0.0.0 --port 7860\n\
' > start.sh && chmod +x start.sh

# åˆ›å»ºé root ç”¨æˆ· (Hugging Face å®‰å…¨è¦æ±‚)
RUN useradd -m -u 1000 user
# ç»™ç”¨æˆ· Ollama ç›®å½•çš„æƒé™
RUN mkdir -p /.ollama && chmod 777 /.ollama
RUN mkdir -p /app && chown -R user:user /app

# åˆ‡æ¢ç”¨æˆ·
USER user

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV HOME=/home/user
ENV PATH=$HOME/.local/bin:$PATH

# æš´éœ²ç«¯å£ (Hugging Face é»˜è®¤ç«¯å£)
EXPOSE 7860

# å¯åŠ¨å‘½ä»¤
CMD ["./start.sh"]
