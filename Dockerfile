# ä½¿ç”¨ Python 3.11 ä½œä¸ºåŸºç¡€é•œåƒ
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
# curl: ä¸‹è½½ Ollama
# build-essential: ç¼–è¯‘æŸäº› Python åº“å¯èƒ½éœ€è¦
RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£… Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# å¤åˆ¶ä¾èµ–æ–‡ä»¶å¹¶å®‰è£…
COPY requirements.txt .
# ç¨å¾®æ”¾å®½ç‰ˆæœ¬é™åˆ¶ä»¥é¿å…å®‰è£…å¤±è´¥
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# åˆ›å»ºå¯åŠ¨è„šæœ¬
# ä¼˜åŒ–ç­–ç•¥ï¼š
# 1. è®¾ç½® OLLAMA_MODELS ç¯å¢ƒå˜é‡åˆ°ç”¨æˆ·ç›®å½•
# 2. å¯åŠ¨ Ollama
# 3. åå°æ‹‰å–æ¨¡å‹ (ä¸é˜»å¡æœåŠ¡å™¨å¯åŠ¨)
# 4. å¯åŠ¨ FastAPI (å°½å¿«ç›‘å¬ç«¯å£ä»¥é€šè¿‡å¥åº·æ£€æŸ¥)
RUN echo '#!/bin/bash\n\
export OLLAMA_MODELS=/home/user/.ollama/models\n\
\n\
echo "ğŸ”´ Starting Ollama..."\n\
ollama serve &\n\
\n\
echo "â³ Waiting for Ollama to start..."\n\
sleep 5\n\
\n\
echo "â¬‡ï¸  Pulling model in background..."\n\
ollama pull tinyllama &\n\
\n\
echo "ğŸŸ¢ Starting FastAPI Server..."\n\
uvicorn server:app --host 0.0.0.0 --port 7860\n\
' > start.sh && chmod +x start.sh

# åˆ›å»ºé root ç”¨æˆ· (Hugging Face å®‰å…¨è¦æ±‚)
RUN useradd -m -u 1000 user

# ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶èµ‹äºˆæƒé™
RUN mkdir -p /home/user/.ollama/models && chown -R user:user /home/user/.ollama
RUN mkdir -p /app && chown -R user:user /app

# åˆ‡æ¢ç”¨æˆ·
USER user

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV HOME=/home/user
ENV PATH=$HOME/.local/bin:$PATH
ENV OLLAMA_MODELS=$HOME/.ollama/models

# æš´éœ²ç«¯å£ (Hugging Face é»˜è®¤ç«¯å£)
EXPOSE 7860

# å¯åŠ¨å‘½ä»¤
CMD ["./start.sh"]
