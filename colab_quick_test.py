#!/usr/bin/env python3
"""
Google Colabä¸€é”®GPUæµ‹è¯•è„šæœ¬
å¤åˆ¶æ­¤æ–‡ä»¶å†…å®¹åˆ°Colabå•å…ƒæ ¼ä¸­ç›´æ¥è¿è¡Œ

ä½¿ç”¨æ–¹æ³•:
1. åœ¨Colabä¸­åˆ›å»ºæ–°ç¬”è®°æœ¬
2. å¯ç”¨GPU (è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU)
3. å¤åˆ¶å¹¶è¿è¡Œæ­¤è„šæœ¬
"""

# ============================================================
# ğŸ”§ è‡ªåŠ¨å®‰è£…ä¾èµ–
# ============================================================
print("ğŸ“¦ æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–...")
import subprocess
import sys

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

# æ£€æŸ¥å¿…è¦çš„åŒ…
required_packages = {
    'torch': 'torch',
    'sentence_transformers': 'sentence-transformers',
    'networkx': 'networkx',
    'numpy': 'numpy'
}

for import_name, package_name in required_packages.items():
    try:
        __import__(import_name)
        print(f"âœ… {package_name} å·²å®‰è£…")
    except ImportError:
        print(f"ğŸ“¥ å®‰è£… {package_name}...")
        install(package_name)

print("\n" + "="*70)
print("ğŸš€ Google Colab GPUæ€§èƒ½æµ‹è¯• - GraphRAGåŠ é€ŸéªŒè¯")
print("="*70)

# ============================================================
# 1ï¸âƒ£ GPUæ£€æµ‹
# ============================================================
import torch
import time

print("\n" + "="*70)
print("ğŸ” æ­¥éª¤1: GPUç¯å¢ƒæ£€æµ‹")
print("="*70)

cuda_available = torch.cuda.is_available()
print(f"\n{'âœ…' if cuda_available else 'âŒ'} CUDAå¯ç”¨: {cuda_available}")

if cuda_available:
    print(f"   ğŸ“Š GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"   ğŸ’¾ æ˜¾å­˜å¤§å°: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
    print(f"   ğŸ”¢ CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   ğŸ“ˆ PyTorchç‰ˆæœ¬: {torch.__version__}")
else:
    print("\nâš ï¸  GPUæœªå¯ç”¨ï¼")
    print("   è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¯ç”¨GPU:")
    print("   1. ç‚¹å‡»é¡¶éƒ¨èœå• 'è¿è¡Œæ—¶'")
    print("   2. é€‰æ‹© 'æ›´æ”¹è¿è¡Œæ—¶ç±»å‹'")
    print("   3. ç¡¬ä»¶åŠ é€Ÿå™¨é€‰æ‹© 'GPU'")
    print("   4. ç‚¹å‡» 'ä¿å­˜'")
    print("   5. é‡æ–°è¿è¡Œæ­¤å•å…ƒæ ¼")
    print("\nâš ï¸  æµ‹è¯•å°†ç»§ç»­ï¼Œä½†GPUç›¸å…³æµ‹è¯•ä¼šè¢«è·³è¿‡")

# ============================================================
# 2ï¸âƒ£ çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•
# ============================================================
print("\n" + "="*70)
print("âš¡ æ­¥éª¤2: çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•")
print("="*70)

matrix_size = 5000
print(f"\næµ‹è¯•é…ç½®: {matrix_size}x{matrix_size} çŸ©é˜µä¹˜æ³•\n")

# CPUæµ‹è¯•
print("ğŸ”µ CPUæ€§èƒ½æµ‹è¯•...")
a_cpu = torch.randn(matrix_size, matrix_size)
b_cpu = torch.randn(matrix_size, matrix_size)

start = time.time()
c_cpu = torch.mm(a_cpu, b_cpu)
cpu_time = time.time() - start

print(f"   â±ï¸  CPUè€—æ—¶: {cpu_time:.3f}ç§’")

# GPUæµ‹è¯•
if cuda_available:
    print("\nğŸŸ¢ GPUæ€§èƒ½æµ‹è¯•...")
    a_gpu = torch.randn(matrix_size, matrix_size).cuda()
    b_gpu = torch.randn(matrix_size, matrix_size).cuda()
    
    # é¢„çƒ­
    _ = torch.mm(a_gpu, b_gpu)
    torch.cuda.synchronize()
    
    start = time.time()
    c_gpu = torch.mm(a_gpu, b_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    
    print(f"   â±ï¸  GPUè€—æ—¶: {gpu_time:.3f}ç§’")
    
    speedup = cpu_time / gpu_time
    print(f"\n   ğŸš€ æ€§èƒ½æå‡: {speedup:.1f}x")
    print(f"   ğŸ’¡ GPUæ¯”CPUå¿« {speedup:.1f} å€!")
    
    matrix_speedup = speedup
else:
    print("\nâš ï¸  è·³è¿‡GPUæµ‹è¯•")
    matrix_speedup = 1.0

# ============================================================
# 3ï¸âƒ£ æ–‡æœ¬åµŒå…¥æ€§èƒ½æµ‹è¯•
# ============================================================
print("\n" + "="*70)
print("ğŸ“ æ­¥éª¤3: æ–‡æœ¬åµŒå…¥æ€§èƒ½æµ‹è¯• (GraphRAGæ ¸å¿ƒç»„ä»¶)")
print("="*70)

try:
    from sentence_transformers import SentenceTransformer
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_texts = [
        "GraphRAG combines knowledge graphs with retrieval augmented generation",
        "GPU acceleration significantly improves machine learning performance",
        "Large language models benefit from efficient embedding computation",
        "Knowledge graph construction requires entity and relation extraction",
    ] * 250  # 1000æ¡æ–‡æœ¬
    
    print(f"\næµ‹è¯•é…ç½®: {len(test_texts)}æ¡æ–‡æœ¬åµŒå…¥\n")
    
    # CPUåµŒå…¥
    print("ğŸ”µ CPUåµŒå…¥æµ‹è¯•...")
    model_cpu = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')
    
    start = time.time()
    embeddings_cpu = model_cpu.encode(test_texts, show_progress_bar=False, batch_size=32)
    cpu_emb_time = time.time() - start
    
    print(f"   â±ï¸  CPUè€—æ—¶: {cpu_emb_time:.2f}ç§’")
    print(f"   ğŸ“Š å¤„ç†é€Ÿåº¦: {len(test_texts)/cpu_emb_time:.1f} æ–‡æœ¬/ç§’")
    
    # GPUåµŒå…¥
    if cuda_available:
        print("\nğŸŸ¢ GPUåµŒå…¥æµ‹è¯•...")
        model_gpu = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')
        
        start = time.time()
        embeddings_gpu = model_gpu.encode(test_texts, show_progress_bar=False, batch_size=32)
        gpu_emb_time = time.time() - start
        
        print(f"   â±ï¸  GPUè€—æ—¶: {gpu_emb_time:.2f}ç§’")
        print(f"   ğŸ“Š å¤„ç†é€Ÿåº¦: {len(test_texts)/gpu_emb_time:.1f} æ–‡æœ¬/ç§’")
        
        emb_speedup = cpu_emb_time / gpu_emb_time
        print(f"\n   ğŸš€ æ€§èƒ½æå‡: {emb_speedup:.1f}x")
        print(f"   â±ï¸  èŠ‚çœæ—¶é—´: {cpu_emb_time - gpu_emb_time:.2f}ç§’")
    else:
        print("\nâš ï¸  è·³è¿‡GPUæµ‹è¯•")
        emb_speedup = 1.0
        
except ImportError:
    print("\nâš ï¸  sentence-transformersæœªå®‰è£…ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
    emb_speedup = None

# ============================================================
# 4ï¸âƒ£ GraphRAGåœºæ™¯æ¨¡æ‹Ÿ
# ============================================================
print("\n" + "="*70)
print("ğŸ” æ­¥éª¤4: GraphRAGå®é™…åœºæ™¯æ¨¡æ‹Ÿ")
print("="*70)

if cuda_available and emb_speedup:
    print("\næ¨¡æ‹ŸGraphRAGç´¢å¼•æ„å»ºè¿‡ç¨‹...\n")
    
    # å‡è®¾100ä¸ªæ–‡æ¡£å—çš„ç´¢å¼•æ„å»º
    documents_count = 100
    
    # å®ä½“æå–æ—¶é—´ (æ¯ä¸ªæ–‡æ¡£çº¦1ç§’)
    entity_extraction_time = documents_count * 1.0
    
    # æ–‡æœ¬åµŒå…¥æ—¶é—´ (åŸºäºå®é™…æµ‹è¯•)
    # å‡è®¾æ¯ä¸ªæ–‡æ¡£å¹³å‡äº§ç”Ÿ10ä¸ªå®ä½“ï¼Œå…±1000ä¸ªå®ä½“éœ€è¦åµŒå…¥
    entities_count = documents_count * 10
    
    cpu_total_time = entity_extraction_time + (entities_count / (len(test_texts)/cpu_emb_time))
    gpu_total_time = entity_extraction_time + (entities_count / (len(test_texts)/gpu_emb_time))
    
    print(f"ğŸ“Š åœºæ™¯: {documents_count}ä¸ªæ–‡æ¡£çš„GraphRAGç´¢å¼•æ„å»º\n")
    print(f"ğŸ”µ CPUé¢„è®¡æ—¶é—´:")
    print(f"   - å®ä½“æå–: {entity_extraction_time/60:.1f}åˆ†é’Ÿ")
    print(f"   - å‘é‡åµŒå…¥: {(entities_count / (len(test_texts)/cpu_emb_time))/60:.1f}åˆ†é’Ÿ")
    print(f"   - æ€»è®¡: {cpu_total_time/60:.1f}åˆ†é’Ÿ")
    
    print(f"\nğŸŸ¢ GPUé¢„è®¡æ—¶é—´:")
    print(f"   - å®ä½“æå–: {entity_extraction_time/60:.1f}åˆ†é’Ÿ (ç›¸åŒ)")
    print(f"   - å‘é‡åµŒå…¥: {(entities_count / (len(test_texts)/gpu_emb_time))/60:.1f}åˆ†é’Ÿ")
    print(f"   - æ€»è®¡: {gpu_total_time/60:.1f}åˆ†é’Ÿ")
    
    total_speedup = cpu_total_time / gpu_total_time
    time_saved = (cpu_total_time - gpu_total_time) / 60
    
    print(f"\nğŸš€ æ•´ä½“åŠ é€Ÿ: {total_speedup:.1f}x")
    print(f"â±ï¸  èŠ‚çœæ—¶é—´: {time_saved:.1f}åˆ†é’Ÿ")

# ============================================================
# 5ï¸âƒ£ GPUæ˜¾å­˜ç›‘æ§
# ============================================================
if cuda_available:
    print("\n" + "="*70)
    print("ğŸ’¾ æ­¥éª¤5: GPUæ˜¾å­˜ä½¿ç”¨ç›‘æ§")
    print("="*70)
    
    allocated = torch.cuda.memory_allocated(0) / (1024**3)
    reserved = torch.cuda.memory_reserved(0) / (1024**3)
    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"\n   å·²åˆ†é…: {allocated:.2f} GB")
    print(f"   å·²ä¿ç•™: {reserved:.2f} GB")
    print(f"   æ€»æ˜¾å­˜: {total:.2f} GB")
    print(f"   ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")

# ============================================================
# 6ï¸âƒ£ æ€§èƒ½æ€»ç»“
# ============================================================
print("\n" + "="*70)
print("ğŸ“ˆ æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
print("="*70)

print("\nğŸ–¥ï¸  ç¡¬ä»¶é…ç½®:")
if cuda_available:
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    print(f"   CUDA: {torch.version.cuda}")
else:
    print("   âš ï¸  GPUæœªå¯ç”¨")

print(f"\nâš¡ æ€§èƒ½æµ‹è¯•ç»“æœ:")
print(f"   çŸ©é˜µè¿ç®—åŠ é€Ÿ: {matrix_speedup:.1f}x")
if emb_speedup:
    print(f"   æ–‡æœ¬åµŒå…¥åŠ é€Ÿ: {emb_speedup:.1f}x")
    if cuda_available:
        print(f"   GraphRAGæ•´ä½“åŠ é€Ÿ: {total_speedup:.1f}x")

print("\nğŸ’¡ ç»“è®ºå’Œå»ºè®®:")
if cuda_available:
    print("   âœ… GPUæ€§èƒ½æµ‹è¯•æˆåŠŸ!")
    print("   âœ… å¼ºçƒˆå»ºè®®åœ¨Colab GPUç¯å¢ƒè¿è¡ŒGraphRAG")
    print(f"   âœ… é¢„è®¡å¯èŠ‚çœ {time_saved:.0f}+ åˆ†é’Ÿçš„ç´¢å¼•æ„å»ºæ—¶é—´")
    print("\nğŸ“š ä¸‹ä¸€æ­¥:")
    print("   1. ä¸Šä¼ adaptive_RAGé¡¹ç›®æ–‡ä»¶åˆ°Colab")
    print("   2. è¿è¡Œ main_graphrag.py æ„å»ºå®Œæ•´çŸ¥è¯†å›¾è°±")
    print("   3. ä¸‹è½½ç»“æœåˆ°æœ¬åœ°ä½¿ç”¨")
else:
    print("   âš ï¸  è¯·å¯ç”¨GPUä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    print("   âš ï¸  è·¯å¾„: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU")

print("\n" + "="*70)
print("âœ… æµ‹è¯•å®Œæˆ! æ„Ÿè°¢ä½¿ç”¨GraphRAG GPUæµ‹è¯•å·¥å…·")
print("="*70)

# ============================================================
# 7ï¸âƒ£ å¯é€‰: æ˜¾ç¤ºnvidia-smi
# ============================================================
if cuda_available:
    print("\nğŸ“Š nvidia-smi è¯¦ç»†ä¿¡æ¯:")
    print("="*70)
    import subprocess
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print(result.stdout)
    except:
        print("âš ï¸  æ— æ³•æ‰§è¡Œnvidia-smiå‘½ä»¤")
