#!/usr/bin/env python3
"""
é‡æ’åŠŸèƒ½æµ‹è¯•è„šæœ¬
æ¼”ç¤ºä¸åŒé‡æ’ç­–ç•¥çš„æ•ˆæœ
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from document_processor import DocumentProcessor
from reranker import *
try:
    from langchain_core.documents import Document
except ImportError:
    try:
        from langchain_core.documents import Document
    except ImportError:
        from langchain.schema import Document
import time


def create_test_documents():
    """åˆ›å»ºæµ‹è¯•æ–‡æ¡£"""
    return [
        Document(
            page_content="äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
            metadata={"source": "ai_intro.txt", "category": "AIåŸºç¡€"}
        ),
        Document(
            page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦å­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚",
            metadata={"source": "ml_basics.txt", "category": "æœºå™¨å­¦ä¹ "}
        ),
        Document(
            page_content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
            metadata={"source": "dl_guide.txt", "category": "æ·±åº¦å­¦ä¹ "}
        ),
        Document(
            page_content="è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºç†è§£å’Œå¤„ç†äººç±»è¯­è¨€ã€‚",
            metadata={"source": "nlp_overview.txt", "category": "è‡ªç„¶è¯­è¨€å¤„ç†"}
        ),
        Document(
            page_content="è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦é¢†åŸŸï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè¯†åˆ«å’Œç†è§£å›¾åƒå’Œè§†é¢‘å†…å®¹ã€‚",
            metadata={"source": "cv_intro.txt", "category": "è®¡ç®—æœºè§†è§‰"}
        ),
        Document(
            page_content="å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ç±»å‹ï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜çš„è¡Œä¸ºç­–ç•¥ã€‚",
            metadata={"source": "rl_basics.txt", "category": "å¼ºåŒ–å­¦ä¹ "}
        ),
        Document(
            page_content="ä»Šå¤©çš„å¤©æ°”éå¸¸å¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå¤–å‡ºæ¸¸ç©å’Œè¿åŠ¨ã€‚",
            metadata={"source": "weather.txt", "category": "å¤©æ°”"}
        ),
        Document(
            page_content="åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œå…·æœ‰å»ä¸­å¿ƒåŒ–ã€ä¸å¯ç¯¡æ”¹ç­‰ç‰¹ç‚¹ã€‚",
            metadata={"source": "blockchain.txt", "category": "åŒºå—é“¾"}
        )
    ]


def test_reranker_comparison():
    """æ¯”è¾ƒä¸åŒé‡æ’å™¨çš„æ•ˆæœ"""
    print("ğŸ” é‡æ’å™¨æ•ˆæœæ¯”è¾ƒæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ï¼Ÿ"
    documents = create_test_documents()
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„åµŒå…¥æ¨¡å‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'}
        )
        print("âœ… æˆåŠŸåŠ è½½åµŒå…¥æ¨¡å‹")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨åŸºç¡€é‡æ’å™¨è¿›è¡Œæµ‹è¯•")
        embeddings = None
    
    # æµ‹è¯•ä¸åŒçš„é‡æ’å™¨
    rerankers = []
    
    # TF-IDFé‡æ’å™¨
    rerankers.append(("TF-IDF", TFIDFReranker()))
    
    # BM25é‡æ’å™¨
    rerankers.append(("BM25", BM25Reranker()))
    
    if embeddings:
        # è¯­ä¹‰é‡æ’å™¨
        rerankers.append(("è¯­ä¹‰ç›¸ä¼¼åº¦", SemanticReranker(embeddings)))
        
        # æ··åˆé‡æ’å™¨
        rerankers.append(("æ··åˆç­–ç•¥", HybridReranker(embeddings)))
        
        # å¤šæ ·æ€§é‡æ’å™¨
        rerankers.append(("å¤šæ ·æ€§ä¼˜åŒ–", DiversityReranker(embeddings)))
    
    # æ‰§è¡Œæµ‹è¯•
    for name, reranker in rerankers:
        print(f"\nğŸ“Š {name} é‡æ’ç»“æœ:")
        print("-" * 40)
        
        start_time = time.time()
        try:
            results = reranker.rerank(query, documents, top_k=5)
            end_time = time.time()
            
            print(f"â±ï¸ å¤„ç†æ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
            
            for i, (doc, score) in enumerate(results, 1):
                content = doc.page_content[:80] + "..." if len(doc.page_content) > 80 else doc.page_content
                category = doc.metadata.get('category', 'æœªçŸ¥')
                print(f"{i}. [åˆ†æ•°: {score:.4f}] [{category}] {content}")
                
        except Exception as e:
            print(f"âŒ é‡æ’å¤±è´¥: {e}")


def test_reranking_with_embeddings():
    """æµ‹è¯•å¸¦åµŒå…¥çš„é‡æ’åŠŸèƒ½"""
    print("\n\nğŸ§  åµŒå…¥æ¨¡å‹é‡æ’æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
        processor = DocumentProcessor()
        
        # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        test_docs = create_test_documents()
        
        # æµ‹è¯•æŸ¥è¯¢
        queries = [
            "äººå·¥æ™ºèƒ½çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«",
            "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        ]
        
        for query in queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            print("-" * 30)
            
            if processor.reranker:
                # ä½¿ç”¨é‡æ’åŠŸèƒ½
                results = processor.reranker.rerank(query, test_docs, top_k=3)
                
                for i, (doc, score) in enumerate(results, 1):
                    content = doc.page_content[:60] + "..." if len(doc.page_content) > 60 else doc.page_content
                    category = doc.metadata.get('category', 'æœªçŸ¥')
                    print(f"{i}. [åˆ†æ•°: {score:.4f}] [{category}] {content}")
            else:
                print("âŒ é‡æ’å™¨æœªåˆå§‹åŒ–")
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def test_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\n\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    documents = create_test_documents() * 10  # å¢åŠ æ–‡æ¡£æ•°é‡
    query = "äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•è¶‹åŠ¿"
    
    # æµ‹è¯•ä¸åŒé‡æ’å™¨çš„æ€§èƒ½
    rerankers_config = [
        ("æ— é‡æ’", None),
        ("TF-IDF", TFIDFReranker()),
        ("BM25", BM25Reranker())
    ]
    
    for name, reranker in rerankers_config:
        times = []
        
        # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
        for _ in range(5):
            start_time = time.time()
            
            if reranker:
                results = reranker.rerank(query, documents, top_k=5)
            else:
                # æ¨¡æ‹Ÿæ— é‡æ’çš„æƒ…å†µ
                results = documents[:5]
            
            end_time = time.time()
            times.append((end_time - start_time) * 1000)
        
        avg_time = sum(times) / len(times)
        print(f"{name}: å¹³å‡å¤„ç†æ—¶é—´ {avg_time:.2f}ms (æ–‡æ¡£æ•°: {len(documents)})")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å‘é‡é‡æ’åŠŸèƒ½ç»¼åˆæµ‹è¯•")
    print("=" * 80)
    
    try:
        # åŸºç¡€é‡æ’å™¨æ¯”è¾ƒ
        test_reranker_comparison()
        
        # åµŒå…¥æ¨¡å‹é‡æ’æµ‹è¯•
        test_reranking_with_embeddings()
        
        # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        test_performance_comparison()
        
        print("\n\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\nâŒ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()