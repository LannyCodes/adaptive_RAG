"""
å‘é‡é‡æ’æ¨¡å—
å®ç°å¤šç§é‡æ’ç­–ç•¥ä»¥æé«˜æ£€ç´¢è´¨é‡
æ”¯æŒ CrossEncoder æ·±åº¦é‡æ’
"""

import torch
import numpy as np
from typing import List, Tuple, Dict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import Counter
import math

# CrossEncoder support
try:
    from sentence_transformers import CrossEncoder as SentenceTransformerCrossEncoder
    CROSSENCODER_AVAILABLE = True
except ImportError:
    CROSSENCODER_AVAILABLE = False
    print("âš ï¸ sentence-transformers not available. CrossEncoder reranking disabled.")


class DocumentReranker:
    """æ–‡æ¡£é‡æ’å™¨åŸºç±»"""
    
    def __init__(self):
        self.name = "BaseReranker"
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """é‡æ’æ–‡æ¡£å¹¶è¿”å›top_kç»“æœ"""
        raise NotImplementedError


class TFIDFReranker(DocumentReranker):
    """åŸºäºTF-IDFçš„é‡æ’å™¨"""
    
    def __init__(self):
        super().__init__()
        self.name = "TFIDFReranker"
        # ç§»é™¤ stop_words ä»¥æ”¯æŒä¸­æ–‡ï¼Œä½¿ç”¨ char_wb åˆ†è¯å™¨
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb',  # å­—ç¬¦çº§åˆ†è¯ï¼Œæ”¯æŒä¸­æ–‡
            ngram_range=(2, 4),  # 2-4 å­—ç¬¦ n-gram
            max_features=5000
        )
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """ä½¿ç”¨TF-IDFé‡æ–°æ’åºæ–‡æ¡£"""
        if not documents:
            return []
        
        # æå–æ–‡æ¡£å†…å®¹
        doc_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
        all_texts = [query] + doc_texts
        
        # è®¡ç®—TF-IDFçŸ©é˜µ
        tfidf_matrix = self.vectorizer.fit_transform(all_texts)
        query_vec = tfidf_matrix[0]
        doc_vecs = tfidf_matrix[1:]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vec, doc_vecs).flatten()
        
        # æ’åºå¹¶è¿”å›top_k
        ranked_indices = np.argsort(similarities)[::-1]
        results = []
        for i in ranked_indices[:top_k]:
            results.append((documents[i], float(similarities[i])))
        
        return results


class BM25Reranker(DocumentReranker):
    """åŸºäºBM25ç®—æ³•çš„é‡æ’å™¨"""
    
    def __init__(self, k1: float = 1.5, b: float = 0.75):
        super().__init__()
        self.name = "BM25Reranker"
        self.k1 = k1
        self.b = b
    
    def _tokenize(self, text: str) -> List[str]:
        """
        æ”¹è¿›çš„åˆ†è¯ï¼Œæ”¯æŒä¸­è‹±æ–‡
        ä¸­æ–‡ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯ï¼Œè‹±æ–‡ä½¿ç”¨å•è¯åˆ†è¯
        """
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text)
        
        if has_chinese:
            # ä¸­æ–‡ï¼šä½¿ç”¨å­—ç¬¦çº§ + 2-gram
            chars = list(text.lower())
            # ç”Ÿæˆ unigram å’Œ bigram
            tokens = chars + [chars[i] + chars[i+1] for i in range(len(chars)-1)]
            return [t for t in tokens if t.strip()]  # ç§»é™¤ç©ºæ ¼
        else:
            # è‹±æ–‡ï¼šä½¿ç”¨å•è¯åˆ†è¯
            return re.findall(r'\b\w+\b', text.lower())
    
    def _compute_idf(self, documents: List[str], query_terms: List[str]) -> Dict[str, float]:
        """è®¡ç®—IDFå€¼"""
        N = len(documents)
        idf = {}
        
        for term in query_terms:
            df = sum(1 for doc in documents if term in self._tokenize(doc))
            idf[term] = math.log((N - df + 0.5) / (df + 0.5))
        
        return idf
    
    def _bm25_score(self, query_terms: List[str], document: str, avg_doc_len: float, idf: Dict[str, float]) -> float:
        """è®¡ç®—BM25åˆ†æ•°"""
        doc_terms = self._tokenize(document)
        doc_len = len(doc_terms)
        term_freq = Counter(doc_terms)
        
        score = 0.0
        for term in query_terms:
            if term in term_freq:
                tf = term_freq[term]
                score += idf.get(term, 0) * (tf * (self.k1 + 1)) / (
                    tf + self.k1 * (1 - self.b + self.b * doc_len / avg_doc_len)
                )
        
        return score
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """ä½¿ç”¨BM25é‡æ–°æ’åºæ–‡æ¡£"""
        if not documents:
            return []
        
        query_terms = self._tokenize(query)
        doc_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        avg_doc_len = sum(len(self._tokenize(doc)) for doc in doc_texts) / len(doc_texts)
        
        # è®¡ç®—IDF
        idf = self._compute_idf(doc_texts, query_terms)
        
        # è®¡ç®—BM25åˆ†æ•°
        scores = []
        for doc_text in doc_texts:
            score = self._bm25_score(query_terms, doc_text, avg_doc_len, idf)
            scores.append(score)
        
        # æ’åºå¹¶è¿”å›top_k
        ranked_indices = np.argsort(scores)[::-1]
        results = []
        for i in ranked_indices[:top_k]:
            results.append((documents[i], float(scores[i])))
        
        return results


class SemanticReranker(DocumentReranker):
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„é‡æ’å™¨"""
    
    def __init__(self, embeddings_model):
        super().__init__()
        self.name = "SemanticReranker"
        self.embeddings_model = embeddings_model
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦é‡æ–°æ’åºæ–‡æ¡£"""
        if not documents:
            return []
        
        # è·å–æŸ¥è¯¢åµŒå…¥
        query_embedding = self.embeddings_model.embed_query(query)
        
        # è·å–æ–‡æ¡£åµŒå…¥
        doc_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
        doc_embeddings = self.embeddings_model.embed_documents(doc_texts)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for doc_emb in doc_embeddings:
            sim = cosine_similarity([query_embedding], [doc_emb])[0][0]
            similarities.append(sim)
        
        # æ’åºå¹¶è¿”å›top_k
        ranked_indices = np.argsort(similarities)[::-1]
        results = []
        for i in ranked_indices[:top_k]:
            results.append((documents[i], float(similarities[i])))
        
        return results


class CrossEncoderReranker(DocumentReranker):
    """
    åŸºäº CrossEncoder çš„é‡æ’å™¨
    ä½¿ç”¨è”åˆç¼–ç ï¼Œç›¸æ¯” Bi-Encoder å‡†ç¡®ç‡æå‡ 15-20%
    é€‚åˆç²¾æ’é˜¶æ®µ (Top 20-100 æ–‡æ¡£)
    """
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2", max_length: int = 512):
        """
        åˆå§‹åŒ– CrossEncoder é‡æ’å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨è½»é‡çº§æ¨¡å‹
                - "cross-encoder/ms-marco-MiniLM-L-6-v2" (è½»é‡çº§ï¼Œæ¨è)
                - "cross-encoder/ms-marco-MiniLM-L-12-v2" (å¹³è¡¡)
                - "BAAI/bge-reranker-base" (ä¸­æ–‡ä¼˜åŒ–)
                - "BAAI/bge-reranker-large" (é«˜ç²¾åº¦)
            max_length: æœ€å¤§è¾“å…¥é•¿åº¦
        """
        super().__init__()
        self.name = "CrossEncoderReranker"
        self.model_name = model_name
        self.max_length = max_length
        
        # åŠ è½½æ¨¡å‹
        if not CROSSENCODER_AVAILABLE:
            raise ImportError(
                "CrossEncoder requires sentence-transformers. "
                "Install with: pip install sentence-transformers"
            )
        
        try:
            print(f"ğŸ”§ åŠ è½½ CrossEncoder æ¨¡å‹: {model_name}...")
            self.model = SentenceTransformerCrossEncoder(model_name, max_length=max_length)
            print(f"âœ… CrossEncoder æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ CrossEncoder æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """
        ä½¿ç”¨ CrossEncoder é‡æ–°æ’åºæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: å€™é€‰æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ’åºåçš„ (document, score) å…ƒç»„åˆ—è¡¨
        """
        if not documents:
            return []
        
        # æå–æ–‡æ¡£å†…å®¹
        doc_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in documents]
        
        # æ„é€  [query, doc] å¯¹
        query_doc_pairs = [[query, doc_text] for doc_text in doc_texts]
        
        # CrossEncoder è¯„åˆ† - è”åˆç¼–ç 
        try:
            scores = self.model.predict(query_doc_pairs)
            
            # æ’åº
            ranked_indices = np.argsort(scores)[::-1]
            
            # è¿”å› top_k ç»“æœ
            results = []
            for i in ranked_indices[:top_k]:
                results.append((documents[i], float(scores[i])))
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ CrossEncoder é‡æ’å¤±è´¥: {e}")
            # å›é€€åˆ°åŸå§‹é¡ºåº
            return [(doc, 0.0) for doc in documents[:top_k]]


class HybridReranker(DocumentReranker):
    """æ··åˆé‡æ’å™¨ï¼Œèåˆå¤šç§ç­–ç•¥"""
    
    def __init__(self, embeddings_model, weights: Dict[str, float] = None):
        super().__init__()
        self.name = "HybridReranker"
        
        # åˆå§‹åŒ–å„ç§é‡æ’å™¨
        self.tfidf_reranker = TFIDFReranker()
        self.bm25_reranker = BM25Reranker()
        self.semantic_reranker = SemanticReranker(embeddings_model)
        
        # è®¾ç½®æƒé‡
        self.weights = weights or {
            'tfidf': 0.3,
            'bm25': 0.3,
            'semantic': 0.4
        }
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """ä½¿ç”¨æ··åˆç­–ç•¥é‡æ–°æ’åºæ–‡æ¡£"""
        if not documents:
            return []
        
        # è·å–å„ç§é‡æ’ç»“æœ
        tfidf_results = self.tfidf_reranker.rerank(query, documents, len(documents))
        bm25_results = self.bm25_reranker.rerank(query, documents, len(documents))
        semantic_results = self.semantic_reranker.rerank(query, documents, len(documents))
        
        # åˆ›å»ºæ–‡æ¡£åˆ°åˆ†æ•°çš„æ˜ å°„
        doc_scores = {}
        for doc in documents:
            doc_id = id(doc)
            doc_scores[doc_id] = {'doc': doc, 'tfidf': 0, 'bm25': 0, 'semantic': 0}
        
        # å¡«å……å„ç§åˆ†æ•°
        for doc, score in tfidf_results:
            doc_scores[id(doc)]['tfidf'] = score
        
        for doc, score in bm25_results:
            doc_scores[id(doc)]['bm25'] = score
        
        for doc, score in semantic_results:
            doc_scores[id(doc)]['semantic'] = score
        
        # å½’ä¸€åŒ–åˆ†æ•°
        for score_type in ['tfidf', 'bm25', 'semantic']:
            scores = [info[score_type] for info in doc_scores.values()]
            if max(scores) > 0:
                max_score = max(scores)
                for doc_id in doc_scores:
                    doc_scores[doc_id][score_type] /= max_score
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        final_scores = []
        for doc_id, info in doc_scores.items():
            combined_score = (
                self.weights['tfidf'] * info['tfidf'] +
                self.weights['bm25'] * info['bm25'] +
                self.weights['semantic'] * info['semantic']
            )
            final_scores.append((info['doc'], combined_score))
        
        # æ’åºå¹¶è¿”å›top_k
        final_scores.sort(key=lambda x: x[1], reverse=True)
        return final_scores[:top_k]


class DiversityReranker(DocumentReranker):
    """å¤šæ ·æ€§é‡æ’å™¨ï¼Œé¿å…ç»“æœé‡å¤"""
    
    def __init__(self, embeddings_model, diversity_lambda: float = 0.5):
        super().__init__()
        self.name = "DiversityReranker"
        self.embeddings_model = embeddings_model
        self.diversity_lambda = diversity_lambda
    
    def _calculate_diversity_penalty(self, candidate_doc: str, selected_docs: List[str]) -> float:
        """è®¡ç®—å¤šæ ·æ€§æƒ©ç½š"""
        if not selected_docs:
            return 0.0
        
        candidate_emb = self.embeddings_model.embed_documents([candidate_doc])[0]
        selected_embs = self.embeddings_model.embed_documents(selected_docs)
        
        max_similarity = 0.0
        for selected_emb in selected_embs:
            sim = cosine_similarity([candidate_emb], [selected_emb])[0][0]
            max_similarity = max(max_similarity, sim)
        
        return max_similarity
    
    def rerank(self, query: str, documents: List[dict], top_k: int = 5) -> List[Tuple[dict, float]]:
        """ä½¿ç”¨å¤šæ ·æ€§ç­–ç•¥é‡æ–°æ’åºæ–‡æ¡£"""
        if not documents:
            return []
        
        # é¦–å…ˆä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è·å–åˆå§‹æ’åº
        semantic_results = SemanticReranker(self.embeddings_model).rerank(
            query, documents, len(documents)
        )
        
        # MMR (Maximal Marginal Relevance) ç®—æ³•
        selected_docs = []
        selected_texts = []
        remaining_docs = [doc for doc, _ in semantic_results]
        relevance_scores = {id(doc): score for doc, score in semantic_results}
        
        while len(selected_docs) < top_k and remaining_docs:
            best_score = -1
            best_doc = None
            best_idx = -1
            
            for i, doc in enumerate(remaining_docs):
                doc_text = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                relevance = relevance_scores[id(doc)]
                diversity_penalty = self._calculate_diversity_penalty(doc_text, selected_texts)
                
                # MMRåˆ†æ•° = Î» * ç›¸å…³æ€§ - (1-Î») * å¤šæ ·æ€§æƒ©ç½š
                mmr_score = (
                    self.diversity_lambda * relevance - 
                    (1 - self.diversity_lambda) * diversity_penalty
                )
                
                if mmr_score > best_score:
                    best_score = mmr_score
                    best_doc = doc
                    best_idx = i
            
            if best_doc is not None:
                selected_docs.append((best_doc, best_score))
                selected_texts.append(
                    best_doc.page_content if hasattr(best_doc, 'page_content') else str(best_doc)
                )
                remaining_docs.pop(best_idx)
        
        return selected_docs


def create_reranker(reranker_type: str, embeddings_model=None, **kwargs) -> DocumentReranker:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæŒ‡å®šç±»å‹çš„é‡æ’å™¨
    
    Args:
        reranker_type: é‡æ’å™¨ç±»å‹
            - 'tfidf': TF-IDF é‡æ’
            - 'bm25': BM25 é‡æ’
            - 'semantic': Bi-Encoder è¯­ä¹‰é‡æ’
            - 'crossencoder': CrossEncoder é‡æ’ (æ¨è) â­
            - 'hybrid': æ··åˆé‡æ’
            - 'diversity': å¤šæ ·æ€§é‡æ’
        embeddings_model: åµŒå…¥æ¨¡å‹ (æŸäº›é‡æ’å™¨éœ€è¦)
        **kwargs: å…¶ä»–å‚æ•°
            - model_name: CrossEncoder æ¨¡å‹åç§°
            - max_length: CrossEncoder æœ€å¤§é•¿åº¦
            - weights: æ··åˆé‡æ’æƒé‡
    
    Returns:
        DocumentReranker: é‡æ’å™¨å®ä¾‹
    """
    
    if reranker_type.lower() == 'tfidf':
        return TFIDFReranker()
    
    elif reranker_type.lower() == 'bm25':
        return BM25Reranker(**kwargs)
    
    elif reranker_type.lower() == 'semantic':
        if embeddings_model is None:
            raise ValueError("SemanticReranker requires embeddings_model")
        return SemanticReranker(embeddings_model)
    
    elif reranker_type.lower() in ['crossencoder', 'cross_encoder', 'cross-encoder']:
        # CrossEncoder ä¸éœ€è¦ embeddings_modelï¼Œä½¿ç”¨è‡ªå·±çš„æ¨¡å‹
        model_name = kwargs.get('model_name', 'cross-encoder/ms-marco-MiniLM-L-6-v2')
        max_length = kwargs.get('max_length', 512)
        return CrossEncoderReranker(model_name=model_name, max_length=max_length)
    
    elif reranker_type.lower() == 'hybrid':
        if embeddings_model is None:
            raise ValueError("HybridReranker requires embeddings_model")
        return HybridReranker(embeddings_model, **kwargs)
    
    elif reranker_type.lower() == 'diversity':
        if embeddings_model is None:
            raise ValueError("DiversityReranker requires embeddings_model")
        return DiversityReranker(embeddings_model, **kwargs)
    
    else:
        raise ValueError(
            f"Unknown reranker type: {reranker_type}. "
            f"Available types: tfidf, bm25, semantic, crossencoder, hybrid, diversity"
        )


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡æ‹Ÿæ–‡æ¡£
    class MockDoc:
        def __init__(self, content):
            self.page_content = content
    
    docs = [
        MockDoc("äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯"),
        MockDoc("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é¢†åŸŸ"),
        MockDoc("æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ"),
        MockDoc("è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†æ–‡æœ¬æ•°æ®"),
        MockDoc("ä»Šå¤©å¤©æ°”å¾ˆå¥½")
    ]
    
    query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    
    # æµ‹è¯•TF-IDFé‡æ’
    tfidf_reranker = TFIDFReranker()
    results = tfidf_reranker.rerank(query, docs, top_k=3)
    
    print("TF-IDFé‡æ’ç»“æœ:")
    for doc, score in results:
        print(f"åˆ†æ•°: {score:.4f} - å†…å®¹: {doc.page_content}")