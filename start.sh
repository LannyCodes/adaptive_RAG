#!/bin/bash
# å¼€å¯è°ƒè¯•æ¨¡å¼
set -x

# è®¾ç½®ç¯å¢ƒå˜é‡ (ç¡®ä¿ä¸ Dockerfile ä¸€è‡´)
export OLLAMA_MODELS=/root/.ollama/models
export OLLAMA_HOST=127.0.0.1:11434

echo "ğŸš€ Starting application on ModelScope (Root Mode)..."

# å¯åŠ¨ Ollama
echo "ğŸ”´ Starting Ollama..."
# ç¡®ä¿ç›®å½•å­˜åœ¨
mkdir -p $OLLAMA_MODELS
ollama serve > ollama.log 2>&1 &

echo "â³ Waiting for Ollama to start..."
sleep 5

# å°è¯•æ‹‰å–æ¨¡å‹
echo "â¬‡ï¸  Pulling model (qwen2:1.5b)..."
# åœ¨åå°æ‹‰å–
(ollama pull qwen2:1.5b && echo "âœ… Model pulled successfully") || echo "âš ï¸ Model pull failed" &

# å¯åŠ¨ FastAPI
echo "ğŸŸ¢ Starting FastAPI Server..."
# ä½¿ç”¨ nohup åå°è¿è¡Œï¼Œå¹¶é‡å®šå‘æ—¥å¿—
nohup uvicorn server:app --host 0.0.0.0 --port 7860 > server.log 2>&1 &

# ç­‰å¾…å‡ ç§’
sleep 2

# æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´»
if pgrep -f "uvicorn" > /dev/null; then
    echo "âœ… FastAPI is running."
else
    echo "âŒ FastAPI failed to start! Checking logs:"
    cat server.log
fi

# ä¿æŒå®¹å™¨è¿è¡Œï¼Œå¹¶è¾“å‡ºæ—¥å¿—åˆ° stdout ä¾› ModelScope æ”¶é›†
echo "ğŸ“œ Tailing logs..."
tail -f server.log ollama.log
