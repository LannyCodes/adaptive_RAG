# Kaggle ç¯å¢ƒä¼˜åŒ–æŒ‡å— - é¿å…é‡å¤ä¸‹è½½æ¨¡å‹

## ğŸš¨ é—®é¢˜

æ¯æ¬¡ Kaggle ä¼šè¯é‡å¯åï¼ŒOllama æ¨¡å‹éœ€è¦é‡æ–°ä¸‹è½½ï¼ŒMistral æ¨¡å‹çº¦ 4GBï¼Œéå¸¸è€—æ—¶ã€‚

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆæ¨èâ­â­â­â­â­ï¼‰

**æœ€ä½³é€‰æ‹©**ï¼šä¸éœ€è¦ä¿®æ”¹ä»£ç ï¼Œåªéœ€åœ¨ä¸‹è½½æ¨¡å‹æ—¶é€‰æ‹©æ›´å°çš„ç‰ˆæœ¬ã€‚

#### å¯é€‰æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | ä¸‹è½½æ—¶é—´ | è´¨é‡ | æ¨èåœºæ™¯ |
|-----|------|---------|------|---------|
| `mistral` | ~4GB | 5-10åˆ†é’Ÿ | â­â­â­â­â­ | æœ¬åœ°å¼€å‘ |
| `phi` | ~1.6GB | 2-3åˆ†é’Ÿ | â­â­â­â­ | **Kaggleæ¨è** |
| `tinyllama` | ~600MB | 1åˆ†é’Ÿ | â­â­â­ | å¿«é€Ÿæµ‹è¯• |
| `qwen:0.5b` | ~350MB | 30ç§’ | â­â­ | æé€Ÿæµ‹è¯• |

#### ä½¿ç”¨æ–¹æ³•

**é€‰é¡¹ A**: ä¿®æ”¹ `config.py`
```python
# åœ¨ /kaggle/working/adaptive_RAG/config.py ä¸­
LOCAL_LLM = "phi"  # ğŸ‘ˆ æ”¹ä¸º phi æˆ– tinyllama
```

**é€‰é¡¹ B**: è¿è¡Œæ—¶è¦†ç›–ï¼ˆä¸ä¿®æ”¹ä»£ç ï¼‰
```python
# åœ¨ Kaggle Notebook ä¸­
import os
os.environ['LOCAL_LLM_OVERRIDE'] = 'phi'

# ç„¶åæ­£å¸¸å¯¼å…¥
from config import LOCAL_LLM
# LOCAL_LLM ä¼šè‡ªåŠ¨ä½¿ç”¨ 'phi'
```

**é€‰é¡¹ C**: ç›´æ¥åœ¨ä¸‹è½½æ—¶æŒ‡å®š
```python
# ä¸‹è½½æ›´å°çš„æ¨¡å‹
!ollama pull phi  # ä»£æ›¿ mistral

# æˆ–è€…
!ollama pull tinyllama
```

---

### æ–¹æ¡ˆ 2: æŒä¹…åŒ–æ¨¡å‹åˆ° Kaggle Datasetï¼ˆä¸­ç­‰æ¨èâ­â­â­ï¼‰

å°†ä¸‹è½½å¥½çš„æ¨¡å‹ä¿å­˜ä¸º Datasetï¼Œä¸‹æ¬¡ä¼šè¯ç›´æ¥åŠ è½½ã€‚

#### æ­¥éª¤

**ä¼šè¯ 1ï¼ˆé¦–æ¬¡ï¼‰ï¼š**
```python
import subprocess
import shutil
import os

# 1. ä¸‹è½½æ¨¡å‹
subprocess.run(['ollama', 'pull', 'phi'])

# 2. æ‰¾åˆ°æ¨¡å‹å­˜å‚¨ä½ç½®
# Ollama æ¨¡å‹é€šå¸¸å­˜å‚¨åœ¨ ~/.ollama/models
ollama_models = os.path.expanduser('~/.ollama/models')

# 3. å¤åˆ¶åˆ°å·¥ä½œç›®å½•ï¼ˆä¼šè¢«ä¿å­˜ä¸ºè¾“å‡ºï¼‰
if os.path.exists(ollama_models):
    shutil.copytree(
        ollama_models,
        '/kaggle/working/ollama_models',
        dirs_exist_ok=True
    )
    print("âœ… æ¨¡å‹å·²å¤åˆ¶åˆ° /kaggle/working/ollama_models")
    print("ğŸ“Œ ä¼šè¯ç»“æŸåï¼Œå°†æ­¤ç›®å½•ä¿å­˜ä¸º Dataset")

# 4. ä¼šè¯ç»“æŸæ—¶ï¼šSave Version â†’ Save as Dataset
#    å‘½åä¸º: ollama-models-cache
```

**ä¼šè¯ 2ï¼ˆåç»­ï¼‰ï¼š**
```python
import shutil
import os

# 1. ä» Dataset æ¢å¤æ¨¡å‹
models_cache = '/kaggle/input/ollama-models-cache'

if os.path.exists(models_cache):
    print("ğŸ“¥ æ¢å¤ Ollama æ¨¡å‹...")
    
    # åˆ›å»º Ollama æ¨¡å‹ç›®å½•
    ollama_dir = os.path.expanduser('~/.ollama/models')
    os.makedirs(ollama_dir, exist_ok=True)
    
    # å¤åˆ¶æ¨¡å‹æ–‡ä»¶
    shutil.copytree(
        models_cache,
        ollama_dir,
        dirs_exist_ok=True
    )
    
    print("âœ… æ¨¡å‹å·²æ¢å¤ï¼Œæ— éœ€é‡æ–°ä¸‹è½½ï¼")
else:
    print("âš ï¸ æœªæ‰¾åˆ°ç¼“å­˜ï¼Œéœ€è¦é‡æ–°ä¸‹è½½")
```

**æ³¨æ„**ï¼šæ­¤æ–¹æ³•æœ‰å±€é™æ€§ï¼Œå› ä¸º Ollama çš„æ¨¡å‹å­˜å‚¨ç»“æ„å¤æ‚ï¼Œå¯èƒ½ä¸å®Œå…¨å…¼å®¹ã€‚

---

### æ–¹æ¡ˆ 3: ä½¿ç”¨äº‘ç«¯ LLM APIï¼ˆé«˜çº§æ–¹æ¡ˆâ­â­â­â­ï¼‰

å®Œå…¨é¿å…æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨äº‘ç«¯ APIã€‚

#### å¯é€‰ API

1. **OpenAI API**ï¼ˆéœ€ä»˜è´¹ï¼‰
2. **Anthropic Claude API**ï¼ˆéœ€ä»˜è´¹ï¼‰
3. **Hugging Face Inference API**ï¼ˆå…è´¹ï¼Œæœ‰é™é¢ï¼‰
4. **Together AI**ï¼ˆå…è´¹é¢åº¦ï¼‰

#### ä»£ç ä¿®æ”¹ç¤ºä¾‹

ä¿®æ”¹ `entity_extractor.py`:

```python
# åŸä»£ç 
from langchain_community.chat_models import ChatOllama
self.llm = ChatOllama(model=LOCAL_LLM, format="json", temperature=0)

# æ”¹ä¸ºä½¿ç”¨ OpenAI API
from langchain_openai import ChatOpenAI
self.llm = ChatOpenAI(
    model="gpt-3.5-turbo",  # æˆ– gpt-4
    temperature=0,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# æˆ–ä½¿ç”¨ Hugging Face
from langchain_community.llms import HuggingFaceHub
self.llm = HuggingFaceHub(
    repo_id="mistralai/Mistral-7B-Instruct-v0.1",
    huggingfacehub_api_token=os.getenv("HUGGINGFACE_API_TOKEN")
)
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ— éœ€ä¸‹è½½æ¨¡å‹
- âœ… é€Ÿåº¦å¿«ï¼ˆäº‘ç«¯ GPUï¼‰
- âœ… è´¨é‡å¥½ï¼ˆGPT-4 ç­‰é«˜çº§æ¨¡å‹ï¼‰

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦ API Key
- âŒ å¯èƒ½äº§ç”Ÿè´¹ç”¨
- âŒ ä¾èµ–ç½‘ç»œ

---

### æ–¹æ¡ˆ 4: é¢„æ„å»º Docker é•œåƒï¼ˆæŠ€æœ¯æ–¹æ¡ˆâ­â­ï¼‰

åˆ›å»ºåŒ…å«é¢„ä¸‹è½½æ¨¡å‹çš„ Docker é•œåƒã€‚

**æ­¥éª¤**ï¼š
1. æœ¬åœ°æ„å»ºåŒ…å« Ollama + æ¨¡å‹çš„ Docker é•œåƒ
2. æ¨é€åˆ° Docker Hub
3. åœ¨ Kaggle ä¸­æ‹‰å–è¯¥é•œåƒ

**å±€é™**ï¼šKaggle å¯¹ Docker æ”¯æŒæœ‰é™ã€‚

---

## ğŸ¯ æœ€ä½³å®è·µæ¨è

### æ¨èç»„åˆç­–ç•¥

**å¿«é€Ÿå¼€å‘/æµ‹è¯•**ï¼š
```python
# ä½¿ç”¨ phi æ¨¡å‹ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ï¼‰
LOCAL_LLM = "phi"
```

**ç”Ÿäº§ç¯å¢ƒ**ï¼š
```python
# ä½¿ç”¨äº‘ç«¯ APIï¼ˆé€Ÿåº¦å¿«ã€è´¨é‡é«˜ï¼‰
# åœ¨ Kaggle Secrets ä¸­è®¾ç½® OPENAI_API_KEY
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-3.5-turbo")
```

**å®Œå…¨ç¦»çº¿**ï¼š
```python
# ä½¿ç”¨ tinyllamaï¼ˆæœ€å¿«ä¸‹è½½ï¼‰
LOCAL_LLM = "tinyllama"
```

---

## ğŸ“‹ Kaggle å®Œæ•´å·¥ä½œæµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

### å•å…ƒæ ¼ 1: åˆå§‹åŒ–
```python
import os, subprocess, sys

os.chdir('/kaggle/working')
if not os.path.exists('adaptive_RAG'):
    subprocess.run(['git', 'clone', 'https://github.com/LannyCodes/adaptive_RAG.git'])

os.chdir('adaptive_RAG')

# ä¿®æ”¹é…ç½®ä½¿ç”¨æ›´å°çš„æ¨¡å‹
with open('config.py', 'r') as f:
    content = f.read()

content = content.replace('LOCAL_LLM = "mistral"', 'LOCAL_LLM = "phi"')

with open('config.py', 'w') as f:
    f.write(content)

print("âœ… å·²åˆ‡æ¢åˆ° phi æ¨¡å‹")

sys.path.insert(0, '/kaggle/working/adaptive_RAG')
```

### å•å…ƒæ ¼ 2: å®‰è£… Ollama
```python
# å®‰è£… Ollama
subprocess.run('curl -fsSL https://ollama.com/install.sh | sh', shell=True)

# å¯åŠ¨æœåŠ¡
subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
time.sleep(15)
```

### å•å…ƒæ ¼ 3: ä¸‹è½½ä¼˜åŒ–çš„æ¨¡å‹
```python
import time

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
print("ğŸ“¥ ä¸‹è½½ phi æ¨¡å‹ï¼ˆçº¦1.6GBï¼Œ2-3åˆ†é’Ÿï¼‰...")
subprocess.run(['ollama', 'pull', 'phi'])

print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
```

### å•å…ƒæ ¼ 4: å®‰è£…ä¾èµ–å¹¶è¿è¡Œ
```python
!pip install -r requirements_graphrag.txt -q

# ç»§ç»­æ‚¨çš„å¤„ç†...
```

---

## ğŸ”¢ æ—¶é—´å¯¹æ¯”

| åœºæ™¯ | Mistral | Phi | TinyLlama | äº‘ç«¯API |
|-----|---------|-----|-----------|---------|
| **é¦–æ¬¡ä¸‹è½½** | 5-10åˆ†é’Ÿ | 2-3åˆ†é’Ÿ | 1åˆ†é’Ÿ | 0åˆ†é’Ÿ |
| **åç»­ä¼šè¯** | 5-10åˆ†é’Ÿ | 2-3åˆ†é’Ÿ | 1åˆ†é’Ÿ | 0åˆ†é’Ÿ |
| **æ¯å‘¨æ€»è€—æ—¶**<br>ï¼ˆ5æ¬¡ä¼šè¯ï¼‰ | 25-50åˆ†é’Ÿ | 10-15åˆ†é’Ÿ | 5åˆ†é’Ÿ | 0åˆ†é’Ÿ |

---

## ğŸ’° æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | æ—¶é—´æˆæœ¬ | é‡‘é’±æˆæœ¬ | è´¨é‡ |
|-----|---------|---------|------|
| Mistral | é«˜ âŒ | å…è´¹ âœ… | é«˜ âœ… |
| Phi | ä¸­ âœ… | å…è´¹ âœ… | ä¸­é«˜ âœ… |
| TinyLlama | ä½ âœ… | å…è´¹ âœ… | ä¸­ âš ï¸ |
| GPT-3.5 API | æä½ âœ… | çº¦$0.5-2/å¤© âš ï¸ | æé«˜ âœ… |

---

## ğŸ å¿«é€Ÿé…ç½®è„šæœ¬

å°†ä»¥ä¸‹ä»£ç ä¿å­˜ä¸º `KAGGLE_QUICK_START.py`ï¼š

```python
"""
Kaggle å¿«é€Ÿå¯åŠ¨è„šæœ¬ - è‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–é…ç½®
"""

import os
import subprocess
import sys
import time

print("ğŸš€ Kaggle å¿«é€Ÿå¯åŠ¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
print("="*60)

# 1. å…‹éš†é¡¹ç›®
os.chdir('/kaggle/working')
if not os.path.exists('adaptive_RAG'):
    subprocess.run(['git', 'clone', 'https://github.com/LannyCodes/adaptive_RAG.git'])

os.chdir('adaptive_RAG')

# 2. è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼ˆæ ¹æ®é…ç½®ï¼‰
USE_SMALL_MODEL = True  # ğŸ‘ˆ æ”¹ä¸º False ä½¿ç”¨ Mistral

if USE_SMALL_MODEL:
    MODEL_NAME = "phi"
    print("âœ… ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹: phi (1.6GB)")
else:
    MODEL_NAME = "mistral"
    print("âœ… ä½¿ç”¨æ ‡å‡†æ¨¡å‹: mistral (4GB)")

# ä¿®æ”¹é…ç½®
with open('config.py', 'r') as f:
    content = f.read()

content = content.replace(
    'LOCAL_LLM = "mistral"',
    f'LOCAL_LLM = "{MODEL_NAME}"'
)

with open('config.py', 'w') as f:
    f.write(content)

# 3. å®‰è£… Ollama
check = subprocess.run(['which', 'ollama'], capture_output=True)
if check.returncode != 0:
    print("ğŸ“¥ å®‰è£… Ollama...")
    subprocess.run('curl -fsSL https://ollama.com/install.sh | sh', shell=True)

# 4. å¯åŠ¨æœåŠ¡
subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
time.sleep(15)

# 5. ä¸‹è½½æ¨¡å‹
print(f"ğŸ“¦ ä¸‹è½½ {MODEL_NAME} æ¨¡å‹...")
subprocess.run(['ollama', 'pull', MODEL_NAME])

# 6. å®‰è£…ä¾èµ–
print("ğŸ“¦ å®‰è£…ä¾èµ–...")
subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements_graphrag.txt', '-q'])

sys.path.insert(0, '/kaggle/working/adaptive_RAG')

print("\n" + "="*60)
print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼")
print("="*60)
print(f"\nğŸ“Œ ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
print("ğŸ“Œ ç°åœ¨å¯ä»¥è¿è¡Œ GraphRAG ç´¢å¼•äº†")
```

---

## æ€»ç»“

**æœ€æ¨èçš„è§£å†³æ–¹æ¡ˆ**ï¼š

1. â­â­â­â­â­ **ä½¿ç”¨ Phi æ¨¡å‹** - å¹³è¡¡äº†é€Ÿåº¦å’Œè´¨é‡
2. â­â­â­â­ **ä½¿ç”¨äº‘ç«¯ API** - é€‚åˆç”Ÿäº§ç¯å¢ƒ
3. â­â­â­ **ä½¿ç”¨ TinyLlama** - å¿«é€Ÿæµ‹è¯•

**å®é™…æ“ä½œ**ï¼š
- åªéœ€å°† `config.py` ä¸­çš„ `LOCAL_LLM = "mistral"` æ”¹ä¸º `LOCAL_LLM = "phi"`
- æˆ–åœ¨ Kaggle ä¸­è¿è¡Œæ—¶è‡ªåŠ¨æ›¿æ¢ï¼ˆè§å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼‰

è¿™æ ·æ¯æ¬¡ä¼šè¯åªéœ€ 2-3 åˆ†é’Ÿä¸‹è½½æ¨¡å‹ï¼Œè€Œä¸æ˜¯ 5-10 åˆ†é’Ÿï¼
