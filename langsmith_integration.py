"""
LangSmith é›†æˆæ¨¡å—
ä¸ºè‡ªé€‚åº” RAG ç³»ç»Ÿæä¾›å¯è§‚æµ‹æ€§å’Œè¿½è¸ªåŠŸèƒ½

åŠŸèƒ½åˆ—è¡¨:
1. åŸºç¡€è¿½è¸ª: è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ LangChain/LangGraph æ“ä½œ
2. è‡ªå®šä¹‰è¿½è¸ª: è®°å½•æ£€ç´¢ã€ç”Ÿæˆã€è¯„åˆ†ç­‰è‡ªå®šä¹‰äº‹ä»¶
3. æ€§èƒ½ç›‘æ§: æ”¶é›†å’Œè¿½è¸ªå„é˜¶æ®µæ€§èƒ½æŒ‡æ ‡
4. å‘Šè­¦ç³»ç»Ÿ: è®¾ç½®é˜ˆå€¼å¹¶åœ¨æ€§èƒ½å¼‚å¸¸æ—¶è§¦å‘å‘Šè­¦
5. ç»Ÿè®¡åˆ†æ: æä¾›æŸ¥è¯¢ç»Ÿè®¡å’Œæ€§èƒ½åˆ†ææŠ¥å‘Š
"""

import os
import time
import json
from typing import Optional, Dict, Any, List, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from langchain.callbacks import LangChainTracer
from langchain_core.runnables import RunnableConfig
import langsmith
from enum import Enum


class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    name: str
    value: float
    unit: str = "ms"
    timestamp: datetime = field(default_factory=datetime.now)
    tags: Dict[str, str] = field(default_factory=dict)


@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™"""
    name: str
    metric_name: str
    threshold: float
    operator: str  # ">", "<", ">=", "<=", "=="
    level: AlertLevel
    enabled: bool = True
    cooldown_seconds: int = 300  # å†·å´æ—¶é—´ï¼Œé¿å…é‡å¤å‘Šè­¦


class LangSmithManager:
    """LangSmith é›†æˆç®¡ç†å™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(
        self,
        project_name: str = "adaptive-rag-project",
        api_key: Optional[str] = None,
        tracing_enabled: bool = True,
        enable_performance_monitoring: bool = True,
        enable_alerts: bool = True
    ):
        """
        åˆå§‹åŒ– LangSmith ç®¡ç†å™¨
        
        Args:
            project_name: LangSmith é¡¹ç›®åç§°
            api_key: LangSmith API å¯†é’¥
            tracing_enabled: æ˜¯å¦å¯ç”¨åŸºç¡€è¿½è¸ª
            enable_performance_monitoring: æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
            enable_alerts: æ˜¯å¦å¯ç”¨å‘Šè­¦ç³»ç»Ÿ
        """
        self.project_name = project_name
        self.tracing_enabled = tracing_enabled
        self.enable_performance_monitoring = enable_performance_monitoring
        self.enable_alerts = enable_alerts
        self.tracer: Optional[LangChainTracer] = None
        
        # æ€§èƒ½æŒ‡æ ‡å­˜å‚¨
        self.metrics: List[PerformanceMetric] = []
        self.query_history: List[Dict] = []
        
        # å‘Šè­¦ç³»ç»Ÿ
        self.alert_rules: List[AlertRule] = []
        self.alert_callbacks: List[Callable] = []
        self.last_alert_times: Dict[str, datetime] = {}
        
        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å– API å¯†é’¥
        self.api_key = api_key or os.environ.get("LANGSMITH_API_KEY")
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.auto_flush = os.environ.get("LANGSMITH_AUTO_FLUSH", "true").lower() == "true"
        self.flush_interval = int(os.environ.get("LANGSMITH_FLUSH_INTERVAL", "60"))
        
        if self.tracing_enabled and self.api_key:
            self._setup_tracer()
            self._setup_default_alert_rules()
        elif self.tracing_enabled:
            print("âš ï¸ æœªæ‰¾åˆ° LangSmith API å¯†é’¥ï¼Œè¿½è¸ªåŠŸèƒ½å°†ä¸å¯ç”¨")
    
    def _setup_tracer(self):
        """è®¾ç½® LangChain è¿½è¸ªå™¨"""
        try:
            self.tracer = LangChainTracer(
                project_name=self.project_name,
                api_key=self.api_key
            )
            print(f"âœ… LangSmith è¿½è¸ªå·²å¯ç”¨ï¼Œé¡¹ç›®: {self.project_name}")
        except Exception as e:
            print(f"âŒ LangSmith è¿½è¸ªè®¾ç½®å¤±è´¥: {e}")
            self.tracer = None
    
    def _setup_default_alert_rules(self):
        """è®¾ç½®é»˜è®¤å‘Šè­¦è§„åˆ™"""
        # é»˜è®¤å‘Šè­¦è§„åˆ™ï¼šæ£€ç´¢è¶…æ—¶ã€ç”Ÿæˆè¶…æ—¶ã€Tokenä½¿ç”¨è¿‡é«˜ç­‰
        self.add_alert_rule(AlertRule(
            name="æ£€ç´¢è¶…æ—¶å‘Šè­¦",
            metric_name="retrieve_latency",
            threshold=30000,  # 30ç§’
            operator=">",
            level=AlertLevel.WARNING
        ))
        
        self.add_alert_rule(AlertRule(
            name="ç”Ÿæˆè¶…æ—¶å‘Šè­¦",
            metric_name="generate_latency",
            threshold=60000,  # 60ç§’
            operator=">",
            level=AlertLevel.WARNING
        ))
        
        self.add_alert_rule(AlertRule(
            name="æ€»å“åº”æ—¶é—´å‘Šè­¦",
            metric_name="total_latency",
            threshold=120000,  # 120ç§’
            operator=">",
            level=AlertLevel.ERROR
        ))
        
        self.add_alert_rule(AlertRule(
            name="Tokenä½¿ç”¨è¿‡é«˜å‘Šè­¦",
            metric_name="total_tokens",
            threshold=4000,
            operator=">",
            level=AlertLevel.INFO
        ))
    
    def get_callback_config(self) -> RunnableConfig:
        """
        è·å–å›è°ƒé…ç½®ï¼Œç”¨äº LangGraph ç¼–è¯‘
        
        Returns:
            RunnableConfig: åŒ…å«å›è°ƒçš„é…ç½®
        """
        if self.tracer and self.tracing_enabled:
            return {
                "callbacks": [self.tracer]
            }
        return {}
    
    # ==================== è‡ªå®šä¹‰è¿½è¸ªåŠŸèƒ½ ====================
    
    def log_custom_event(
        self,
        name: str,
        inputs: Dict[str, Any],
        outputs: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        è®°å½•è‡ªå®šä¹‰äº‹ä»¶åˆ° LangSmith
        
        Args:
            name: äº‹ä»¶åç§°
            inputs: è¾“å…¥æ•°æ®
            outputs: è¾“å‡ºæ•°æ®ï¼ˆå¯é€‰ï¼‰
            metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        if not (self.tracing_enabled and self.api_key):
            return
        
        try:
            client = langsmith.Client(api_key=self.api_key)
            
            run_data = {
                "name": name,
                "inputs": inputs,
                "outputs": outputs or {},
                "metadata": metadata or {},
                "run_type": "custom_event"
            }
            
            # ä½¿ç”¨create_runè®°å½•äº‹ä»¶
            client.create_run(
                project_name=self.project_name,
                **run_data
            )
            
        except Exception as e:
            print(f"âš ï¸ è®°å½•è‡ªå®šä¹‰äº‹ä»¶å¤±è´¥: {e}")
    
    def log_retrieval_event(
        self,
        query: str,
        documents_count: int,
        retrieval_time: float,
        top_k: int,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        è®°å½•æ£€ç´¢äº‹ä»¶
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            documents_count: æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡
            retrieval_time: æ£€ç´¢è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
            top_k: è¯·æ±‚çš„top-kå€¼
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        if not self.enable_performance_monitoring:
            return
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.record_metric("retrieve_latency", retrieval_time, "ms", {
            "query_length": str(len(query)),
            "documents_count": str(documents_count),
            "top_k": str(top_k)
        })
        
        # è®°å½•æ£€ç´¢äº‹ä»¶
        self.log_custom_event(
            name="retrieval",
            inputs={"query": query, "top_k": top_k},
            outputs={
                "documents_count": documents_count,
                "retrieval_time_ms": retrieval_time
            },
            metadata=metadata
        )
    
    def log_generation_event(
        self,
        prompt: str,
        generation: str,
        generation_time: float,
        tokens_used: int,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        è®°å½•ç”Ÿæˆäº‹ä»¶
        
        Args:
            prompt: æç¤ºè¯
            generation: ç”Ÿæˆçš„æ–‡æœ¬
            generation_time: ç”Ÿæˆè€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
            tokens_used: ä½¿ç”¨çš„tokenæ•°é‡
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        if not self.enable_performance_monitoring:
            return
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.record_metric("generate_latency", generation_time, "ms", {
            "prompt_length": str(len(prompt)),
            "generation_length": str(len(generation)),
            "tokens_used": str(tokens_used)
        })
        
        self.record_metric("generation_length", len(generation), "chars")
        self.record_metric("total_tokens", tokens_used, "tokens")
        
        # è®°å½•ç”Ÿæˆäº‹ä»¶
        self.log_custom_event(
            name="generation",
            inputs={"prompt": prompt[:500] if prompt else ""},  # é™åˆ¶é•¿åº¦
            outputs={
                "generation_length": len(generation),
                "generation_time_ms": generation_time,
                "tokens_used": tokens_used
            },
            metadata=metadata
        )
    
    def log_query_complete(
        self,
        question: str,
        answer: str,
        total_latency: float,
        routing_decision: str,
        metrics: Optional[Dict[str, Any]] = None
    ):
        """
        è®°å½•å®Œæ•´æŸ¥è¯¢å®Œæˆäº‹ä»¶
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: ç”Ÿæˆç­”æ¡ˆ
            total_latency: æ€»è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
            routing_decision: è·¯ç”±å†³ç­–
            metrics: è¯„ä¼°æŒ‡æ ‡
        """
        if not self.enable_performance_monitoring:
            return
        
        # è®°å½•æ€»è€—æ—¶
        self.record_metric("total_latency", total_latency, "ms", {
            "routing_decision": routing_decision,
            "question_length": str(len(question)),
            "answer_length": str(len(answer))
        })
        
        # è®°å½•æŸ¥è¯¢å†å²
        query_record = {
            "question": question,
            "answer": answer,
            "total_latency_ms": total_latency,
            "routing_decision": routing_decision,
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics or {}
        }
        self.query_history.append(query_record)
        
        # é™åˆ¶å†å²è®°å½•å¤§å°
        if len(self.query_history) > 1000:
            self.query_history = self.query_history[-500:]
        
        # è®°å½•å®Œæ•´æŸ¥è¯¢äº‹ä»¶
        self.log_custom_event(
            name="query_complete",
            inputs={"question": question, "routing_decision": routing_decision},
            outputs={
                "answer_length": len(answer),
                "total_latency_ms": total_latency,
                "metrics": metrics or {}
            }
        )
    
    # ==================== æ€§èƒ½ç›‘æ§åŠŸèƒ½ ====================
    
    def record_metric(
        self,
        name: str,
        value: float,
        unit: str = "ms",
        tags: Optional[Dict[str, str]] = None
    ):
        """
        è®°å½•æ€§èƒ½æŒ‡æ ‡
        
        Args:
            name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            unit: å•ä½
            tags: æ ‡ç­¾
        """
        metric = PerformanceMetric(
            name=name,
            value=value,
            unit=unit,
            tags=tags or {}
        )
        self.metrics.append(metric)
        
        # é™åˆ¶æŒ‡æ ‡æ•°é‡
        if len(self.metrics) > 10000:
            self.metrics = self.metrics[-5000:]
        
        # æ£€æŸ¥å‘Šè­¦è§„åˆ™
        if self.enable_alerts:
            self._check_alert_rules(metric)
    
    def get_metric_stats(self, metric_name: str) -> Dict[str, float]:
        """
        è·å–æŒ‡å®šæŒ‡æ ‡çš„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            metric_name: æŒ‡æ ‡åç§°
            
        Returns:
            Dict: åŒ…å«count, sum, avg, min, max, p50, p95, p99çš„å­—å…¸
        """
        relevant_metrics = [m for m in self.metrics if m.name == metric_name]
        
        if not relevant_metrics:
            return {
                "count": 0,
                "sum": 0,
                "avg": 0,
                "min": 0,
                "max": 0,
                "p50": 0,
                "p95": 0,
                "p99": 0
            }
        
        values = sorted([m.value for m in relevant_metrics])
        count = len(values)
        total = sum(values)
        
        def percentile(p: float) -> float:
            if not values:
                return 0
            idx = int(len(values) * p / 100)
            return values[min(idx, len(values) - 1)]
        
        return {
            "count": count,
            "sum": total,
            "avg": total / count,
            "min": min(values),
            "max": max(values),
            "p50": percentile(50),
            "p95": percentile(95),
            "p99": percentile(99)
        }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        
        Returns:
            Dict: æ€§èƒ½æŠ¥å‘Š
        """
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_queries": len(self.query_history),
            "total_metrics": len(self.metrics),
            "metrics_summary": {},
            "query_stats": {},
            "alerts_triggered": len(self.last_alert_times)
        }
        
        # å…³é”®æŒ‡æ ‡ç»Ÿè®¡
        key_metrics = [
            "retrieve_latency",
            "generate_latency",
            "total_latency",
            "total_tokens"
        ]
        
        for metric_name in key_metrics:
            stats = self.get_metric_stats(metric_name)
            if stats["count"] > 0:
                report["metrics_summary"][metric_name] = stats
        
        # æŸ¥è¯¢ç»Ÿè®¡
        if self.query_history:
            latencies = [q["total_latency_ms"] for q in self.query_history if "total_latency_ms" in q]
            if latencies:
                report["query_stats"] = {
                    "total_queries": len(self.query_history),
                    "avg_latency_ms": sum(latencies) / len(latencies),
                    "min_latency_ms": min(latencies),
                    "max_latency_ms": max(latencies),
                    "p50_latency_ms": sorted(latencies)[len(latencies) // 2],
                    "p95_latency_ms": sorted(latencies)[int(len(latencies) * 0.95)]
                }
        
        return report
    
    # ==================== å‘Šè­¦ç³»ç»Ÿ ====================
    
    def add_alert_rule(self, rule: AlertRule):
        """
        æ·»åŠ å‘Šè­¦è§„åˆ™
        
        Args:
            rule: å‘Šè­¦è§„åˆ™
        """
        self.alert_rules.append(rule)
        print(f"âœ… æ·»åŠ å‘Šè­¦è§„åˆ™: {rule.name}")
    
    def remove_alert_rule(self, rule_name: str):
        """
        ç§»é™¤å‘Šè­¦è§„åˆ™
        
        Args:
            rule_name: è§„åˆ™åç§°
        """
        self.alert_rules = [r for r in self.alert_rules if r.name != rule_name]
    
    def add_alert_callback(self, callback: Callable):
        """
        æ·»åŠ å‘Šè­¦å›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (AlertRule, PerformanceMetric) å‚æ•°
        """
        self.alert_callbacks.append(callback)
    
    def _check_alert_rules(self, metric: PerformanceMetric):
        """
        æ£€æŸ¥å‘Šè­¦è§„åˆ™
        
        Args:
            metric: æ€§èƒ½æŒ‡æ ‡
        """
        for rule in self.alert_rules:
            if not rule.enabled:
                continue
            
            if rule.metric_name != metric.name:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸå†…
            if rule.name in self.last_alert_times:
                elapsed = (datetime.now() - self.last_alert_times[rule.name]).total_seconds()
                if elapsed < rule.cooldown_seconds:
                    continue
            
            # æ£€æŸ¥é˜ˆå€¼
            triggered = False
            if rule.operator == ">" and metric.value > rule.threshold:
                triggered = True
            elif rule.operator == "<" and metric.value < rule.threshold:
                triggered = True
            elif rule.operator == ">=" and metric.value >= rule.threshold:
                triggered = True
            elif rule.operator == "<=" and metric.value <= rule.threshold:
                triggered = True
            elif rule.operator == "==" and metric.value == rule.threshold:
                triggered = True
            
            if triggered:
                self._trigger_alert(rule, metric)
                self.last_alert_times[rule.name] = datetime.now()
    
    def _trigger_alert(self, rule: AlertRule, metric: PerformanceMetric):
        """
        è§¦å‘å‘Šè­¦
        
        Args:
            rule: è§¦å‘çš„è§„åˆ™
            metric: è§¦å‘å‘Šè­¦çš„æŒ‡æ ‡
        """
        alert_data = {
            "rule_name": rule.name,
            "level": rule.level.value,
            "metric_name": metric.name,
            "value": metric.value,
            "threshold": rule.threshold,
            "timestamp": datetime.now().isoformat()
        }
        
        # æ‰“å°å‘Šè­¦ä¿¡æ¯
        level_str = {
            AlertLevel.INFO: "â„¹ï¸",
            AlertLevel.WARNING: "âš ï¸",
            AlertLevel.ERROR: "âŒ",
            AlertLevel.CRITICAL: "ğŸš¨"
        }
        
        print(f"{level_str[rule.level]} [å‘Šè­¦] {rule.name}")
        print(f"   æŒ‡æ ‡: {metric.name} = {metric.value:.2f}{metric.unit} (é˜ˆå€¼: {rule.threshold})")
        
        # è°ƒç”¨å‘Šè­¦å›è°ƒ
        for callback in self.alert_callbacks:
            try:
                callback(rule, metric)
            except Exception as e:
                print(f"âš ï¸ å‘Šè­¦å›è°ƒå¤±è´¥: {e}")
        
        # è®°å½•åˆ° LangSmith
        if self.tracing_enabled and self.api_key:
            self.log_custom_event(
                name="alert_triggered",
                inputs=alert_data,
                outputs={},
                metadata={"level": rule.level.value}
            )
    
    # ==================== ç»Ÿè®¡å’ŒæŠ¥å‘ŠåŠŸèƒ½ ====================
    
    def get_query_history(self, limit: int = 100) -> List[Dict]:
        """
        è·å–æŸ¥è¯¢å†å²
        
        Args:
            limit: è¿”å›çš„æœ€å¤§è®°å½•æ•°
            
        Returns:
            List: æŸ¥è¯¢å†å²è®°å½•
        """
        return self.query_history[-limit:]
    
    def export_metrics(self, format: str = "json") -> str:
        """
        å¯¼å‡ºæŒ‡æ ‡æ•°æ®
        
        Args:
            format: å¯¼å‡ºæ ¼å¼ ("json" æˆ– "csv")
            
        Returns:
            str: æ ¼å¼åŒ–åçš„æŒ‡æ ‡æ•°æ®
        """
        if format == "json":
            return json.dumps([{
                "name": m.name,
                "value": m.value,
                "unit": m.unit,
                "timestamp": m.timestamp.isoformat(),
                "tags": m.tags
            } for m in self.metrics], indent=2, ensure_ascii=False)
        
        elif format == "csv":
            lines = ["name,value,unit,timestamp,tags"]
            for m in self.metrics:
                tags_str = "|".join([f"{k}={v}" for k, v in m.tags.items()])
                lines.append(f"{m.name},{m.value},{m.unit},{m.timestamp.isoformat()},{tags_str}")
            return "\n".join(lines)
        
        return ""
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        report = self.get_performance_report()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š LangSmith æ€§èƒ½ç›‘æ§æ‘˜è¦")
        print("=" * 60)
        
        print(f"æ€»æŸ¥è¯¢æ•°: {report['total_queries']}")
        print(f"æ€»æŒ‡æ ‡æ•°: {report['total_metrics']}")
        print(f"è§¦å‘å‘Šè­¦æ•°: {report['alerts_triggered']}")
        
        if report["query_stats"]:
            stats = report["query_stats"]
            print(f"\næŸ¥è¯¢å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  å¹³å‡å»¶è¿Ÿ: {stats['avg_latency_ms']:.2f}ms")
            print(f"  æœ€å°å»¶è¿Ÿ: {stats['min_latency_ms']:.2f}ms")
            print(f"  æœ€å¤§å»¶è¿Ÿ: {stats['max_latency_ms']:.2f}ms")
            print(f"  P50å»¶è¿Ÿ: {stats['p50_latency_ms']:.2f}ms")
            print(f"  P95å»¶è¿Ÿ: {stats['p95_latency_ms']:.2f}ms")
        
        print("\nå…³é”®æŒ‡æ ‡ç»Ÿè®¡:")
        for metric_name, stats in report["metrics_summary"].items():
            if stats["count"] > 0:
                print(f"  {metric_name}:")
                print(f"    å¹³å‡å€¼: {stats['avg']:.2f}")
                print(f"    æœ€å°å€¼: {stats['min']:.2f}")
                print(f"    æœ€å¤§å€¼: {stats['max']:.2f}")
                print(f"    P95: {stats['p95']:.2f}")
        
        print("=" * 60)


def setup_langsmith(
    project_name: str = "adaptive-rag-project",
    enable_tracing: bool = None,
    enable_performance_monitoring: bool = True,
    enable_alerts: bool = True
) -> LangSmithManager:
    """
    è®¾ç½® LangSmith é›†æˆ
    
    Args:
        project_name: é¡¹ç›®åç§°
        enable_tracing: æ˜¯å¦å¯ç”¨è¿½è¸ªï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        enable_performance_monitoring: æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
        enable_alerts: æ˜¯å¦å¯ç”¨å‘Šè­¦ç³»ç»Ÿ
    
    Returns:
        LangSmithManager å®ä¾‹
    """
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    if enable_tracing is None:
        enable_tracing = os.environ.get("LANGSMITH_TRACING", "true").lower() == "true"
    
    api_key = os.environ.get("LANGSMITH_API_KEY")
    
    # å¦‚æœæ²¡æœ‰ API å¯†é’¥ä¸”è¿½è¸ªå·²å¯ç”¨ï¼Œå‘å‡ºè­¦å‘Š
    if enable_tracing and not api_key:
        print("âš ï¸ LANGSMITH_TRACING=true ä½†æœªè®¾ç½® LANGSMITH_API_KEY")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® LANGSMITH_API_KEY")
        enable_tracing = False
    
    manager = LangSmithManager(
        project_name=project_name,
        api_key=api_key,
        tracing_enabled=enable_tracing,
        enable_performance_monitoring=enable_performance_monitoring,
        enable_alerts=enable_alerts
    )
    
    return manager