{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 GraphRAG GPU检测与测试 - Google Colab版本\n",
    "\n",
    "本Notebook用于在Google Colab上检测GPU可用性并测试GraphRAG系统的性能。\n",
    "\n",
    "## 📋 使用步骤\n",
    "\n",
    "1. **启用GPU**: 运行时 → 更改运行时类型 → 硬件加速器 → GPU (T4)\n",
    "2. **运行所有单元格**: 依次执行下面的代码\n",
    "3. **查看结果**: 检查GPU加速效果\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ GPU环境检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测GPU可用性\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🔍 GPU环境检测\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PyTorch GPU检测\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\n✅ CUDA可用: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"   GPU数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"   当前GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"   GPU名称: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA版本: {torch.version.cuda}\")\n",
    "    \n",
    "    # 显存信息\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"   总显存: {total_memory:.2f} GB\")\n",
    "    \n",
    "    # nvidia-smi信息\n",
    "    print(\"\\n📊 nvidia-smi 输出:\")\n",
    "    print(\"-\"*60)\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"\\n⚠️  警告: 未检测到GPU\")\n",
    "    print(\"   请检查: 运行时 → 更改运行时类型 → 硬件加速器 → GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ GPU性能基准测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU vs CPU 性能对比\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"⚡ GPU vs CPU 矩阵运算性能测试\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 测试参数\n",
    "matrix_size = 5000\n",
    "\n",
    "# CPU测试\n",
    "print(f\"\\n🔵 CPU测试 (矩阵大小: {matrix_size}x{matrix_size})\")\n",
    "a_cpu = torch.randn(matrix_size, matrix_size)\n",
    "b_cpu = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "start = time.time()\n",
    "c_cpu = torch.mm(a_cpu, b_cpu)\n",
    "cpu_time = time.time() - start\n",
    "print(f\"   CPU时间: {cpu_time:.2f} 秒\")\n",
    "\n",
    "# GPU测试\n",
    "if cuda_available:\n",
    "    print(f\"\\n🟢 GPU测试 (矩阵大小: {matrix_size}x{matrix_size})\")\n",
    "    a_gpu = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    b_gpu = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    \n",
    "    # 预热GPU\n",
    "    _ = torch.mm(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    c_gpu = torch.mm(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"   GPU时间: {gpu_time:.2f} 秒\")\n",
    "    \n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\n🚀 加速比: {speedup:.2f}x\")\n",
    "    print(f\"   GPU比CPU快 {speedup:.1f} 倍!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  跳过GPU测试（GPU不可用）\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ 安装GraphRAG依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆项目（如果需要）\n",
    "import os\n",
    "\n",
    "print(\"📦 安装GraphRAG依赖...\\n\")\n",
    "\n",
    "# 安装核心依赖\n",
    "!pip install -q langchain langchain-community langchain-core langgraph\n",
    "!pip install -q chromadb sentence-transformers transformers\n",
    "!pip install -q tiktoken beautifulsoup4 requests\n",
    "!pip install -q tavily-python python-dotenv\n",
    "!pip install -q networkx python-louvain\n",
    "!pip install -q torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"\\n✅ 依赖安装完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ 上传项目文件\n",
    "\n",
    "**选项A**: 从GitHub克隆\n",
    "```python\n",
    "!git clone https://github.com/your-repo/adaptive_RAG.git\n",
    "%cd adaptive_RAG\n",
    "```\n",
    "\n",
    "**选项B**: 手动上传文件到Colab\n",
    "- 使用左侧文件浏览器上传以下核心文件：\n",
    "  - `config.py`\n",
    "  - `entity_extractor.py`\n",
    "  - `knowledge_graph.py`\n",
    "  - `graph_indexer.py`\n",
    "  - `graph_retriever.py`\n",
    "  - `.env` (包含API密钥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建必要的目录\n",
    "!mkdir -p data\n",
    "\n",
    "# 如果使用选项A，运行下面的命令\n",
    "# !git clone YOUR_REPO_URL\n",
    "# %cd adaptive_RAG\n",
    "\n",
    "print(\"✅ 目录准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ 配置API密钥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置API密钥（替换为您的真实密钥）\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"🔑 配置API密钥\\n\")\n",
    "\n",
    "# 方式1: 直接设置（不安全，仅用于测试）\n",
    "# os.environ['TAVILY_API_KEY'] = 'your_tavily_api_key_here'\n",
    "\n",
    "# 方式2: 安全输入\n",
    "if 'TAVILY_API_KEY' not in os.environ:\n",
    "    os.environ['TAVILY_API_KEY'] = getpass('输入 TAVILY_API_KEY: ')\n",
    "    print(\"✅ TAVILY_API_KEY 已设置\")\n",
    "else:\n",
    "    print(\"✅ TAVILY_API_KEY 已存在\")\n",
    "\n",
    "print(\"\\n注意: GraphRAG在Colab上使用HuggingFace嵌入，不需要NOMIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ 简化版GraphRAG测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化版GraphRAG核心组件\n",
    "# 适用于Colab快速测试，无需完整项目文件\n",
    "\n",
    "from typing import List, Dict\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "class SimpleGraphRAG:\n",
    "    \"\"\"简化版GraphRAG用于GPU性能测试\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu=True):\n",
    "        print(\"🚀 初始化SimpleGraphRAG...\")\n",
    "        \n",
    "        # 检测设备\n",
    "        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"   设备: {self.device.upper()}\")\n",
    "        \n",
    "        # 加载嵌入模型\n",
    "        print(f\"   加载嵌入模型...\")\n",
    "        self.embedder = SentenceTransformer(\n",
    "            'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # 知识图谱\n",
    "        self.graph = nx.Graph()\n",
    "        self.entities = {}\n",
    "        \n",
    "        print(\"✅ 初始化完成!\")\n",
    "    \n",
    "    def add_sample_data(self):\n",
    "        \"\"\"添加示例数据\"\"\"\n",
    "        print(\"\\n📊 添加示例数据...\")\n",
    "        \n",
    "        # 示例实体\n",
    "        entities = [\n",
    "            {\"name\": \"LLM\", \"type\": \"CONCEPT\", \"desc\": \"大语言模型\"},\n",
    "            {\"name\": \"GPT\", \"type\": \"TECHNOLOGY\", \"desc\": \"生成式预训练转换器\"},\n",
    "            {\"name\": \"Transformer\", \"type\": \"CONCEPT\", \"desc\": \"注意力机制架构\"},\n",
    "            {\"name\": \"OpenAI\", \"type\": \"ORGANIZATION\", \"desc\": \"人工智能研究公司\"},\n",
    "            {\"name\": \"Attention\", \"type\": \"CONCEPT\", \"desc\": \"注意力机制\"},\n",
    "        ]\n",
    "        \n",
    "        for entity in entities:\n",
    "            self.graph.add_node(\n",
    "                entity[\"name\"],\n",
    "                type=entity[\"type\"],\n",
    "                description=entity[\"desc\"]\n",
    "            )\n",
    "            self.entities[entity[\"name\"]] = entity\n",
    "        \n",
    "        # 示例关系\n",
    "        relations = [\n",
    "            (\"GPT\", \"LLM\", \"IS_A\"),\n",
    "            (\"GPT\", \"Transformer\", \"USES\"),\n",
    "            (\"Transformer\", \"Attention\", \"CONTAINS\"),\n",
    "            (\"OpenAI\", \"GPT\", \"DEVELOPS\"),\n",
    "        ]\n",
    "        \n",
    "        for source, target, rel_type in relations:\n",
    "            self.graph.add_edge(source, target, relation=rel_type)\n",
    "        \n",
    "        print(f\"   ✅ 添加了 {len(entities)} 个实体\")\n",
    "        print(f\"   ✅ 添加了 {len(relations)} 个关系\")\n",
    "    \n",
    "    def test_gpu_embedding(self, texts: List[str]):\n",
    "        \"\"\"测试GPU嵌入性能\"\"\"\n",
    "        print(f\"\\n⚡ 测试嵌入性能 ({len(texts)} 个文本)...\")\n",
    "        \n",
    "        import time\n",
    "        \n",
    "        start = time.time()\n",
    "        embeddings = self.embedder.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"   ✅ 完成! 耗时: {elapsed:.2f}秒\")\n",
    "        print(f\"   📊 嵌入维度: {embeddings.shape}\")\n",
    "        print(f\"   🚀 速度: {len(texts)/elapsed:.1f} 文本/秒\")\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def query(self, question: str):\n",
    "        \"\"\"简单查询\"\"\"\n",
    "        print(f\"\\n🔍 查询: {question}\")\n",
    "        \n",
    "        # 简单的关键词匹配\n",
    "        results = []\n",
    "        for entity_name in self.entities:\n",
    "            if entity_name.lower() in question.lower():\n",
    "                neighbors = list(self.graph.neighbors(entity_name))\n",
    "                results.append({\n",
    "                    \"entity\": entity_name,\n",
    "                    \"info\": self.entities[entity_name],\n",
    "                    \"neighbors\": neighbors\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n📋 找到 {len(results)} 个相关实体:\")\n",
    "        for r in results:\n",
    "            print(f\"   • {r['entity']} ({r['info']['type']})\")\n",
    "            print(f\"     描述: {r['info']['desc']}\")\n",
    "            print(f\"     关联: {', '.join(r['neighbors'])}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"✅ SimpleGraphRAG类定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ 运行GPU性能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化GraphRAG（GPU版本）\n",
    "print(\"=\"*60)\n",
    "print(\"🎯 GraphRAG GPU性能测试\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "graph_rag = SimpleGraphRAG(use_gpu=True)\n",
    "\n",
    "# 添加示例数据\n",
    "graph_rag.add_sample_data()\n",
    "\n",
    "# 准备测试文本\n",
    "test_texts = [\n",
    "    \"Large Language Models are transforming AI\",\n",
    "    \"GPT uses Transformer architecture\",\n",
    "    \"Attention mechanism is key to modern NLP\",\n",
    "    \"OpenAI develops cutting-edge AI models\",\n",
    "] * 25  # 100个文本\n",
    "\n",
    "print(f\"\\n准备了 {len(test_texts)} 个测试文本\")\n",
    "\n",
    "# GPU嵌入测试\n",
    "embeddings = graph_rag.test_gpu_embedding(test_texts)\n",
    "\n",
    "# 测试查询\n",
    "graph_rag.query(\"What is GPT?\")\n",
    "graph_rag.query(\"Tell me about Transformer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ GPU性能测试完成!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ CPU vs GPU 性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPU 嵌入性能对比\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📊 CPU vs GPU 嵌入性能对比\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 准备大量测试文本\n",
    "large_test_texts = test_texts * 10  # 1000个文本\n",
    "print(f\"\\n测试数据: {len(large_test_texts)} 个文本\\n\")\n",
    "\n",
    "# CPU测试\n",
    "print(\"🔵 CPU测试...\")\n",
    "graph_rag_cpu = SimpleGraphRAG(use_gpu=False)\n",
    "start = time.time()\n",
    "embeddings_cpu = graph_rag_cpu.embedder.encode(\n",
    "    large_test_texts,\n",
    "    show_progress_bar=False,\n",
    "    batch_size=32\n",
    ")\n",
    "cpu_time = time.time() - start\n",
    "print(f\"   CPU时间: {cpu_time:.2f}秒\")\n",
    "print(f\"   速度: {len(large_test_texts)/cpu_time:.1f} 文本/秒\")\n",
    "\n",
    "# GPU测试\n",
    "if cuda_available:\n",
    "    print(\"\\n🟢 GPU测试...\")\n",
    "    graph_rag_gpu = SimpleGraphRAG(use_gpu=True)\n",
    "    start = time.time()\n",
    "    embeddings_gpu = graph_rag_gpu.embedder.encode(\n",
    "        large_test_texts,\n",
    "        show_progress_bar=False,\n",
    "        batch_size=32\n",
    "    )\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"   GPU时间: {gpu_time:.2f}秒\")\n",
    "    print(f\"   速度: {len(large_test_texts)/gpu_time:.1f} 文本/秒\")\n",
    "    \n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\n🚀 加速比: {speedup:.2f}x\")\n",
    "    print(f\"   GPU比CPU快 {speedup:.1f} 倍!\")\n",
    "    \n",
    "    # 节省的时间\n",
    "    time_saved = cpu_time - gpu_time\n",
    "    print(f\"   ⏱️  节省时间: {time_saved:.2f}秒\")\n",
    "else:\n",
    "    print(\"\\n⚠️  GPU不可用，跳过GPU测试\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ 显存使用监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监控GPU显存使用\n",
    "if cuda_available:\n",
    "    print(\"=\"*60)\n",
    "    print(\"💾 GPU显存使用情况\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\n已分配: {allocated:.2f} GB\")\n",
    "    print(f\"已保留: {reserved:.2f} GB\")\n",
    "    print(f\"总显存: {total:.2f} GB\")\n",
    "    print(f\"使用率: {(allocated/total)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n详细信息:\")\n",
    "    print(torch.cuda.memory_summary(0, abbreviated=True))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"⚠️  GPU不可用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔟 性能总结报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成性能报告\n",
    "print(\"=\"*60)\n",
    "print(\"📈 GraphRAG GPU性能测试报告\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n🖥️  硬件信息:\")\n",
    "if cuda_available:\n",
    "    print(f\"   GPU型号: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   显存: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"   CUDA版本: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   ⚠️  GPU不可用\")\n",
    "\n",
    "print(f\"\\n   PyTorch版本: {torch.__version__}\")\n",
    "print(f\"   Python版本: {sys.version.split()[0]}\")\n",
    "\n",
    "print(\"\\n⚡ 性能测试结果:\")\n",
    "print(f\"   矩阵运算加速: ~{speedup if cuda_available else 'N/A'}x\")\n",
    "print(f\"   文本嵌入加速: ~{cpu_time/gpu_time if cuda_available else 'N/A'}x\")\n",
    "\n",
    "print(\"\\n💡 建议:\")\n",
    "if cuda_available:\n",
    "    print(\"   ✅ GPU运行良好！建议在Colab上运行完整的GraphRAG索引构建\")\n",
    "    print(\"   ✅ 预计索引构建时间将大幅缩短\")\n",
    "    print(\"   ✅ 可以处理更大规模的文档集\")\n",
    "else:\n",
    "    print(\"   ⚠️  建议启用GPU以获得最佳性能\")\n",
    "    print(\"   ⚠️  路径: 运行时 → 更改运行时类型 → GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ 测试完成!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 下一步\n",
    "\n",
    "如果GPU测试成功，您可以：\n",
    "\n",
    "1. **上传完整项目**: 将整个adaptive_RAG项目上传到Colab\n",
    "2. **运行GraphRAG索引**: 使用GPU加速构建知识图谱\n",
    "3. **保存结果**: 将构建好的图谱下载到本地\n",
    "\n",
    "### 运行完整GraphRAG的命令:\n",
    "\n",
    "```python\n",
    "# 上传项目后运行\n",
    "!python main_graphrag.py\n",
    "```\n",
    "\n",
    "### 预期加速效果:\n",
    "\n",
    "- 实体提取: 使用GPU的LLM推理会更快\n",
    "- 文本嵌入: **5-10倍加速**\n",
    "- 向量相似度计算: **10-20倍加速**\n",
    "- 总体索引构建时间: **3-5倍加速**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
