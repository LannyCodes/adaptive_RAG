{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ GraphRAG GPUæ£€æµ‹ä¸æµ‹è¯• - Google Colabç‰ˆæœ¬\n",
    "\n",
    "æœ¬Notebookç”¨äºåœ¨Google Colabä¸Šæ£€æµ‹GPUå¯ç”¨æ€§å¹¶æµ‹è¯•GraphRAGç³»ç»Ÿçš„æ€§èƒ½ã€‚\n",
    "\n",
    "## ğŸ“‹ ä½¿ç”¨æ­¥éª¤\n",
    "\n",
    "1. **å¯ç”¨GPU**: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ ç¡¬ä»¶åŠ é€Ÿå™¨ â†’ GPU (T4)\n",
    "2. **è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼**: ä¾æ¬¡æ‰§è¡Œä¸‹é¢çš„ä»£ç \n",
    "3. **æŸ¥çœ‹ç»“æœ**: æ£€æŸ¥GPUåŠ é€Ÿæ•ˆæœ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ GPUç¯å¢ƒæ£€æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æµ‹GPUå¯ç”¨æ€§\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” GPUç¯å¢ƒæ£€æµ‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PyTorch GPUæ£€æµ‹\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nâœ… CUDAå¯ç”¨: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"   GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"   å½“å‰GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"   GPUåç§°: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    \n",
    "    # æ˜¾å­˜ä¿¡æ¯\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"   æ€»æ˜¾å­˜: {total_memory:.2f} GB\")\n",
    "    \n",
    "    # nvidia-smiä¿¡æ¯\n",
    "    print(\"\\nğŸ“Š nvidia-smi è¾“å‡º:\")\n",
    "    print(\"-\"*60)\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"\\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPU\")\n",
    "    print(\"   è¯·æ£€æŸ¥: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ ç¡¬ä»¶åŠ é€Ÿå™¨ â†’ GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ GPUæ€§èƒ½åŸºå‡†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU vs CPU æ€§èƒ½å¯¹æ¯”\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âš¡ GPU vs CPU çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æµ‹è¯•å‚æ•°\n",
    "matrix_size = 5000\n",
    "\n",
    "# CPUæµ‹è¯•\n",
    "print(f\"\\nğŸ”µ CPUæµ‹è¯• (çŸ©é˜µå¤§å°: {matrix_size}x{matrix_size})\")\n",
    "a_cpu = torch.randn(matrix_size, matrix_size)\n",
    "b_cpu = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "start = time.time()\n",
    "c_cpu = torch.mm(a_cpu, b_cpu)\n",
    "cpu_time = time.time() - start\n",
    "print(f\"   CPUæ—¶é—´: {cpu_time:.2f} ç§’\")\n",
    "\n",
    "# GPUæµ‹è¯•\n",
    "if cuda_available:\n",
    "    print(f\"\\nğŸŸ¢ GPUæµ‹è¯• (çŸ©é˜µå¤§å°: {matrix_size}x{matrix_size})\")\n",
    "    a_gpu = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    b_gpu = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    \n",
    "    # é¢„çƒ­GPU\n",
    "    _ = torch.mm(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    c_gpu = torch.mm(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"   GPUæ—¶é—´: {gpu_time:.2f} ç§’\")\n",
    "    \n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "    print(f\"   GPUæ¯”CPUå¿« {speedup:.1f} å€!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  è·³è¿‡GPUæµ‹è¯•ï¼ˆGPUä¸å¯ç”¨ï¼‰\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ å®‰è£…GraphRAGä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…‹éš†é¡¹ç›®ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“¦ å®‰è£…GraphRAGä¾èµ–...\\n\")\n",
    "\n",
    "# å®‰è£…æ ¸å¿ƒä¾èµ–\n",
    "!pip install -q langchain langchain-community langchain-core langgraph\n",
    "!pip install -q chromadb sentence-transformers transformers\n",
    "!pip install -q tiktoken beautifulsoup4 requests\n",
    "!pip install -q tavily-python python-dotenv\n",
    "!pip install -q networkx python-louvain\n",
    "!pip install -q torch --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "print(\"\\nâœ… ä¾èµ–å®‰è£…å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ä¸Šä¼ é¡¹ç›®æ–‡ä»¶\n",
    "\n",
    "**é€‰é¡¹A**: ä»GitHubå…‹éš†\n",
    "```python\n",
    "!git clone https://github.com/your-repo/adaptive_RAG.git\n",
    "%cd adaptive_RAG\n",
    "```\n",
    "\n",
    "**é€‰é¡¹B**: æ‰‹åŠ¨ä¸Šä¼ æ–‡ä»¶åˆ°Colab\n",
    "- ä½¿ç”¨å·¦ä¾§æ–‡ä»¶æµè§ˆå™¨ä¸Šä¼ ä»¥ä¸‹æ ¸å¿ƒæ–‡ä»¶ï¼š\n",
    "  - `config.py`\n",
    "  - `entity_extractor.py`\n",
    "  - `knowledge_graph.py`\n",
    "  - `graph_indexer.py`\n",
    "  - `graph_retriever.py`\n",
    "  - `.env` (åŒ…å«APIå¯†é’¥)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "!mkdir -p data\n",
    "\n",
    "# å¦‚æœä½¿ç”¨é€‰é¡¹Aï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤\n",
    "# !git clone YOUR_REPO_URL\n",
    "# %cd adaptive_RAG\n",
    "\n",
    "print(\"âœ… ç›®å½•å‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ é…ç½®APIå¯†é’¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®APIå¯†é’¥ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„çœŸå®å¯†é’¥ï¼‰\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"ğŸ”‘ é…ç½®APIå¯†é’¥\\n\")\n",
    "\n",
    "# æ–¹å¼1: ç›´æ¥è®¾ç½®ï¼ˆä¸å®‰å…¨ï¼Œä»…ç”¨äºæµ‹è¯•ï¼‰\n",
    "# os.environ['TAVILY_API_KEY'] = 'your_tavily_api_key_here'\n",
    "\n",
    "# æ–¹å¼2: å®‰å…¨è¾“å…¥\n",
    "if 'TAVILY_API_KEY' not in os.environ:\n",
    "    os.environ['TAVILY_API_KEY'] = getpass('è¾“å…¥ TAVILY_API_KEY: ')\n",
    "    print(\"âœ… TAVILY_API_KEY å·²è®¾ç½®\")\n",
    "else:\n",
    "    print(\"âœ… TAVILY_API_KEY å·²å­˜åœ¨\")\n",
    "\n",
    "print(\"\\næ³¨æ„: GraphRAGåœ¨Colabä¸Šä½¿ç”¨HuggingFaceåµŒå…¥ï¼Œä¸éœ€è¦NOMIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ ç®€åŒ–ç‰ˆGraphRAGæµ‹è¯•ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€åŒ–ç‰ˆGraphRAGæ ¸å¿ƒç»„ä»¶\n",
    "# é€‚ç”¨äºColabå¿«é€Ÿæµ‹è¯•ï¼Œæ— éœ€å®Œæ•´é¡¹ç›®æ–‡ä»¶\n",
    "\n",
    "from typing import List, Dict\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "class SimpleGraphRAG:\n",
    "    \"\"\"ç®€åŒ–ç‰ˆGraphRAGç”¨äºGPUæ€§èƒ½æµ‹è¯•\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu=True):\n",
    "        print(\"ğŸš€ åˆå§‹åŒ–SimpleGraphRAG...\")\n",
    "        \n",
    "        # æ£€æµ‹è®¾å¤‡\n",
    "        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"   è®¾å¤‡: {self.device.upper()}\")\n",
    "        \n",
    "        # åŠ è½½åµŒå…¥æ¨¡å‹\n",
    "        print(f\"   åŠ è½½åµŒå…¥æ¨¡å‹...\")\n",
    "        self.embedder = SentenceTransformer(\n",
    "            'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # çŸ¥è¯†å›¾è°±\n",
    "        self.graph = nx.Graph()\n",
    "        self.entities = {}\n",
    "        \n",
    "        print(\"âœ… åˆå§‹åŒ–å®Œæˆ!\")\n",
    "    \n",
    "    def add_sample_data(self):\n",
    "        \"\"\"æ·»åŠ ç¤ºä¾‹æ•°æ®\"\"\"\n",
    "        print(\"\\nğŸ“Š æ·»åŠ ç¤ºä¾‹æ•°æ®...\")\n",
    "        \n",
    "        # ç¤ºä¾‹å®ä½“\n",
    "        entities = [\n",
    "            {\"name\": \"LLM\", \"type\": \"CONCEPT\", \"desc\": \"å¤§è¯­è¨€æ¨¡å‹\"},\n",
    "            {\"name\": \"GPT\", \"type\": \"TECHNOLOGY\", \"desc\": \"ç”Ÿæˆå¼é¢„è®­ç»ƒè½¬æ¢å™¨\"},\n",
    "            {\"name\": \"Transformer\", \"type\": \"CONCEPT\", \"desc\": \"æ³¨æ„åŠ›æœºåˆ¶æ¶æ„\"},\n",
    "            {\"name\": \"OpenAI\", \"type\": \"ORGANIZATION\", \"desc\": \"äººå·¥æ™ºèƒ½ç ”ç©¶å…¬å¸\"},\n",
    "            {\"name\": \"Attention\", \"type\": \"CONCEPT\", \"desc\": \"æ³¨æ„åŠ›æœºåˆ¶\"},\n",
    "        ]\n",
    "        \n",
    "        for entity in entities:\n",
    "            self.graph.add_node(\n",
    "                entity[\"name\"],\n",
    "                type=entity[\"type\"],\n",
    "                description=entity[\"desc\"]\n",
    "            )\n",
    "            self.entities[entity[\"name\"]] = entity\n",
    "        \n",
    "        # ç¤ºä¾‹å…³ç³»\n",
    "        relations = [\n",
    "            (\"GPT\", \"LLM\", \"IS_A\"),\n",
    "            (\"GPT\", \"Transformer\", \"USES\"),\n",
    "            (\"Transformer\", \"Attention\", \"CONTAINS\"),\n",
    "            (\"OpenAI\", \"GPT\", \"DEVELOPS\"),\n",
    "        ]\n",
    "        \n",
    "        for source, target, rel_type in relations:\n",
    "            self.graph.add_edge(source, target, relation=rel_type)\n",
    "        \n",
    "        print(f\"   âœ… æ·»åŠ äº† {len(entities)} ä¸ªå®ä½“\")\n",
    "        print(f\"   âœ… æ·»åŠ äº† {len(relations)} ä¸ªå…³ç³»\")\n",
    "    \n",
    "    def test_gpu_embedding(self, texts: List[str]):\n",
    "        \"\"\"æµ‹è¯•GPUåµŒå…¥æ€§èƒ½\"\"\"\n",
    "        print(f\"\\nâš¡ æµ‹è¯•åµŒå…¥æ€§èƒ½ ({len(texts)} ä¸ªæ–‡æœ¬)...\")\n",
    "        \n",
    "        import time\n",
    "        \n",
    "        start = time.time()\n",
    "        embeddings = self.embedder.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"   âœ… å®Œæˆ! è€—æ—¶: {elapsed:.2f}ç§’\")\n",
    "        print(f\"   ğŸ“Š åµŒå…¥ç»´åº¦: {embeddings.shape}\")\n",
    "        print(f\"   ğŸš€ é€Ÿåº¦: {len(texts)/elapsed:.1f} æ–‡æœ¬/ç§’\")\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def query(self, question: str):\n",
    "        \"\"\"ç®€å•æŸ¥è¯¢\"\"\"\n",
    "        print(f\"\\nğŸ” æŸ¥è¯¢: {question}\")\n",
    "        \n",
    "        # ç®€å•çš„å…³é”®è¯åŒ¹é…\n",
    "        results = []\n",
    "        for entity_name in self.entities:\n",
    "            if entity_name.lower() in question.lower():\n",
    "                neighbors = list(self.graph.neighbors(entity_name))\n",
    "                results.append({\n",
    "                    \"entity\": entity_name,\n",
    "                    \"info\": self.entities[entity_name],\n",
    "                    \"neighbors\": neighbors\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³å®ä½“:\")\n",
    "        for r in results:\n",
    "            print(f\"   â€¢ {r['entity']} ({r['info']['type']})\")\n",
    "            print(f\"     æè¿°: {r['info']['desc']}\")\n",
    "            print(f\"     å…³è”: {', '.join(r['neighbors'])}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… SimpleGraphRAGç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–GraphRAGï¼ˆGPUç‰ˆæœ¬ï¼‰\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ GraphRAG GPUæ€§èƒ½æµ‹è¯•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "graph_rag = SimpleGraphRAG(use_gpu=True)\n",
    "\n",
    "# æ·»åŠ ç¤ºä¾‹æ•°æ®\n",
    "graph_rag.add_sample_data()\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ–‡æœ¬\n",
    "test_texts = [\n",
    "    \"Large Language Models are transforming AI\",\n",
    "    \"GPT uses Transformer architecture\",\n",
    "    \"Attention mechanism is key to modern NLP\",\n",
    "    \"OpenAI develops cutting-edge AI models\",\n",
    "] * 25  # 100ä¸ªæ–‡æœ¬\n",
    "\n",
    "print(f\"\\nå‡†å¤‡äº† {len(test_texts)} ä¸ªæµ‹è¯•æ–‡æœ¬\")\n",
    "\n",
    "# GPUåµŒå…¥æµ‹è¯•\n",
    "embeddings = graph_rag.test_gpu_embedding(test_texts)\n",
    "\n",
    "# æµ‹è¯•æŸ¥è¯¢\n",
    "graph_rag.query(\"What is GPT?\")\n",
    "graph_rag.query(\"Tell me about Transformer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… GPUæ€§èƒ½æµ‹è¯•å®Œæˆ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ CPU vs GPU æ€§èƒ½å¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPU åµŒå…¥æ€§èƒ½å¯¹æ¯”\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š CPU vs GPU åµŒå…¥æ€§èƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# å‡†å¤‡å¤§é‡æµ‹è¯•æ–‡æœ¬\n",
    "large_test_texts = test_texts * 10  # 1000ä¸ªæ–‡æœ¬\n",
    "print(f\"\\næµ‹è¯•æ•°æ®: {len(large_test_texts)} ä¸ªæ–‡æœ¬\\n\")\n",
    "\n",
    "# CPUæµ‹è¯•\n",
    "print(\"ğŸ”µ CPUæµ‹è¯•...\")\n",
    "graph_rag_cpu = SimpleGraphRAG(use_gpu=False)\n",
    "start = time.time()\n",
    "embeddings_cpu = graph_rag_cpu.embedder.encode(\n",
    "    large_test_texts,\n",
    "    show_progress_bar=False,\n",
    "    batch_size=32\n",
    ")\n",
    "cpu_time = time.time() - start\n",
    "print(f\"   CPUæ—¶é—´: {cpu_time:.2f}ç§’\")\n",
    "print(f\"   é€Ÿåº¦: {len(large_test_texts)/cpu_time:.1f} æ–‡æœ¬/ç§’\")\n",
    "\n",
    "# GPUæµ‹è¯•\n",
    "if cuda_available:\n",
    "    print(\"\\nğŸŸ¢ GPUæµ‹è¯•...\")\n",
    "    graph_rag_gpu = SimpleGraphRAG(use_gpu=True)\n",
    "    start = time.time()\n",
    "    embeddings_gpu = graph_rag_gpu.embedder.encode(\n",
    "        large_test_texts,\n",
    "        show_progress_bar=False,\n",
    "        batch_size=32\n",
    "    )\n",
    "    gpu_time = time.time() - start\n",
    "    print(f\"   GPUæ—¶é—´: {gpu_time:.2f}ç§’\")\n",
    "    print(f\"   é€Ÿåº¦: {len(large_test_texts)/gpu_time:.1f} æ–‡æœ¬/ç§’\")\n",
    "    \n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x\")\n",
    "    print(f\"   GPUæ¯”CPUå¿« {speedup:.1f} å€!\")\n",
    "    \n",
    "    # èŠ‚çœçš„æ—¶é—´\n",
    "    time_saved = cpu_time - gpu_time\n",
    "    print(f\"   â±ï¸  èŠ‚çœæ—¶é—´: {time_saved:.2f}ç§’\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  GPUä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ æ˜¾å­˜ä½¿ç”¨ç›‘æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨\n",
    "if cuda_available:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ’¾ GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\\nå·²åˆ†é…: {allocated:.2f} GB\")\n",
    "    print(f\"å·²ä¿ç•™: {reserved:.2f} GB\")\n",
    "    print(f\"æ€»æ˜¾å­˜: {total:.2f} GB\")\n",
    "    print(f\"ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nè¯¦ç»†ä¿¡æ¯:\")\n",
    "    print(torch.cuda.memory_summary(0, abbreviated=True))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"âš ï¸  GPUä¸å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ æ€§èƒ½æ€»ç»“æŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“ˆ GraphRAG GPUæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ–¥ï¸  ç¡¬ä»¶ä¿¡æ¯:\")\n",
    "if cuda_available:\n",
    "    print(f\"   GPUå‹å·: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"   CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   âš ï¸  GPUä¸å¯ç”¨\")\n",
    "\n",
    "print(f\"\\n   PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"   Pythonç‰ˆæœ¬: {sys.version.split()[0]}\")\n",
    "\n",
    "print(\"\\nâš¡ æ€§èƒ½æµ‹è¯•ç»“æœ:\")\n",
    "print(f\"   çŸ©é˜µè¿ç®—åŠ é€Ÿ: ~{speedup if cuda_available else 'N/A'}x\")\n",
    "print(f\"   æ–‡æœ¬åµŒå…¥åŠ é€Ÿ: ~{cpu_time/gpu_time if cuda_available else 'N/A'}x\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å»ºè®®:\")\n",
    "if cuda_available:\n",
    "    print(\"   âœ… GPUè¿è¡Œè‰¯å¥½ï¼å»ºè®®åœ¨Colabä¸Šè¿è¡Œå®Œæ•´çš„GraphRAGç´¢å¼•æ„å»º\")\n",
    "    print(\"   âœ… é¢„è®¡ç´¢å¼•æ„å»ºæ—¶é—´å°†å¤§å¹…ç¼©çŸ­\")\n",
    "    print(\"   âœ… å¯ä»¥å¤„ç†æ›´å¤§è§„æ¨¡çš„æ–‡æ¡£é›†\")\n",
    "else:\n",
    "    print(\"   âš ï¸  å»ºè®®å¯ç”¨GPUä»¥è·å¾—æœ€ä½³æ€§èƒ½\")\n",
    "    print(\"   âš ï¸  è·¯å¾„: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… æµ‹è¯•å®Œæˆ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ä¸‹ä¸€æ­¥\n",
    "\n",
    "å¦‚æœGPUæµ‹è¯•æˆåŠŸï¼Œæ‚¨å¯ä»¥ï¼š\n",
    "\n",
    "1. **ä¸Šä¼ å®Œæ•´é¡¹ç›®**: å°†æ•´ä¸ªadaptive_RAGé¡¹ç›®ä¸Šä¼ åˆ°Colab\n",
    "2. **è¿è¡ŒGraphRAGç´¢å¼•**: ä½¿ç”¨GPUåŠ é€Ÿæ„å»ºçŸ¥è¯†å›¾è°±\n",
    "3. **ä¿å­˜ç»“æœ**: å°†æ„å»ºå¥½çš„å›¾è°±ä¸‹è½½åˆ°æœ¬åœ°\n",
    "\n",
    "### è¿è¡Œå®Œæ•´GraphRAGçš„å‘½ä»¤:\n",
    "\n",
    "```python\n",
    "# ä¸Šä¼ é¡¹ç›®åè¿è¡Œ\n",
    "!python main_graphrag.py\n",
    "```\n",
    "\n",
    "### é¢„æœŸåŠ é€Ÿæ•ˆæœ:\n",
    "\n",
    "- å®ä½“æå–: ä½¿ç”¨GPUçš„LLMæ¨ç†ä¼šæ›´å¿«\n",
    "- æ–‡æœ¬åµŒå…¥: **5-10å€åŠ é€Ÿ**\n",
    "- å‘é‡ç›¸ä¼¼åº¦è®¡ç®—: **10-20å€åŠ é€Ÿ**\n",
    "- æ€»ä½“ç´¢å¼•æ„å»ºæ—¶é—´: **3-5å€åŠ é€Ÿ**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
