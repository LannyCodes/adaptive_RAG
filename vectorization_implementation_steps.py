"""
æ–‡å­—è½¬å‘é‡çš„å…·ä½“å®ç°æ­¥éª¤ï¼ˆä»£ç å±‚é¢ï¼‰
å±•ç¤º HuggingFace Embeddings å†…éƒ¨çš„å®é™…æ“ä½œ
"""

print("=" * 80)
print("æ–‡å­— â†’ å‘é‡çš„å…·ä½“å®ç°æ­¥éª¤")
print("=" * 80)

# ============================================================================
# å‡†å¤‡å·¥ä½œï¼šæ¨¡æ‹Ÿå®Œæ•´çš„å‘é‡åŒ–è¿‡ç¨‹
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ”§ å‡†å¤‡ï¼šå®‰è£…å’Œå¯¼å…¥éœ€è¦çš„åº“")
print("=" * 80)

print("""
éœ€è¦çš„åº“ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
pip install transformers torch sentence-transformers

å¯¼å…¥ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
""")


# ============================================================================
# Step 1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
# ============================================================================
print("\n" + "=" * 80)
print("Step 1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨")
print("=" * 80)

print("""
ä»£ç ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
from transformers import AutoTokenizer, AutoModel

model_name = "sentence-transformers/all-MiniLM-L6-v2"

# 1. åŠ è½½åˆ†è¯å™¨ï¼ˆè´Ÿè´£æ–‡å­— â†’ IDï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. åŠ è½½æ¨¡å‹ï¼ˆè´Ÿè´£ ID â†’ å‘é‡ï¼‰
model = AutoModel.from_pretrained(model_name)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¸è®­ç»ƒï¼‰

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¿™ä¸¤ä¸ªä¸œè¥¿åšä»€ä¹ˆï¼Ÿ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰ï¼š
â”œâ”€ è¯æ±‡è¡¨ï¼ˆvocabularyï¼‰ï¼š30,000+ ä¸ªè¯
â”‚  ä¾‹å¦‚ï¼š{"hello": 1, "world": 2, "machine": 3456, ...}
â””â”€ åˆ†è¯è§„åˆ™ï¼šå¦‚ä½•åˆ‡åˆ†æ–‡å­—

Modelï¼ˆæ¨¡å‹ï¼‰ï¼š
â”œâ”€ Embedding å±‚ï¼šè¯æ±‡è¡¨ â†’ åˆå§‹å‘é‡
â”‚  30,000 Ã— 384 çš„çŸ©é˜µï¼ˆæ¯ä¸ªè¯å¯¹åº”ä¸€ä¸ª 384 ç»´å‘é‡ï¼‰
â”œâ”€ Transformer å±‚ï¼š6 å±‚ BERT encoder
â”‚  æ¯å±‚éƒ½æœ‰ Self-Attention + Feed Forward
â””â”€ å‚æ•°é‡ï¼š22Mï¼ˆ2200ä¸‡ä¸ªæ•°å­—ï¼‰
""")


# ============================================================================
# Step 2: åˆ†è¯ï¼ˆTokenizationï¼‰
# ============================================================================
print("\n" + "=" * 80)
print("Step 2: åˆ†è¯ - æ–‡å­—è½¬ä¸º Token IDs")
print("=" * 80)

print("""
è¾“å…¥æ–‡æœ¬ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
text = "Machine learning is a subset of artificial intelligence"

ä»£ç ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# åˆ†è¯å¹¶è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
encoded_input = tokenizer(
    text,
    padding=True,      # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    truncation=True,   # è¶…é•¿æˆªæ–­
    max_length=512,    # æœ€å¤§é•¿åº¦
    return_tensors='pt' # è¿”å› PyTorch tensor
)

print(encoded_input)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å‡ºï¼ˆencoded_input åŒ…å«ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{
  'input_ids': tensor([[
      101,     # [CLS] ç‰¹æ®Šæ ‡è®°
      3698,    # "machine"
      4083,    # "learning"
      2003,    # "is"
      1037,    # "a"
      2042,    # "subset"
      1997,    # "of"
      7976,    # "artificial"
      4454,    # "intelligence"
      102      # [SEP] ç‰¹æ®Šæ ‡è®°
  ]]),
  
  'attention_mask': tensor([[
      1, 1, 1, 1, 1, 1, 1, 1, 1, 1  # æ‰€æœ‰ä½ç½®éƒ½æœ‰æ•ˆï¼ˆ1è¡¨ç¤ºå…³æ³¨ï¼Œ0è¡¨ç¤ºå¿½ç•¥ï¼‰
  ]])
}

è¯¦ç»†è§£é‡Šï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

input_ids:
  æ¯ä¸ªæ•°å­—å¯¹åº”ä¸€ä¸ªè¯
  101 = [CLS]ï¼ˆå¥å­å¼€å§‹æ ‡è®°ï¼‰
  3698 = "machine"
  102 = [SEP]ï¼ˆå¥å­ç»“æŸæ ‡è®°ï¼‰

attention_mask:
  å‘Šè¯‰æ¨¡å‹å“ªäº›ä½ç½®æ˜¯çœŸå®å†…å®¹ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ˜¯å¡«å……ï¼ˆ0ï¼‰
  ä¾‹å¦‚ï¼š[1, 1, 1, 0, 0] è¡¨ç¤ºå‰3ä¸ªæ˜¯çœŸå®è¯ï¼Œå2ä¸ªæ˜¯å¡«å……
""")


# ============================================================================
# Step 3: é€šè¿‡ Embedding å±‚è·å–åˆå§‹å‘é‡
# ============================================================================
print("\n" + "=" * 80)
print("Step 3: Token IDs â†’ åˆå§‹å‘é‡ï¼ˆEmbedding å±‚ï¼‰")
print("=" * 80)

print("""
è¿™ä¸€æ­¥å‘ç”Ÿåœ¨æ¨¡å‹å†…éƒ¨ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

input_ids = [101, 3698, 4083, 2003, ...]
                â†“
        Embedding è¡¨æŸ¥è¯¢
                â†“

Embedding è¡¨ï¼ˆç®€åŒ–ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„çŸ©é˜µï¼š30,522 Ã— 384
ï¼ˆ30,522 æ˜¯è¯æ±‡è¡¨å¤§å°ï¼Œ384 æ˜¯å‘é‡ç»´åº¦ï¼‰

  ID    |  ç¬¬1ç»´  ç¬¬2ç»´  ç¬¬3ç»´  ...  ç¬¬384ç»´
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  101   |  0.12  -0.34   0.56  ...   0.78   â† [CLS]
  3698  |  0.23   0.45  -0.67  ...   0.89   â† "machine"
  4083  |  0.34  -0.56   0.78  ...  -0.90   â† "learning"
  2003  |  0.45   0.67  -0.89  ...   0.12   â† "is"
  ...

æŸ¥è¯¢è¿‡ç¨‹ï¼ˆç±»ä¼¼å­—å…¸æŸ¥è¯¢ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ID 101  â†’ æŸ¥è¡¨ â†’ [0.12, -0.34, 0.56, ..., 0.78]
ID 3698 â†’ æŸ¥è¡¨ â†’ [0.23, 0.45, -0.67, ..., 0.89]
ID 4083 â†’ æŸ¥è¡¨ â†’ [0.34, -0.56, 0.78, ..., -0.90]
...

ç»“æœï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
token_embeddings = [
    [0.12, -0.34, 0.56, ..., 0.78],  # [CLS]
    [0.23,  0.45, -0.67, ..., 0.89],  # "machine"
    [0.34, -0.56, 0.78, ..., -0.90],  # "learning"
    [0.45,  0.67, -0.89, ..., 0.12],  # "is"
    ...
]
å½¢çŠ¶ï¼š(10, 384)  # 10 ä¸ª tokensï¼Œæ¯ä¸ª 384 ç»´

âš ï¸ æ³¨æ„ï¼šè¿™äº›è¿˜ä¸æ˜¯æœ€ç»ˆå‘é‡ï¼éœ€è¦é€šè¿‡ Transformer å¤„ç†ï¼
""")


# ============================================================================
# Step 4: Transformer å¤„ç†ï¼ˆæ ¸å¿ƒï¼ï¼‰
# ============================================================================
print("\n" + "=" * 80)
print("Step 4: Transformer å¤„ç† - Self-Attentionï¼ˆæ ¸å¿ƒæ­¥éª¤ï¼‰")
print("=" * 80)

print("""
ä»£ç ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆä¸è®­ç»ƒï¼‰
    outputs = model(**encoded_input)

# outputs.last_hidden_state å°±æ˜¯ Transformer çš„è¾“å‡º
token_embeddings = outputs.last_hidden_state
print(token_embeddings.shape)  # torch.Size([1, 10, 384])
                               #   æ‰¹æ¬¡  tokens  ç»´åº¦

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Transformer å†…éƒ¨åšäº†ä»€ä¹ˆï¼Ÿï¼ˆ6 å±‚å¤„ç†ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å…¥ï¼šåˆå§‹ embeddings
  [CLS]:     [0.12, -0.34, 0.56, ...]
  machine:   [0.23,  0.45, -0.67, ...]
  learning:  [0.34, -0.56, 0.78, ...]
  is:        [0.45,  0.67, -0.89, ...]
  ...

        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Self-Attention                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ æ¯ä¸ªè¯"çœ‹"å…¶ä»–æ‰€æœ‰è¯ï¼Œæ›´æ–°è‡ªå·±çš„å‘é‡ï¼š                    â”‚
â”‚                                                          â”‚
â”‚ "machine" çœ‹åˆ° "learning" â†’ ç†è§£è¿™æ˜¯ä¸€ä¸ªè¯ç»„              â”‚
â”‚ "learning" çœ‹åˆ° "artificial" â†’ ç†è§£ä¸AIç›¸å…³              â”‚
â”‚ "is" çœ‹åˆ°å‰åè¯ â†’ ç†è§£æ˜¯è¿æ¥è¯                           â”‚
â”‚                                                          â”‚
â”‚ æ›´æ–°åçš„å‘é‡åŒ…å«äº†ä¸Šä¸‹æ–‡ä¿¡æ¯                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: Self-Attention                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ ç»§ç»­æ·±åŒ–ç†è§£...                                          â”‚
â”‚ "machine learning" ä½œä¸ºæ•´ä½“ç†è§£                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
        ... (Layer 3, 4, 5) ...
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 6: Self-Attention (æœ€åä¸€å±‚)                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ æ¯ä¸ªè¯çš„å‘é‡ç°åœ¨åŒ…å«äº†ï¼š                                  â”‚
â”‚ - è‡ªå·±çš„è¯­ä¹‰                                             â”‚
â”‚ - ä¸Šä¸‹æ–‡ä¿¡æ¯                                             â”‚
â”‚ - æ•´ä¸ªå¥å­çš„å«ä¹‰                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
æœ€ç»ˆè¾“å‡ºï¼š
  [CLS]:     [0.234,  0.567, -0.890, ...]  # æ›´æ–°åï¼ŒåŒ…å«å…¨å¥ä¿¡æ¯
  machine:   [0.345, -0.678,  0.123, ...]  # åŒ…å« "learning" çš„ä¿¡æ¯
  learning:  [0.456,  0.789, -0.234, ...]  # åŒ…å« "machine" çš„ä¿¡æ¯
  ...

å½¢çŠ¶ï¼š(1, 10, 384)
      æ‰¹æ¬¡ tokens ç»´åº¦
""")


# ============================================================================
# Step 5: Mean Pooling - åˆå¹¶æˆä¸€ä¸ªå¥å­å‘é‡
# ============================================================================
print("\n" + "=" * 80)
print("Step 5: Mean Pooling - æŠŠå¤šä¸ªè¯å‘é‡åˆå¹¶æˆä¸€ä¸ªå¥å­å‘é‡")
print("=" * 80)

print("""
é—®é¢˜ï¼šç°åœ¨æœ‰ 10 ä¸ªè¯ï¼Œæ¯ä¸ªè¯ä¸€ä¸ªå‘é‡
     å¦‚ä½•å˜æˆ 1 ä¸ªå¥å­å‘é‡ï¼Ÿ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä»£ç ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def mean_pooling(token_embeddings, attention_mask):
    \"\"\"
    å¯¹æ‰€æœ‰è¯å‘é‡æ±‚å¹³å‡ï¼ˆè€ƒè™‘ attention_maskï¼‰
    \"\"\"
    # token_embeddings: (1, 10, 384)
    # attention_mask:   (1, 10)
    
    # æ‰©å±• mask çš„ç»´åº¦ä»¥åŒ¹é… embeddings
    # (1, 10) â†’ (1, 10, 1) â†’ (1, 10, 384)
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(
        token_embeddings.size()
    ).float()
    
    # å°† embeddings ä¸ mask ç›¸ä¹˜ï¼ˆå¿½ç•¥å¡«å……éƒ¨åˆ†ï¼‰
    # ç„¶åå¯¹æ‰€æœ‰è¯æ±‚å’Œ
    sum_embeddings = torch.sum(
        token_embeddings * input_mask_expanded, 
        dim=1  # åœ¨ token ç»´åº¦æ±‚å’Œ
    )
    
    # è®¡ç®—æœ‰æ•ˆ token çš„æ•°é‡
    sum_mask = torch.clamp(
        input_mask_expanded.sum(dim=1), 
        min=1e-9  # é¿å…é™¤é›¶
    )
    
    # æ±‚å¹³å‡
    mean_embeddings = sum_embeddings / sum_mask
    
    return mean_embeddings

# ä½¿ç”¨
sentence_embedding = mean_pooling(
    token_embeddings, 
    encoded_input['attention_mask']
)

print(sentence_embedding.shape)  # torch.Size([1, 384])
                                 #   æ‰¹æ¬¡  ç»´åº¦

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å…·ä½“è®¡ç®—ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

10 ä¸ªè¯å‘é‡ï¼Œæ¯ä¸ª 384 ç»´ï¼š
  Token 1: [0.234,  0.567, -0.890, ..., 0.123]
  Token 2: [0.345, -0.678,  0.123, ..., 0.234]
  Token 3: [0.456,  0.789, -0.234, ..., 0.345]
  ...
  Token 10: [0.567, 0.890,  0.345, ..., 0.456]

æ±‚å¹³å‡ï¼ˆå¯¹æ¯ä¸€ç»´åˆ†åˆ«å¹³å‡ï¼‰ï¼š
  ç¬¬1ç»´: (0.234 + 0.345 + 0.456 + ... + 0.567) / 10 = 0.412
  ç¬¬2ç»´: (0.567 - 0.678 + 0.789 + ... + 0.890) / 10 = 0.523
  ç¬¬3ç»´: (-0.890 + 0.123 - 0.234 + ... + 0.345) / 10 = -0.089
  ...
  ç¬¬384ç»´: (0.123 + 0.234 + 0.345 + ... + 0.456) / 10 = 0.289

å¥å­å‘é‡ = [0.412, 0.523, -0.089, ..., 0.289]  (384ç»´)
""")


# ============================================================================
# Step 6: å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰
# ============================================================================
print("\n" + "=" * 80)
print("Step 6: L2 å½’ä¸€åŒ– - å°†å‘é‡é•¿åº¦ç¼©æ”¾åˆ° 1")
print("=" * 80)

print("""
ä»£ç ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
import torch.nn.functional as F

# L2 å½’ä¸€åŒ–
sentence_embedding = F.normalize(
    sentence_embedding, 
    p=2,    # L2 èŒƒæ•°
    dim=1   # åœ¨ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–
)

print(sentence_embedding.shape)  # torch.Size([1, 384])

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å½’ä¸€åŒ–çš„ä½œç”¨ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å½’ä¸€åŒ–å‰çš„å‘é‡ï¼š
  v = [0.412, 0.523, -0.089, ..., 0.289]
  é•¿åº¦ ||v|| = âˆš(0.412Â² + 0.523Â² + ... + 0.289Â²) = 2.37

å½’ä¸€åŒ–åçš„å‘é‡ï¼š
  v_norm = v / ||v||
  v_norm = [0.412/2.37, 0.523/2.37, ..., 0.289/2.37]
         = [0.174, 0.221, -0.038, ..., 0.122]
  é•¿åº¦ ||v_norm|| = 1  âœ“

å¥½å¤„ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… æ‰€æœ‰å‘é‡é•¿åº¦ç›¸åŒï¼ˆéƒ½æ˜¯1ï¼‰ï¼Œæ–¹ä¾¿æ¯”è¾ƒ
âœ… ä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯ï¼ˆè®¡ç®—æ›´å¿«ï¼‰
   cos_sim(a, b) = aÂ·b / (||a|| Ã— ||b||)
   å¦‚æœå½’ä¸€åŒ–: cos_sim(a, b) = aÂ·b  â† ç®€åŒ–äº†ï¼

âœ… æ¶ˆé™¤å‘é‡é•¿åº¦çš„å½±å“ï¼Œåªå…³æ³¨æ–¹å‘
""")


# ============================================================================
# Step 7: æœ€ç»ˆè¾“å‡º
# ============================================================================
print("\n" + "=" * 80)
print("Step 7: å¾—åˆ°æœ€ç»ˆçš„å¥å­å‘é‡")
print("=" * 80)

print("""
æœ€ç»ˆç»“æœï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# è½¬æ¢ä¸º numpy æ•°ç»„ï¼ˆæ–¹ä¾¿ä½¿ç”¨ï¼‰
final_vector = sentence_embedding.cpu().numpy()[0]

print(final_vector.shape)  # (384,)
print(final_vector[:5])    # å‰5ä¸ªæ•°å­—
# [0.174, 0.221, -0.038, 0.095, 0.312]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¿™å°±æ˜¯æœ€ç»ˆçš„å¥å­å‘é‡ï¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å…¥: "Machine learning is a subset of artificial intelligence"
è¾“å‡º: [0.174, 0.221, -0.038, ..., 0.122]  (384 ä¸ªæ•°å­—)

è¿™ä¸ªå‘é‡åŒ…å«äº†ï¼š
âœ… æ¯ä¸ªè¯çš„è¯­ä¹‰
âœ… è¯ä¸è¯ä¹‹é—´çš„å…³ç³»
âœ… æ•´ä¸ªå¥å­çš„å«ä¹‰

å¯ä»¥ç”¨æ¥ï¼š
âœ… è®¡ç®—ä¸å…¶ä»–å¥å­çš„ç›¸ä¼¼åº¦
âœ… å­˜å…¥å‘é‡æ•°æ®åº“
âœ… è¿›è¡Œè¯­ä¹‰æœç´¢
""")


# ============================================================================
# å®Œæ•´ä»£ç æ±‡æ€»
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“ å®Œæ•´ä»£ç æ±‡æ€»ï¼ˆå®é™…å¯è¿è¡Œï¼‰")
print("=" * 80)

print("""
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

def text_to_vector(text):
    \"\"\"
    å®Œæ•´çš„æ–‡å­—è½¬å‘é‡æµç¨‹
    \"\"\"
    # Step 1: åŠ è½½æ¨¡å‹
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    model.eval()
    
    # Step 2: åˆ†è¯
    encoded_input = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )
    
    # Step 3 & 4: é€šè¿‡æ¨¡å‹ï¼ˆEmbedding + Transformerï¼‰
    with torch.no_grad():
        outputs = model(**encoded_input)
        token_embeddings = outputs.last_hidden_state
    
    # Step 5: Mean Pooling
    attention_mask = encoded_input['attention_mask']
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(
        token_embeddings.size()
    ).float()
    
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
    sentence_embedding = sum_embeddings / sum_mask
    
    # Step 6: å½’ä¸€åŒ–
    sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)
    
    # Step 7: è½¬ä¸º numpy
    return sentence_embedding.cpu().numpy()[0]


# ä½¿ç”¨ç¤ºä¾‹ï¼š
text = "Machine learning is a subset of artificial intelligence"
vector = text_to_vector(text)

print(f"è¾“å…¥: {text}")
print(f"å‘é‡ç»´åº¦: {vector.shape}")  # (384,)
print(f"å‰10ä¸ªæ•°å­—: {vector[:10]}")
print(f"å‘é‡é•¿åº¦: {np.linalg.norm(vector)}")  # åº”è¯¥æ˜¯ 1.0

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä½ çš„é¡¹ç›®ä¸­çš„ç®€åŒ–è°ƒç”¨ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vector = embeddings.embed_query(text)
# â†‘ è¿™ä¸€è¡Œå†…éƒ¨æ‰§è¡Œäº†ä¸Šé¢æ‰€æœ‰ 7 ä¸ªæ­¥éª¤ï¼

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")


# ============================================================================
# å…³é”®æ­¥éª¤æ—¶é—´åˆ†æ
# ============================================================================
print("\n" + "=" * 80)
print("â±ï¸  å„æ­¥éª¤è€—æ—¶åˆ†æ")
print("=" * 80)

print("""
å‡è®¾å¤„ç†ä¸€ä¸ªå¥å­ï¼ˆ10ä¸ªè¯ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: åŠ è½½æ¨¡å‹           0.5-2ç§’   (åªéœ€ä¸€æ¬¡ï¼Œå¯å¤ç”¨)
Step 2: åˆ†è¯               <1æ¯«ç§’    (éå¸¸å¿«)
Step 3: Embedding æŸ¥è¡¨     <1æ¯«ç§’    (çŸ©é˜µç´¢å¼•)
Step 4: Transformer å¤„ç†   10-50æ¯«ç§’ (6å±‚è®¡ç®—ï¼Œæœ€æ…¢)
Step 5: Mean Pooling       <1æ¯«ç§’    (ç®€å•å¹³å‡)
Step 6: å½’ä¸€åŒ–             <1æ¯«ç§’    (ç®€å•é™¤æ³•)
Step 7: è½¬æ¢æ ¼å¼           <1æ¯«ç§’

æ€»è€—æ—¶: 10-50æ¯«ç§’ (GPU) æˆ– 50-200æ¯«ç§’ (CPU)

æ‰¹é‡å¤„ç†ï¼ˆ20ä¸ªå¥å­ï¼‰:
  å•ä¸ªå¤„ç†: 20 Ã— 50ms = 1000ms
  æ‰¹é‡å¤„ç†: 100ms â† å¿«10å€ï¼(GPUå¹¶è¡Œ)
  
è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦æ‰¹é‡å‘é‡åŒ–ï¼
""")


print("\n" + "=" * 80)
print("âœ… æ–‡å­—è½¬å‘é‡çš„å®ç°æ­¥éª¤è®²è§£å®Œæ¯•ï¼")
print("=" * 80)
print("""
æ ¸å¿ƒæ­¥éª¤å›é¡¾ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ–‡å­—
  â†“ Step 1: åŠ è½½æ¨¡å‹
Tokenizer + Model
  â†“ Step 2: åˆ†è¯
Token IDs: [101, 3698, 4083, ...]
  â†“ Step 3: Embedding æŸ¥è¡¨
åˆå§‹å‘é‡: [(10, 384)]
  â†“ Step 4: Transformer å¤„ç†
æ›´æ–°å‘é‡: [(10, 384)]  åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
  â†“ Step 5: Mean Pooling
å¥å­å‘é‡: [(1, 384)]
  â†“ Step 6: å½’ä¸€åŒ–
å½’ä¸€åŒ–å‘é‡: [(1, 384)]  é•¿åº¦=1
  â†“ Step 7: è¾“å‡º
æœ€ç»ˆå‘é‡: [0.174, 0.221, ..., 0.122]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ç°åœ¨ä½ çŸ¥é“äº†æ¯ä¸€æ­¥çš„å…·ä½“æ“ä½œï¼
""")
print()
