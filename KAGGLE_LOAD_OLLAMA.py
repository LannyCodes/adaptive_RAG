"""
Kaggle Ollama åŠ è½½è„šæœ¬
ä» Kaggle Dataset å¿«é€ŸåŠ è½½ Ollama å’Œæ¨¡å‹ï¼Œæ— éœ€é‡æ–°ä¸‹è½½

å‰ç½®æ¡ä»¶:
1. å·²ä½¿ç”¨ KAGGLE_SAVE_OLLAMA.py åˆ›å»ºå¤‡ä»½
2. å·²åœ¨ Kaggle ä¸Šä¼  Dataset
3. å·²åœ¨ Notebook ä¸­æ·»åŠ è¯¥ Dataset

ä½¿ç”¨æ–¹æ³•:
åœ¨ Kaggle Notebook ç¬¬ä¸€ä¸ªå•å…ƒæ ¼è¿è¡Œ:
    exec(open('/kaggle/working/adaptive_RAG/KAGGLE_LOAD_OLLAMA.py').read())
"""

import os
import subprocess
import tarfile
import shutil
import time

print("="*70)
print("ğŸ“¦ ä» Dataset åŠ è½½ Ollamaï¼ˆå¿«é€Ÿå¯åŠ¨ï¼‰")
print("="*70)

# ==================== é…ç½® ====================
# ä¿®æ”¹ä¸ºä½ çš„ Dataset åç§°
DATASET_NAME = "ollama-mistral-backup"  # ğŸ‘ˆ ä¿®æ”¹è¿™é‡Œ
DATASET_PATH = f"/kaggle/input/{DATASET_NAME}"

print(f"\nğŸ“‹ é…ç½®:")
print(f"   Dataset è·¯å¾„: {DATASET_PATH}")

# ==================== æ£€æŸ¥ Dataset ====================
print(f"\nğŸ” æ­¥éª¤ 1/5: æ£€æŸ¥ Dataset...")

if not os.path.exists(DATASET_PATH):
    print(f"   âŒ Dataset ä¸å­˜åœ¨: {DATASET_PATH}")
    print(f"\nğŸ’¡ è¯·æ£€æŸ¥:")
    print(f"   1. Dataset æ˜¯å¦å·²æ·»åŠ åˆ° Notebook")
    print(f"   2. Dataset åç§°æ˜¯å¦æ­£ç¡®")
    print(f"   3. å¯ç”¨çš„ Datasets:")
    
    if os.path.exists("/kaggle/input"):
        for item in os.listdir("/kaggle/input"):
            print(f"      â€¢ {item}")
    
    print(f"\nğŸ“ å¦‚ä½•æ·»åŠ  Dataset:")
    print(f"   1. ç‚¹å‡»å³ä¾§ 'Add data' æŒ‰é’®")
    print(f"   2. é€‰æ‹© 'Your Datasets'")
    print(f"   3. æ‰¾åˆ°ä½ çš„ ollama å¤‡ä»½ Dataset")
    print(f"   4. ç‚¹å‡» 'Add'")
    
    exit(1)

print(f"   âœ… Dataset å­˜åœ¨")

# åˆ—å‡º Dataset å†…å®¹
print(f"\n   Dataset å†…å®¹:")
for item in os.listdir(DATASET_PATH):
    item_path = os.path.join(DATASET_PATH, item)
    if os.path.isfile(item_path):
        size = os.path.getsize(item_path)
        size_str = f"{size / (1024**3):.2f} GB" if size > 1024**3 else f"{size / (1024**2):.2f} MB"
        print(f"      â€¢ {item}: {size_str}")

# ==================== å®‰è£… Ollama äºŒè¿›åˆ¶æ–‡ä»¶ ====================
print(f"\nğŸ”§ æ­¥éª¤ 2/5: å®‰è£… Ollama äºŒè¿›åˆ¶æ–‡ä»¶...")

ollama_bin_source = os.path.join(DATASET_PATH, "ollama")

if os.path.exists(ollama_bin_source):
    # å¤åˆ¶åˆ°ç³»ç»Ÿè·¯å¾„
    ollama_bin_dest = "/usr/local/bin/ollama"
    shutil.copy2(ollama_bin_source, ollama_bin_dest)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod(ollama_bin_dest, 0o755)
    
    print(f"   âœ… Ollama å·²å®‰è£…åˆ°: {ollama_bin_dest}")
    
    # éªŒè¯ç‰ˆæœ¬
    version_result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
    if version_result.returncode == 0:
        print(f"   ğŸ“Œ {version_result.stdout.strip()}")
else:
    print(f"   âŒ æœªæ‰¾åˆ° Ollama äºŒè¿›åˆ¶æ–‡ä»¶")
    exit(1)

# ==================== è§£å‹æ¨¡å‹æ–‡ä»¶ ====================
print(f"\nğŸ“¦ æ­¥éª¤ 3/5: è§£å‹æ¨¡å‹æ–‡ä»¶...")

models_archive = os.path.join(DATASET_PATH, "ollama_models.tar.gz")

if os.path.exists(models_archive):
    print(f"   æ‰¾åˆ°æ¨¡å‹å‹ç¼©åŒ…: {os.path.getsize(models_archive) / (1024**3):.2f} GB")
    print(f"   ğŸ“¦ å¼€å§‹è§£å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ 10-30 ç§’ï¼‰...")
    
    start_time = time.time()
    
    # è§£å‹åˆ°ç”¨æˆ·ç›®å½•ï¼ˆæ¢å¤åˆ° ~/.ollamaï¼‰
    ollama_home = os.path.expanduser("~")
    
    with tarfile.open(models_archive, 'r:gz') as tar:
        tar.extractall(ollama_home)  # ä¼šè‡ªåŠ¨åˆ›å»º ~/.ollama ç›®å½•
    
    elapsed = time.time() - start_time
    print(f"   âœ… è§£å‹å®Œæˆï¼ˆè€—æ—¶: {int(elapsed)}ç§’ï¼‰")
    
    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    models_dir = os.path.join(ollama_home, ".ollama")
    if os.path.exists(models_dir):
        total_size = sum(
            os.path.getsize(os.path.join(dirpath, filename))
            for dirpath, dirnames, filenames in os.walk(models_dir)
            for filename in filenames
        )
        print(f"   ğŸ“Š æ¨¡å‹æ€»å¤§å°: {total_size / (1024**3):.2f} GB")
else:
    print(f"   âŒ æœªæ‰¾åˆ°æ¨¡å‹å‹ç¼©åŒ…")
    exit(1)

# ==================== å¯åŠ¨ Ollama æœåŠ¡ ====================
print(f"\nğŸš€ æ­¥éª¤ 4/5: å¯åŠ¨ Ollama æœåŠ¡...")

# æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
ps_check = subprocess.run(['pgrep', '-f', 'ollama serve'], capture_output=True)

if ps_check.returncode == 0:
    print(f"   âœ… Ollama æœåŠ¡å·²åœ¨è¿è¡Œ")
else:
    print(f"   ğŸ”„ å¯åŠ¨æœåŠ¡...")
    subprocess.Popen(
        ['ollama', 'serve'],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    print(f"   â³ ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼ˆ15ç§’ï¼‰...")
    time.sleep(15)
    
    # éªŒè¯æœåŠ¡
    import requests
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=10)
        if response.status_code == 200:
            print(f"   âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
    except Exception as e:
        print(f"   âš ï¸ æœåŠ¡éªŒè¯å¤±è´¥: {e}")
        print(f"   ä½†å¯èƒ½ä»åœ¨å¯åŠ¨ä¸­...")

# ==================== éªŒè¯æ¨¡å‹ ====================
print(f"\nâœ… æ­¥éª¤ 5/5: éªŒè¯æ¨¡å‹...")

list_result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
print(f"\n   å¯ç”¨æ¨¡å‹:")
print(f"   {list_result.stdout}")

# ==================== å®Œæˆ ====================
print("="*70)
print("âœ… Ollama åŠ è½½å®Œæˆï¼")
print("="*70)

print(f"\nğŸ“Š åŠ è½½æ€»ç»“:")
print(f"   â€¢ Ollama æœåŠ¡: âœ… è¿è¡Œä¸­")
print(f"   â€¢ æ¨¡å‹: âœ… å·²åŠ è½½")
print(f"   â€¢ æ€»è€—æ—¶: < 1 åˆ†é’Ÿ")

print(f"\nğŸ’¡ å¯¹æ¯”:")
print(f"   â€¢ ä¼ ç»Ÿæ–¹å¼: 5-10 åˆ†é’Ÿï¼ˆé‡æ–°ä¸‹è½½ï¼‰")
print(f"   â€¢ Dataset æ–¹å¼: < 1 åˆ†é’Ÿï¼ˆç›´æ¥åŠ è½½ï¼‰")
print(f"   â€¢ èŠ‚çœæ—¶é—´: çº¦ 90%ï¼")

print(f"\nğŸ§ª å¿«é€Ÿæµ‹è¯•:")
print(f"   åœ¨æ–°å•å…ƒæ ¼è¿è¡Œ:")
print(f"   !ollama run mistral 'Hi, respond in one word'")

print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
print(f"   ç»§ç»­è¿è¡Œä½ çš„ GraphRAG ç´¢å¼•:")
print(f"""
   from document_processor import DocumentProcessor
   from graph_indexer import GraphRAGIndexer
   
   processor = DocumentProcessor()
   vectorstore, retriever, doc_splits = processor.setup_knowledge_base(enable_graphrag=True)
   
   indexer = GraphRAGIndexer(async_batch_size=8)
   graph = indexer.index_documents(doc_splits)
""")

print("\n" + "="*70)
