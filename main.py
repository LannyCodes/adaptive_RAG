"""
ä¸»åº”ç”¨ç¨‹åºå…¥å£
é›†æˆæ‰€æœ‰æ¨¡å—ï¼Œæ„å»ºå·¥ä½œæµå¹¶è¿è¡Œè‡ªé€‚åº”RAGç³»ç»Ÿ
"""

import time
from langgraph.graph import END, StateGraph, START
from pprint import pprint

from config import setup_environment, validate_api_keys, ENABLE_GRAPHRAG
from document_processor import initialize_document_processor
from routers_and_graders import initialize_graders_and_router
from workflow_nodes import WorkflowNodes, GraphState

# æ·»åŠ  LangSmith é›†æˆ
from langsmith_integration import setup_langsmith
from langsmith_integration import (
    AlertLevel,
    AlertRule
)
from typing import Optional
from dataclasses import dataclass, field
import asyncio
from concurrent.futures import ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore', category=UserWarning)

try:
    from knowledge_graph import initialize_knowledge_graph, initialize_community_summarizer
    from graph_retriever import initialize_graph_retriever
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥çŸ¥è¯†å›¾è°±æ¨¡å—ï¼ŒGraphRAGåŠŸèƒ½å°†ä¸å¯ç”¨")
    ENABLE_GRAPHRAG = False


class AdaptiveRAGSystem:
    """è‡ªé€‚åº”RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        print("åˆå§‹åŒ–è‡ªé€‚åº”RAGç³»ç»Ÿ...")
        
        # è®¾ç½® LangSmith è¿½è¸ªå’Œæ€§èƒ½ç›‘æ§
        print("è®¾ç½® LangSmith è¿½è¸ª...")
        self.langsmith_manager = setup_langsmith(
            project_name="adaptive-rag-project",
            enable_performance_monitoring=True,
            enable_alerts=True
        )
        
        # åˆå§‹åŒ–å‘Šè­¦å›è°ƒå‡½æ•°
        self._setup_alert_callbacks()
        
        # è®¾ç½®ç¯å¢ƒå’ŒéªŒè¯APIå¯†é’¥
        try:
            setup_environment()
            validate_api_keys()  # éªŒè¯APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®
            print("âœ… APIå¯†é’¥éªŒè¯æˆåŠŸ")
        except ValueError as e:
            print(f"âŒ {e}")
            raise
        
        from config import LLM_BACKEND
        if LLM_BACKEND == "ollama":
            print("ğŸ” æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
            if not self._check_ollama_service():
                print("\n" + "="*60)
                print("âŒ Ollama æœåŠ¡æœªå¯åŠ¨ï¼")
                print("="*60)
                print("\nè¯·å…ˆå¯åŠ¨ Ollama æœåŠ¡ï¼š")
                print("\næ–¹æ³•1: åœ¨ç»ˆç«¯è¿è¡Œ")
                print("  $ ollama serve")
                print("\næ–¹æ³•2: åœ¨ Kaggle Notebook ä¸­è¿è¡Œ")
                print("  import subprocess")
                print("  subprocess.Popen(['ollama', 'serve'])")
                print("\næ–¹æ³•3: ä½¿ç”¨å¿«æ·è„šæœ¬")
                print("  %run KAGGLE_LOAD_OLLAMA.py")
                print("="*60)
                raise ConnectionError("Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
            print("âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        print("è®¾ç½®æ–‡æ¡£å¤„ç†å™¨...")
        self.doc_processor, self.vectorstore, self.retriever, self.doc_splits = initialize_document_processor()
        
        # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œè·¯ç”±å™¨
        print("åˆå§‹åŒ–è¯„åˆ†å™¨å’Œè·¯ç”±å™¨...")
        self.graders = initialize_graders_and_router()
        
        # åˆå§‹åŒ–çŸ¥è¯†å›¾è°± (å¦‚æœå¯ç”¨)
        self.graph_retriever = None
        if ENABLE_GRAPHRAG:
            print("åˆå§‹åŒ– GraphRAG...")
            try:
                kg = initialize_knowledge_graph()
                # å°è¯•åŠ è½½å·²æœ‰çš„å›¾è°±æ•°æ®
                try:
                    kg.load_from_file("knowledge_graph.json")
                except FileNotFoundError:
                    print("   æœªæ‰¾åˆ° existing knowledge_graph.json, å°†ä½¿ç”¨ç©ºå›¾è°±")
                
                self.graph_retriever = initialize_graph_retriever(kg)
                print("âœ… GraphRAG åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ GraphRAG åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–å·¥ä½œæµèŠ‚ç‚¹
        print("è®¾ç½®å·¥ä½œæµèŠ‚ç‚¹...")
        # WorkflowNodes å°†åœ¨ _build_workflow ä¸­åˆå§‹åŒ–
        
        # æ„å»ºå·¥ä½œæµ
        print("æ„å»ºå·¥ä½œæµå›¾...")
        self.app = self._build_workflow()
        
        print("âœ… è‡ªé€‚åº”RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def _setup_alert_callbacks(self):
        """è®¾ç½®å‘Šè­¦å›è°ƒå‡½æ•°"""
        def alert_callback(rule, metric_value):
            """é»˜è®¤å‘Šè­¦å›è°ƒï¼šè®°å½•åˆ°æ§åˆ¶å°"""
            print(f"\nğŸ”” [å‘Šè­¦é€šçŸ¥] {rule.name}\n"
                  f"   çº§åˆ«: {rule.level.value}\n"
                  f"   æŒ‡æ ‡: {rule.metric_name}\n"
                  f"   å½“å‰å€¼: {metric_value:.2f}\n"
                  f"   é˜ˆå€¼: {rule.operator} {rule.threshold}")
        
        self.langsmith_manager.add_alert_callback(alert_callback)
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ"""
        import requests
        try:
            # å°è¯•è¿æ¥ Ollama API
            response = requests.get('http://localhost:11434/api/tags', timeout=2)
            return response.status_code == 200
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):
            return False
    
    def _build_workflow(self):
        """æ„å»ºå·¥ä½œæµå›¾"""
        # åˆ›å»ºå·¥ä½œæµèŠ‚ç‚¹å®ä¾‹ï¼Œä¼ é€’DocumentProcessorå®ä¾‹å’Œretriever
        self.workflow_nodes = WorkflowNodes(
            doc_processor=self.doc_processor,
            graders=self.graders,
            retriever=self.retriever
        )
        
        workflow = StateGraph(GraphState)
        
        # å®šä¹‰èŠ‚ç‚¹
        workflow.add_node("web_search", self.workflow_nodes.web_search)
        workflow.add_node("retrieve", self.workflow_nodes.retrieve)
        workflow.add_node("grade_documents", self.workflow_nodes.grade_documents)
        workflow.add_node("generate", self.workflow_nodes.generate)
        workflow.add_node("transform_query", self.workflow_nodes.transform_query)
        workflow.add_node("decompose_query", self.workflow_nodes.decompose_query)
        workflow.add_node("prepare_next_query", self.workflow_nodes.prepare_next_query)
        
        # æ„å»ºå›¾
        workflow.add_conditional_edges(
            START,
            self.workflow_nodes.route_question,
            {
                "web_search": "web_search",
                "vectorstore": "decompose_query", # å‘é‡æ£€ç´¢å‰å…ˆè¿›è¡ŒæŸ¥è¯¢åˆ†è§£
            },
        )
        workflow.add_edge("web_search", "generate")
        workflow.add_edge("decompose_query", "retrieve")
        workflow.add_edge("retrieve", "grade_documents")
        workflow.add_conditional_edges(
            "grade_documents",
            self.workflow_nodes.decide_to_generate,
            {
                "transform_query": "transform_query",
                "prepare_next_query": "prepare_next_query",
                "generate": "generate",
                "web_search": "web_search", # æ·»åŠ  web_search ä½œä¸ºå›é€€é€‰é¡¹
            },
        )
        workflow.add_edge("transform_query", "retrieve")
        workflow.add_edge("prepare_next_query", "retrieve")
        workflow.add_conditional_edges(
            "generate",
            self.workflow_nodes.grade_generation_v_documents_and_question,
            {
                "not supported": "transform_query",  # ä¿®å¤ï¼šæœ‰å¹»è§‰æ—¶é‡æ–°è½¬æ¢æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯å†æ¬¡ç”Ÿæˆ
                "useful": END,
                "not useful": "transform_query",
            },
        )
        
        # ç¼–è¯‘ï¼ˆè®¾ç½®é€’å½’é™åˆ¶ä»¥é˜²æ­¢æ— é™å¾ªç¯ï¼‰
        # è·å– LangSmith å›è°ƒé…ç½®
        callback_config = self.langsmith_manager.get_callback_config()
        
        return workflow.compile(
            checkpointer=None,
            interrupt_before=None,
            interrupt_after=None,
            debug=False,
            **callback_config  # æ·»åŠ  LangSmith å›è°ƒ
        )
    
    async def query(self, question: str, verbose: bool = True):
        """
        å¤„ç†æŸ¥è¯¢ (å¼‚æ­¥ç‰ˆæœ¬)
        
        Args:
            question (str): ç”¨æˆ·é—®é¢˜
            verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
            
        Returns:
            dict: åŒ…å«æœ€ç»ˆç­”æ¡ˆå’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        import asyncio
        from datetime import datetime
        
        print(f"\nğŸ” å¤„ç†é—®é¢˜: {question}")
        print("=" * 50)
        
        # è®°å½•æŸ¥è¯¢å¼€å§‹æ—¶é—´
        query_start_time = datetime.now()
        
        inputs = {"question": question, "retry_count": 0}  # åˆå§‹åŒ–é‡è¯•è®¡æ•°å™¨
        final_generation = None
        retrieval_metrics = None
        routing_decision = "unknown"
        
        # è®¾ç½®é…ç½®ï¼Œå¢åŠ é€’å½’é™åˆ¶
        config = {"recursion_limit": 50}  # å¢åŠ åˆ° 50ï¼Œé»˜è®¤æ˜¯ 25
        
        print("\nğŸ¤– æ€è€ƒè¿‡ç¨‹:")
        async for output in self.app.astream(inputs, config=config):
            for key, value in output.items():
                if verbose:
                    # ç®€å•çš„èŠ‚ç‚¹æ‰§è¡Œæç¤ºï¼Œæ¨¡æ‹Ÿæµå¼æ„Ÿ
                    print(f"  â†³ æ‰§è¡ŒèŠ‚ç‚¹: {key}...", end="\r")
                    # å¼‚æ­¥æš‚åœ
                    await asyncio.sleep(0.1) 
                    print(f"  âœ… å®ŒæˆèŠ‚ç‚¹: {key}      ")
                    
                # è®°å½•è·¯ç”±å†³ç­–
                if key == "start":
                    routing_decision = value.get("next_node", "unknown")
                
                final_generation = value.get("generation", final_generation)
                # ä¿å­˜æ£€ç´¢è¯„ä¼°æŒ‡æ ‡
                if "retrieval_metrics" in value:
                    retrieval_metrics = value["retrieval_metrics"]
                    
                    # ä½¿ç”¨ LangSmith è®°å½•æ£€ç´¢äº‹ä»¶
                    if hasattr(self, 'langsmith_manager') and self.langsmith_manager.enable_performance_monitoring:
                        self.langsmith_manager.log_retrieval_event(
                            query=question,
                            documents_count=retrieval_metrics.get('retrieved_docs_count', 0),
                            retrieval_time=retrieval_metrics.get('latency', 0) * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                            top_k=3
                        )
                
                # è®°å½•ç”Ÿæˆäº‹ä»¶
                if key == "generate":
                    generation = value.get("generation", "")
                    if generation and hasattr(self, 'langsmith_manager') and self.langsmith_manager.enable_performance_monitoring:
                        # ä¼°ç®—tokenä½¿ç”¨é‡ï¼ˆä¸­æ–‡çº¦2å­—ç¬¦=1tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼‰
                        estimated_tokens = len(generation) // 2
                        
                        self.langsmith_manager.log_generation_event(
                            prompt=question,
                            generation=generation,
                            generation_time=0,  # ç”Ÿæˆæ—¶é—´å·²åœ¨generateèŠ‚ç‚¹ä¸­å¤„ç†
                            tokens_used=estimated_tokens
                        )
        
        print("\n" + "=" * 50)
        print("ğŸ¯ æœ€ç»ˆç­”æ¡ˆ:")
        print("-" * 30)
        
        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ•ˆæœ (æ‰“å­—æœºæ•ˆæœ)
        if final_generation:
            import sys
            for char in final_generation:
                sys.stdout.write(char)
                sys.stdout.flush()
                # å¼‚æ­¥æš‚åœ
                await asyncio.sleep(0.01) # æ§åˆ¶æ‰“å­—é€Ÿåº¦
            print() # æ¢è¡Œ
        else:
            print("æœªç”Ÿæˆç­”æ¡ˆ")
            
        print("=" * 50)
        
        # è®¡ç®—æ€»æŸ¥è¯¢æ—¶é—´å¹¶è®°å½•åˆ° LangSmith
        query_end_time = datetime.now()
        total_latency = (query_end_time - query_start_time).total_seconds() * 1000  # æ¯«ç§’
        
        if hasattr(self, 'langsmith_manager') and self.langsmith_manager.enable_performance_monitoring:
            self.langsmith_manager.log_query_complete(
                question=question,
                answer=final_generation or "",
                total_latency=total_latency,
                routing_decision=routing_decision,
                metrics=retrieval_metrics
            )
        
        # è¿”å›åŒ…å«ç­”æ¡ˆå’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        return {
            "answer": final_generation,
            "retrieval_metrics": retrieval_metrics
        }
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼ï¼Œå…è®¸ç”¨æˆ·æŒç»­æé—®"""
        import asyncio
        print("\nğŸ¤– æ¬¢è¿ä½¿ç”¨è‡ªé€‚åº”RAGç³»ç»Ÿ!")
        print("ğŸ’¡ è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("-" * 50)
        
        while True:
            try:
                question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§!")
                    break
                
                if not question:
                    print("âš ï¸  è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„é—®é¢˜")
                    continue
                
                # ä½¿ç”¨ asyncio.run æ‰§è¡Œå¼‚æ­¥æŸ¥è¯¢
                result = asyncio.run(self.query(question))
                
                # æ˜¾ç¤ºæ£€ç´¢è¯„ä¼°æ‘˜è¦
                if result.get("retrieval_metrics"):
                    metrics = result["retrieval_metrics"]
                    print("\nğŸ“Š æ£€ç´¢è¯„ä¼°æ‘˜è¦:")
                    print(f"   - æ£€ç´¢è€—æ—¶: {metrics.get('latency', 0):.4f}ç§’")
                    print(f"   - æ£€ç´¢æ–‡æ¡£æ•°: {metrics.get('retrieved_docs_count', 0)}")
                    print(f"   - Precision@3: {metrics.get('precision_at_3', 0):.4f}")
                    print(f"   - Recall@3: {metrics.get('recall_at_3', 0):.4f}")
                    print(f"   - MAP: {metrics.get('map_score', 0):.4f}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§!")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")


def main():
    """ä¸»å‡½æ•°"""
    import asyncio
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system: AdaptiveRAGSystem = AdaptiveRAGSystem()
        
        # æµ‹è¯•æŸ¥è¯¢ - åŸºäºLilian Wengçš„ä¸‰ç¯‡åšå®¢ç”Ÿæˆçš„10ä¸ªé—®é¢˜
        test_questions = [
            "AI Agentçš„å››ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä»€ä¹ˆæ˜¯Chain-of-Thought (CoT) æç¤ºæŠ€æœ¯ï¼Ÿ",
            "å¤§è¯­è¨€æ¨¡å‹é¢ä¸´å“ªäº›ç±»å‹çš„å¯¹æŠ—æ”»å‡»ï¼Ÿ",
            "AI Agentä¸­çš„è®°å¿†ç³»ç»Ÿåˆ†ä¸ºå“ªä¸¤ç§ç±»å‹ï¼Ÿ",
            "å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹æ¥å¼•å¯¼LLMçš„è¡Œä¸ºï¼Ÿ",
            "å¯¹æŠ—æ€§æ”»å‡»å¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Ÿ",
            "AI Agentçš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æç¤ºå·¥ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡æç¤ºï¼Ÿ",
            "å¦‚ä½•æå‡LLMé¢å¯¹å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§ï¼Ÿ",
            "AI Agentçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ"
        ]
        
        # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
        print("\nğŸ” æ£€æŸ¥ç¡¬ä»¶åŠ é€Ÿé…ç½®...")
        print("=" * 60)
        
        # æ£€æŸ¥CUDA/GPU
        try:
            import torch
            print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
            if torch.cuda.is_available():
                print(f"CUDAå¯ç”¨: âœ…")
                print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
                print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
                for i in range(torch.cuda.device_count()):
                    print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            else:
                print(f"CUDAå¯ç”¨: âŒ")
                print(f"ä½¿ç”¨è®¾å¤‡: CPU")
        except ImportError:
            print("âš ï¸ æœªå®‰è£…PyTorchï¼Œæ— æ³•æ£€æµ‹GPU")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“é…ç½®
        print("\nğŸ“Š å‘é‡æ•°æ®åº“é…ç½®:")
        try:
            from config import VECTOR_STORE_TYPE, MILVUS_URI, MILVUS_HOST, MILVUS_PORT
            print(f"å‘é‡æ•°æ®åº“ç±»å‹: {VECTOR_STORE_TYPE}")
            
            if VECTOR_STORE_TYPE == "milvus":
                print(f"Milvus URI: {MILVUS_URI if MILVUS_URI else f'{MILVUS_HOST}:{MILVUS_PORT}'}")
                
                # æ£€æŸ¥ Milvus è¿æ¥çŠ¶æ€
                try:
                    from pymilvus import connections, utility
                    
                    # æ£€æŸ¥æ˜¯å¦å·²è¿æ¥
                    if connections.has_connection("default"):
                        print(f"Milvus è¿æ¥çŠ¶æ€: âœ… å·²è¿æ¥")
                        
                        # æ£€æŸ¥é›†åˆä¿¡æ¯
                        if utility.has_collection("rag_milvus", using="default"):
                            print(f"Milvus é›†åˆ: rag_milvus âœ…")
                        else:
                            print(f"Milvus é›†åˆ: rag_milvus âŒ (ä¸å­˜åœ¨)")
                    else:
                        print(f"Milvus è¿æ¥çŠ¶æ€: âš ï¸ æœªè¿æ¥ (å°†åœ¨æŸ¥è¯¢æ—¶è¿æ¥)")
                        
                except ImportError:
                    print("âš ï¸ æœªå®‰è£… pymilvusï¼Œæ— æ³•æ£€æµ‹ Milvus çŠ¶æ€")
                except Exception as e:
                    print(f"âš ï¸ Milvus çŠ¶æ€æ£€æµ‹å¤±è´¥: {e}")
            else:
                print(f"âš ï¸ æœªçŸ¥çš„å‘é‡æ•°æ®åº“ç±»å‹: {VECTOR_STORE_TYPE}")
                
        except ImportError:
            print("âš ï¸ æ— æ³•å¯¼å…¥é…ç½®ï¼Œè·³è¿‡å‘é‡æ•°æ®åº“æ£€æµ‹")
        
        print("=" * 60)
        
        # æµ‹è¯•å¼‚æ­¥æ£€ç´¢æ€§èƒ½ - ä½¿ç”¨çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
        print("\nğŸš€ å¼€å§‹æµ‹è¯•å¼‚æ­¥æ£€ç´¢æ€§èƒ½ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰")
        print("=" * 60)
        print(f"æµ‹è¯•é—®é¢˜æ•°é‡: {len(test_questions)}")
        print("=" * 60)
        
        import time
        start_time = time.time()
        
        # ä½¿ç”¨ asyncio.gather å®ç°çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
        async def run_concurrent_queries():
            """å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢"""
            tasks = []
            for idx, test_question in enumerate(test_questions, 1):
                # ä¸ºæ¯ä¸ªæŸ¥è¯¢åˆ›å»ºä»»åŠ¡
                task = asyncio.create_task(
                    rag_system.query(test_question, verbose=False)
                )
                tasks.append((idx, test_question, task))
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = []
            for idx, test_question, task in tasks:
                print(f"\n{'='*60}")
                print(f"æŸ¥è¯¢ {idx}/{len(test_questions)}: {test_question[:50]}...")
                print(f"{'='*60}")
                
                try:
                    result = await task
                    query_time = time.time() - start_time  # ç›¸å¯¹äºå¼€å§‹æ—¶é—´
                    results.append({
                        "question": test_question,
                        "time": query_time,
                        "metrics": result.get("retrieval_metrics")
                    })
                    print(f"âœ… å®Œæˆ - è€—æ—¶: {query_time:.4f}ç§’")
                except Exception as e:
                    print(f"âŒ å¤±è´¥: {e}")
                    results.append({
                        "question": test_question,
                        "time": 0,
                        "error": str(e),
                        "metrics": None
                    })
            
            return results
        
        # è¿è¡Œå¹¶å‘æŸ¥è¯¢
        results = asyncio.run(run_concurrent_queries())
        
        total_time = time.time() - start_time
        
        # æ˜¾ç¤ºæ€§èƒ½æµ‹è¯•æ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š å¼‚æ­¥æ£€ç´¢æ€§èƒ½æµ‹è¯•æ‘˜è¦ï¼ˆå¹¶å‘æ‰§è¡Œï¼‰")
        print("=" * 60)
        print(f"æ€»æŸ¥è¯¢æ•°: {len(test_questions)}")
        print(f"æ€»è€—æ—¶: {total_time:.4f}ç§’")
        print(f"å¹³å‡è€—æ—¶: {total_time/len(test_questions):.4f}ç§’")
        print(f"æœ€å¿«æŸ¥è¯¢: {min(r['time'] for r in results if r['time'] > 0):.4f}ç§’")
        print(f"æœ€æ…¢æŸ¥è¯¢: {max(r['time'] for r in results if r['time'] > 0):.4f}ç§’")
        
        # è®¡ç®—å¹¶å‘æ•ˆç‡
        if len(test_questions) > 1:
            # å¦‚æœæ˜¯ä¸²è¡Œæ‰§è¡Œï¼Œæ€»æ—¶é—´åº”è¯¥æ˜¯æ‰€æœ‰æŸ¥è¯¢æ—¶é—´çš„æ€»å’Œ
            serial_time = sum(r['time'] for r in results if r['time'] > 0)
            efficiency = (serial_time / total_time) * 100 if total_time > 0 else 0
            print(f"å¹¶å‘æ•ˆç‡: {efficiency:.1f}% (ç›¸æ¯”ä¸²è¡Œæ‰§è¡Œ)")
        print("=" * 60)
        
        # æ˜¾ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„è¯¦ç»†æŒ‡æ ‡
        print("\nğŸ“‹ å„æŸ¥è¯¢è¯¦ç»†æŒ‡æ ‡:")
        print("-" * 60)
        for idx, result in enumerate(results, 1):
            print(f"\næŸ¥è¯¢ {idx}: {result['question'][:50]}...")
            print(f"  è€—æ—¶: {result['time']:.4f}ç§’")
            if result.get('metrics'):
                metrics = result['metrics']
                print(f"  æ£€ç´¢æ–‡æ¡£æ•°: {metrics.get('retrieved_docs_count', 0)}")
                print(f"  Precision@3: {metrics.get('precision_at_3', 0):.4f}")
                print(f"  Recall@3: {metrics.get('recall_at_3', 0):.4f}")
                print(f"  MAP: {metrics.get('map_score', 0):.4f}")
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤º LangSmith æ€§èƒ½æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“ˆ LangSmith æ€§èƒ½æŠ¥å‘Š")
        print("=" * 60)
        
        if hasattr(rag_system, 'langsmith_manager'):
            # è·å–æ€§èƒ½æŠ¥å‘Š
            performance_report = rag_system.langsmith_manager.get_performance_report(hours=24)
            
            if "summary" in performance_report:
                summary = performance_report["summary"]
                print(f"ğŸ“Š æŸ¥è¯¢ç»Ÿè®¡ (è¿‡å»24å°æ—¶):")
                print(f"   æ€»æŸ¥è¯¢æ•°: {summary.get('total_queries', 0)}")
                print(f"   å¹³å‡å»¶è¿Ÿ: {summary.get('average_latency_ms', 0):.2f}ms")
                print(f"   æœ€å°å»¶è¿Ÿ: {summary.get('min_latency_ms', 0):.2f}ms")
                print(f"   æœ€å¤§å»¶è¿Ÿ: {summary.get('max_latency_ms', 0):.2f}ms")
                
                # æ˜¾ç¤ºè·¯ç”±åˆ†å¸ƒ
                routing_dist = summary.get('routing_distribution', {})
                if routing_dist:
                    print(f"\nğŸ”€ è·¯ç”±å†³ç­–åˆ†å¸ƒ:")
                    for decision, count in routing_dist.items():
                        print(f"   {decision}: {count}æ¬¡")
                
                # æ˜¾ç¤ºæœ€æ…¢æŸ¥è¯¢
                slowest = performance_report.get('slowest_queries', [])
                if slowest:
                    print(f"\nğŸ¢ æœ€æ…¢çš„5ä¸ªæŸ¥è¯¢:")
                    for i, query in enumerate(slowest, 1):
                        print(f"   {i}. [{query['routing']}] {query['question'][:40]}... ({query['latency_ms']:.0f}ms)")
            else:
                print("   æš‚æ— æŸ¥è¯¢æ•°æ®")
            
            # æ˜¾ç¤ºå‘Šè­¦è§„åˆ™çŠ¶æ€
            print(f"\nğŸ”” å‘Šè­¦è§„åˆ™çŠ¶æ€:")
            for rule in rag_system.langsmith_manager.alert_rules:
                status = "âœ…" if rule.enabled else "âŒ"
                print(f"   {status} {rule.name} ({rule.metric_name} {rule.operator} {rule.threshold})")
        else:
            print("   LangSmith ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        
        print("=" * 60)
        
        # å¯åŠ¨äº¤äº’æ¨¡å¼
        rag_system.interactive_mode()
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("è¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    main()