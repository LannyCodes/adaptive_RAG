"""
ä¸»åº”ç”¨ç¨‹åºå…¥å£
é›†æˆæ‰€æœ‰æ¨¡å—ï¼Œæ„å»ºå·¥ä½œæµå¹¶è¿è¡Œè‡ªé€‚åº”RAGç³»ç»Ÿ
"""

import time
from langgraph.graph import END, StateGraph, START
from pprint import pprint

from config import setup_environment, validate_api_keys, ENABLE_GRAPHRAG
from document_processor import initialize_document_processor
from routers_and_graders import initialize_graders_and_router
from workflow_nodes import WorkflowNodes, GraphState
try:
    from knowledge_graph import initialize_knowledge_graph, initialize_community_summarizer
    from graph_retriever import initialize_graph_retriever
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥çŸ¥è¯†å›¾è°±æ¨¡å—ï¼ŒGraphRAGåŠŸèƒ½å°†ä¸å¯ç”¨")
    ENABLE_GRAPHRAG = False


class AdaptiveRAGSystem:
    """è‡ªé€‚åº”RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        print("åˆå§‹åŒ–è‡ªé€‚åº”RAGç³»ç»Ÿ...")
        
        # è®¾ç½®ç¯å¢ƒå’ŒéªŒè¯APIå¯†é’¥
        try:
            setup_environment()
            validate_api_keys()  # éªŒè¯APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®
            print("âœ… APIå¯†é’¥éªŒè¯æˆåŠŸ")
        except ValueError as e:
            print(f"âŒ {e}")
            raise
        
        # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
        print("ğŸ” æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
        if not self._check_ollama_service():
            print("\n" + "="*60)
            print("âŒ Ollama æœåŠ¡æœªå¯åŠ¨ï¼")
            print("="*60)
            print("\nè¯·å…ˆå¯åŠ¨ Ollama æœåŠ¡ï¼š")
            print("\næ–¹æ³•1: åœ¨ç»ˆç«¯è¿è¡Œ")
            print("  $ ollama serve")
            print("\næ–¹æ³•2: åœ¨ Kaggle Notebook ä¸­è¿è¡Œ")
            print("  import subprocess")
            print("  subprocess.Popen(['ollama', 'serve'])")
            print("\næ–¹æ³•3: ä½¿ç”¨å¿«æ·è„šæœ¬")
            print("  %run KAGGLE_LOAD_OLLAMA.py")
            print("="*60)
            raise ConnectionError("Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        
        print("âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        print("è®¾ç½®æ–‡æ¡£å¤„ç†å™¨...")
        self.doc_processor, self.vectorstore, self.retriever, self.doc_splits = initialize_document_processor()
        
        # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œè·¯ç”±å™¨
        print("åˆå§‹åŒ–è¯„åˆ†å™¨å’Œè·¯ç”±å™¨...")
        self.graders = initialize_graders_and_router()
        
        # åˆå§‹åŒ–çŸ¥è¯†å›¾è°± (å¦‚æœå¯ç”¨)
        self.graph_retriever = None
        if ENABLE_GRAPHRAG:
            print("åˆå§‹åŒ– GraphRAG...")
            try:
                kg = initialize_knowledge_graph()
                # å°è¯•åŠ è½½å·²æœ‰çš„å›¾è°±æ•°æ®
                try:
                    kg.load_from_file("knowledge_graph.json")
                except FileNotFoundError:
                    print("   æœªæ‰¾åˆ° existing knowledge_graph.json, å°†ä½¿ç”¨ç©ºå›¾è°±")
                
                self.graph_retriever = initialize_graph_retriever(kg)
                print("âœ… GraphRAG åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ GraphRAG åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–å·¥ä½œæµèŠ‚ç‚¹
        print("è®¾ç½®å·¥ä½œæµèŠ‚ç‚¹...")
        # WorkflowNodes å°†åœ¨ _build_workflow ä¸­åˆå§‹åŒ–
        
        # æ„å»ºå·¥ä½œæµ
        print("æ„å»ºå·¥ä½œæµå›¾...")
        self.app = self._build_workflow()
        
        print("âœ… è‡ªé€‚åº”RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ"""
        import requests
        try:
            # å°è¯•è¿æ¥ Ollama API
            response = requests.get('http://localhost:11434/api/tags', timeout=2)
            return response.status_code == 200
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):
            return False
    
    def _build_workflow(self):
        """æ„å»ºå·¥ä½œæµå›¾"""
        # åˆ›å»ºå·¥ä½œæµèŠ‚ç‚¹å®ä¾‹ï¼Œä¼ é€’DocumentProcessorå®ä¾‹å’Œretriever
        self.workflow_nodes = WorkflowNodes(
            doc_processor=self.doc_processor,
            graders=self.graders,
            retriever=self.retriever
        )
        
        workflow = StateGraph(GraphState)
        
        # å®šä¹‰èŠ‚ç‚¹
        workflow.add_node("web_search", self.workflow_nodes.web_search)
        workflow.add_node("retrieve", self.workflow_nodes.retrieve)
        workflow.add_node("grade_documents", self.workflow_nodes.grade_documents)
        workflow.add_node("generate", self.workflow_nodes.generate)
        workflow.add_node("transform_query", self.workflow_nodes.transform_query)
        workflow.add_node("decompose_query", self.workflow_nodes.decompose_query)
        workflow.add_node("prepare_next_query", self.workflow_nodes.prepare_next_query)
        
        # æ„å»ºå›¾
        workflow.add_conditional_edges(
            START,
            self.workflow_nodes.route_question,
            {
                "web_search": "web_search",
                "vectorstore": "decompose_query", # å‘é‡æ£€ç´¢å‰å…ˆè¿›è¡ŒæŸ¥è¯¢åˆ†è§£
            },
        )
        workflow.add_edge("web_search", "generate")
        workflow.add_edge("decompose_query", "retrieve")
        workflow.add_edge("retrieve", "grade_documents")
        workflow.add_conditional_edges(
            "grade_documents",
            self.workflow_nodes.decide_to_generate,
            {
                "transform_query": "transform_query",
                "prepare_next_query": "prepare_next_query",
                "generate": "generate",
            },
        )
        workflow.add_edge("transform_query", "retrieve")
        workflow.add_edge("prepare_next_query", "retrieve")
        workflow.add_conditional_edges(
            "generate",
            self.workflow_nodes.grade_generation_v_documents_and_question,
            {
                "not supported": "transform_query",  # ä¿®å¤ï¼šæœ‰å¹»è§‰æ—¶é‡æ–°è½¬æ¢æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯å†æ¬¡ç”Ÿæˆ
                "useful": END,
                "not useful": "transform_query",
            },
        )
        
        # ç¼–è¯‘ï¼ˆè®¾ç½®é€’å½’é™åˆ¶ä»¥é˜²æ­¢æ— é™å¾ªç¯ï¼‰
        return workflow.compile(
            checkpointer=None,
            interrupt_before=None,
            interrupt_after=None,
            debug=False
        )
    
    async def query(self, question: str, verbose: bool = True):
        """
        å¤„ç†æŸ¥è¯¢ (å¼‚æ­¥ç‰ˆæœ¬)
        
        Args:
            question (str): ç”¨æˆ·é—®é¢˜
            verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
            
        Returns:
            dict: åŒ…å«æœ€ç»ˆç­”æ¡ˆå’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        import asyncio
        print(f"\nğŸ” å¤„ç†é—®é¢˜: {question}")
        print("=" * 50)
        
        inputs = {"question": question, "retry_count": 0}  # åˆå§‹åŒ–é‡è¯•è®¡æ•°å™¨
        final_generation = None
        retrieval_metrics = None
        
        # è®¾ç½®é…ç½®ï¼Œå¢åŠ é€’å½’é™åˆ¶
        config = {"recursion_limit": 50}  # å¢åŠ åˆ° 50ï¼Œé»˜è®¤æ˜¯ 25
        
        print("\nğŸ¤– æ€è€ƒè¿‡ç¨‹:")
        async for output in self.app.astream(inputs, config=config):
            for key, value in output.items():
                if verbose:
                    # ç®€å•çš„èŠ‚ç‚¹æ‰§è¡Œæç¤ºï¼Œæ¨¡æ‹Ÿæµå¼æ„Ÿ
                    print(f"  â†³ æ‰§è¡ŒèŠ‚ç‚¹: {key}...", end="\r")
                    # å¼‚æ­¥æš‚åœ
                    await asyncio.sleep(0.1) 
                    print(f"  âœ… å®ŒæˆèŠ‚ç‚¹: {key}      ")
                    
                final_generation = value.get("generation", final_generation)
                # ä¿å­˜æ£€ç´¢è¯„ä¼°æŒ‡æ ‡
                if "retrieval_metrics" in value:
                    retrieval_metrics = value["retrieval_metrics"]
        
        print("\n" + "=" * 50)
        print("ğŸ¯ æœ€ç»ˆç­”æ¡ˆ:")
        print("-" * 30)
        
        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ•ˆæœ (æ‰“å­—æœºæ•ˆæœ)
        if final_generation:
            import sys
            for char in final_generation:
                sys.stdout.write(char)
                sys.stdout.flush()
                # å¼‚æ­¥æš‚åœ
                await asyncio.sleep(0.01) # æ§åˆ¶æ‰“å­—é€Ÿåº¦
            print() # æ¢è¡Œ
        else:
            print("æœªç”Ÿæˆç­”æ¡ˆ")
            
        print("=" * 50)
        
        # è¿”å›åŒ…å«ç­”æ¡ˆå’Œè¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        return {
            "answer": final_generation,
            "retrieval_metrics": retrieval_metrics
        }
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼ï¼Œå…è®¸ç”¨æˆ·æŒç»­æé—®"""
        import asyncio
        print("\nğŸ¤– æ¬¢è¿ä½¿ç”¨è‡ªé€‚åº”RAGç³»ç»Ÿ!")
        print("ğŸ’¡ è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("-" * 50)
        
        while True:
            try:
                question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§!")
                    break
                
                if not question:
                    print("âš ï¸  è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„é—®é¢˜")
                    continue
                
                # ä½¿ç”¨ asyncio.run æ‰§è¡Œå¼‚æ­¥æŸ¥è¯¢
                result = asyncio.run(self.query(question))
                
                # æ˜¾ç¤ºæ£€ç´¢è¯„ä¼°æ‘˜è¦
                if result.get("retrieval_metrics"):
                    metrics = result["retrieval_metrics"]
                    print("\nğŸ“Š æ£€ç´¢è¯„ä¼°æ‘˜è¦:")
                    print(f"   - æ£€ç´¢è€—æ—¶: {metrics.get('latency', 0):.4f}ç§’")
                    print(f"   - æ£€ç´¢æ–‡æ¡£æ•°: {metrics.get('retrieved_docs_count', 0)}")
                    print(f"   - Precision@3: {metrics.get('precision_at_3', 0):.4f}")
                    print(f"   - Recall@3: {metrics.get('recall_at_3', 0):.4f}")
                    print(f"   - MAP: {metrics.get('map_score', 0):.4f}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§!")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")


def main():
    """ä¸»å‡½æ•°"""
    import asyncio
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system: AdaptiveRAGSystem = AdaptiveRAGSystem()
        
        # æµ‹è¯•æŸ¥è¯¢
        # test_question = "AlphaCodiumè®ºæ–‡è®²çš„æ˜¯ä»€ä¹ˆï¼Ÿ"
        test_question = "LangGraphçš„ä½œè€…ç›®å‰åœ¨å“ªå®¶å…¬å¸å·¥ä½œï¼Ÿ"
        # test_question = "è§£é‡ŠembeddingåµŒå…¥çš„åŸç†ï¼Œæœ€å¥½åˆ—ä¸¾å®ç°è¿‡ç¨‹çš„å…·ä½“æ­¥éª¤"
        
        # ä½¿ç”¨ asyncio.run æ‰§è¡Œå¼‚æ­¥æŸ¥è¯¢
        result = asyncio.run(rag_system.query(test_question))
        
        # æ˜¾ç¤ºæµ‹è¯•æŸ¥è¯¢çš„æ£€ç´¢è¯„ä¼°æ‘˜è¦
        if result.get("retrieval_metrics"):
            metrics = result["retrieval_metrics"]
            print("\nğŸ“Š æµ‹è¯•æŸ¥è¯¢æ£€ç´¢è¯„ä¼°æ‘˜è¦:")
            print(f"   - æ£€ç´¢è€—æ—¶: {metrics.get('latency', 0):.4f}ç§’")
            print(f"   - æ£€ç´¢æ–‡æ¡£æ•°: {metrics.get('retrieved_docs_count', 0)}")
            print(f"   - Precision@3: {metrics.get('precision_at_3', 0):.4f}")
            print(f"   - Recall@3: {metrics.get('recall_at_3', 0):.4f}")
            print(f"   - MAP: {metrics.get('map_score', 0):.4f}")
        
        # å¯åŠ¨äº¤äº’æ¨¡å¼
        rag_system.interactive_mode()
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("è¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    main()