#!/usr/bin/env python3
"""
Google Colab GPUæ£€æµ‹å’ŒGraphRAGæ€§èƒ½æµ‹è¯•è„šæœ¬
å¯ä»¥ç›´æ¥åœ¨Colabä¸­è¿è¡Œï¼špython colab_gpu_test.py
"""

import sys
import time
import torch
import numpy as np
from typing import List, Dict

def print_section(title: str):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"{title}")
    print("="*60 + "\n")


def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print_section("ğŸ” GPUç¯å¢ƒæ£€æµ‹")
    
    cuda_available = torch.cuda.is_available()
    print(f"âœ… CUDAå¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   æ€»æ˜¾å­˜: {total_memory:.2f} GB")
        
        return True
    else:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPU")
        print("   åœ¨Colabä¸­å¯ç”¨GPU: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU")
        return False


def benchmark_matrix_multiplication(matrix_size=5000):
    """GPU vs CPU çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•"""
    print_section("âš¡ GPU vs CPU çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•")
    
    print(f"çŸ©é˜µå¤§å°: {matrix_size}x{matrix_size}\n")
    
    # CPUæµ‹è¯•
    print("ğŸ”µ CPUæµ‹è¯•...")
    a_cpu = torch.randn(matrix_size, matrix_size)
    b_cpu = torch.randn(matrix_size, matrix_size)
    
    start = time.time()
    c_cpu = torch.mm(a_cpu, b_cpu)
    cpu_time = time.time() - start
    print(f"   CPUæ—¶é—´: {cpu_time:.2f} ç§’")
    
    # GPUæµ‹è¯•
    if torch.cuda.is_available():
        print("\nğŸŸ¢ GPUæµ‹è¯•...")
        a_gpu = torch.randn(matrix_size, matrix_size).cuda()
        b_gpu = torch.randn(matrix_size, matrix_size).cuda()
        
        # é¢„çƒ­GPU
        _ = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        
        start = time.time()
        c_gpu = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        print(f"   GPUæ—¶é—´: {gpu_time:.2f} ç§’")
        
        speedup = cpu_time / gpu_time
        print(f"\nğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"   GPUæ¯”CPUå¿« {speedup:.1f} å€!")
        
        return speedup
    else:
        print("\nâš ï¸  è·³è¿‡GPUæµ‹è¯•ï¼ˆGPUä¸å¯ç”¨ï¼‰")
        return 1.0


def test_text_embedding_performance():
    """æµ‹è¯•æ–‡æœ¬åµŒå…¥æ€§èƒ½ï¼ˆéœ€è¦sentence-transformersï¼‰"""
    print_section("ğŸ“ æ–‡æœ¬åµŒå…¥æ€§èƒ½æµ‹è¯•")
    
    try:
        from sentence_transformers import SentenceTransformer
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_texts = [
            "Large Language Models are transforming AI",
            "GraphRAG combines knowledge graphs with retrieval",
            "GPU acceleration significantly improves performance",
            "Natural language processing is advancing rapidly",
        ] * 250  # 1000ä¸ªæ–‡æœ¬
        
        print(f"æµ‹è¯•æ•°æ®: {len(test_texts)} ä¸ªæ–‡æœ¬\n")
        
        # CPUæµ‹è¯•
        print("ğŸ”µ CPUåµŒå…¥æµ‹è¯•...")
        model_cpu = SentenceTransformer(
            'sentence-transformers/all-MiniLM-L6-v2',
            device='cpu'
        )
        start = time.time()
        embeddings_cpu = model_cpu.encode(test_texts, show_progress_bar=False, batch_size=32)
        cpu_time = time.time() - start
        print(f"   CPUæ—¶é—´: {cpu_time:.2f}ç§’")
        print(f"   é€Ÿåº¦: {len(test_texts)/cpu_time:.1f} æ–‡æœ¬/ç§’")
        
        # GPUæµ‹è¯•
        if torch.cuda.is_available():
            print("\nğŸŸ¢ GPUåµŒå…¥æµ‹è¯•...")
            model_gpu = SentenceTransformer(
                'sentence-transformers/all-MiniLM-L6-v2',
                device='cuda'
            )
            start = time.time()
            embeddings_gpu = model_gpu.encode(test_texts, show_progress_bar=False, batch_size=32)
            gpu_time = time.time() - start
            print(f"   GPUæ—¶é—´: {gpu_time:.2f}ç§’")
            print(f"   é€Ÿåº¦: {len(test_texts)/gpu_time:.1f} æ–‡æœ¬/ç§’")
            
            speedup = cpu_time / gpu_time
            print(f"\nğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"   èŠ‚çœæ—¶é—´: {cpu_time - gpu_time:.2f}ç§’")
            
            return speedup
        else:
            print("\nâš ï¸  è·³è¿‡GPUæµ‹è¯•")
            return 1.0
            
    except ImportError:
        print("âš ï¸  sentence-transformersæœªå®‰è£…")
        print("   å®‰è£…: pip install sentence-transformers")
        return None


def monitor_gpu_memory():
    """ç›‘æ§GPUæ˜¾å­˜ä½¿ç”¨"""
    if not torch.cuda.is_available():
        return
    
    print_section("ğŸ’¾ GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ")
    
    allocated = torch.cuda.memory_allocated(0) / (1024**3)
    reserved = torch.cuda.memory_reserved(0) / (1024**3)
    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"å·²åˆ†é…: {allocated:.2f} GB")
    print(f"å·²ä¿ç•™: {reserved:.2f} GB")
    print(f"æ€»æ˜¾å­˜: {total:.2f} GB")
    print(f"ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")


def generate_performance_report(matrix_speedup, embedding_speedup):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    print_section("ğŸ“ˆ æ€§èƒ½æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    
    print("ğŸ–¥ï¸  ç¡¬ä»¶ä¿¡æ¯:")
    if torch.cuda.is_available():
        print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
    else:
        print("   âš ï¸  GPUä¸å¯ç”¨")
    
    print(f"\n   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    
    print("\nâš¡ æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"   çŸ©é˜µè¿ç®—åŠ é€Ÿ: {matrix_speedup:.2f}x")
    if embedding_speedup:
        print(f"   æ–‡æœ¬åµŒå…¥åŠ é€Ÿ: {embedding_speedup:.2f}x")
    
    print("\nğŸ’¡ å»ºè®®:")
    if torch.cuda.is_available():
        print("   âœ… GPUè¿è¡Œè‰¯å¥½ï¼")
        print("   âœ… å»ºè®®åœ¨Colabä¸Šè¿è¡Œå®Œæ•´çš„GraphRAGç´¢å¼•æ„å»º")
        print("   âœ… é¢„è®¡ç´¢å¼•æ„å»ºæ—¶é—´å°†ç¼©çŸ­ 3-5 å€")
        
        # ä¼°ç®—æ—¶é—´èŠ‚çœ
        if embedding_speedup and embedding_speedup > 1:
            print(f"\nâ±ï¸  æ—¶é—´èŠ‚çœä¼°ç®—:")
            print(f"   100æ–‡æ¡£CPUè€—æ—¶: ~15åˆ†é’Ÿ")
            print(f"   100æ–‡æ¡£GPUè€—æ—¶: ~{15/embedding_speedup:.1f}åˆ†é’Ÿ")
            print(f"   èŠ‚çœ: ~{15 - 15/embedding_speedup:.1f}åˆ†é’Ÿ")
    else:
        print("   âš ï¸  å»ºè®®å¯ç”¨GPUä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        print("   âš ï¸  Colabå¯ç”¨GPU: è¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU")


def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–ï¼ˆä»…åœ¨Colabä¸­ï¼‰"""
    try:
        import google.colab
        is_colab = True
    except:
        is_colab = False
    
    if is_colab:
        print_section("ğŸ“¦ å®‰è£…ä¾èµ–")
        print("æ£€æµ‹åˆ°Colabç¯å¢ƒï¼Œå®‰è£…å¿…è¦çš„åŒ…...\n")
        
        import subprocess
        packages = [
            'sentence-transformers',
            'networkx',
            'python-louvain',
        ]
        
        for package in packages:
            try:
                __import__(package.replace('-', '_'))
                print(f"âœ… {package} å·²å®‰è£…")
            except ImportError:
                print(f"ğŸ“¥ å®‰è£… {package}...")
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])
                print(f"âœ… {package} å®‰è£…å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ Google Colab GPUæ£€æµ‹å’ŒGraphRAGæ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Colabä¸­è¿è¡Œ
    try:
        import google.colab
        print("\nâœ… è¿è¡Œç¯å¢ƒ: Google Colab")
    except:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°Colabç¯å¢ƒ")
        print("   æœ¬è„šæœ¬ä¸“ä¸ºGoogle Colabè®¾è®¡")
    
    # å®‰è£…ä¾èµ–
    install_dependencies()
    
    # 1. GPUæ£€æµ‹
    gpu_available = test_gpu_availability()
    
    # 2. çŸ©é˜µè¿ç®—æ€§èƒ½æµ‹è¯•
    matrix_speedup = benchmark_matrix_multiplication(matrix_size=5000)
    
    # 3. æ–‡æœ¬åµŒå…¥æ€§èƒ½æµ‹è¯•
    embedding_speedup = test_text_embedding_performance()
    
    # 4. æ˜¾å­˜ç›‘æ§
    if gpu_available:
        monitor_gpu_memory()
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    generate_performance_report(matrix_speedup, embedding_speedup)
    
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("="*60)
    
    print("\nğŸ“š ä¸‹ä¸€æ­¥:")
    print("   1. å¦‚æœGPUæµ‹è¯•æˆåŠŸï¼Œå¯ä»¥ä¸Šä¼ å®Œæ•´çš„adaptive_RAGé¡¹ç›®")
    print("   2. è¿è¡Œ main_graphrag.py è¿›è¡Œå®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»º")
    print("   3. äº«å—GPUå¸¦æ¥çš„3-5å€é€Ÿåº¦æå‡!")


if __name__ == "__main__":
    main()
