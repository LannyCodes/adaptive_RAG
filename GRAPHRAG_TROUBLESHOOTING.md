# GraphRAG æ•…éšœæ’é™¤æŒ‡å—

## é—®é¢˜ï¼šå¤„ç†æ‰¹æ¬¡æ—¶å¡ä½ä¸åŠ¨

### ç—‡çŠ¶
- å¤„ç†åˆ°ç¬¬6ä¸ªæ‰¹æ¬¡æ—¶ï¼Œå®ä½“æå–åç¨‹åºå¡ä½
- æ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼Œåªæ˜¯åœæ­¢å“åº”
- CPU/GPUä½¿ç”¨ç‡ä¸‹é™åˆ°0

### æ ¹æœ¬åŸå› 

#### 1. **LLMè¶…æ—¶é—®é¢˜** â±ï¸
- **åŸå› **: OllamaæœåŠ¡å¯èƒ½åœ¨å¤„ç†å¤æ‚è¯·æ±‚æ—¶è¶…æ—¶
- **è¡¨ç°**: è¯·æ±‚æŒ‚èµ·ï¼Œæ²¡æœ‰å“åº”ä¹Ÿæ²¡æœ‰é”™è¯¯
- **è§£å†³æ–¹æ¡ˆ**: å·²æ·»åŠ timeoutå‚æ•°å’Œé‡è¯•æœºåˆ¶

#### 2. **å†…å­˜æ³„æ¼** ğŸ’¾
- **åŸå› **: å¤šæ¬¡LLMè°ƒç”¨åï¼ŒOllamaå¯èƒ½ç§¯ç´¯å†…å­˜
- **è¡¨ç°**: å“åº”å˜æ…¢ï¼Œæœ€ç»ˆå®Œå…¨åœæ­¢
- **è§£å†³æ–¹æ¡ˆ**: 
  ```bash
  # é‡å¯OllamaæœåŠ¡
  pkill ollama
  ollama serve
  ```

#### 3. **è¿æ¥æ± è€—å°½** ğŸ”Œ
- **åŸå› **: å¤ªå¤šå¹¶å‘è¯·æ±‚ï¼Œæ²¡æœ‰æ­£ç¡®å…³é—­è¿æ¥
- **è¡¨ç°**: æ–°è¯·æ±‚æ— æ³•å»ºç«‹è¿æ¥
- **è§£å†³æ–¹æ¡ˆ**: å·²æ·»åŠ é‡è¯•å»¶è¿Ÿå’Œå¼‚å¸¸å¤„ç†

#### 4. **æ–‡æ¡£å†…å®¹è¿‡é•¿** ğŸ“„
- **åŸå› **: æŸäº›æ–‡æ¡£chunkå¯èƒ½è¶…è¿‡LLMçš„ä¸Šä¸‹æ–‡çª—å£
- **è¡¨ç°**: LLMé™é»˜å¤±è´¥
- **è§£å†³æ–¹æ¡ˆ**: å·²é™åˆ¶ä¸º2000å­—ç¬¦

## å·²å®æ–½çš„ä¿®å¤

### 1. æ·»åŠ è¶…æ—¶æ§åˆ¶
```python
EntityExtractor(timeout=60, max_retries=3)
```
- æ¯æ¬¡LLMè°ƒç”¨æœ€å¤š60ç§’è¶…æ—¶
- å¤±è´¥åæœ€å¤šé‡è¯•3æ¬¡
- é‡è¯•é—´éš”é€’å¢ï¼ˆ2s, 4s, 6sï¼‰

### 2. æ”¹è¿›çš„é”™è¯¯å¤„ç†
```python
try:
    result = extractor.extract_from_document(...)
except Exception as e:
    print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
    extraction_results.append({"entities": [], "relations": []})
```
- æ•è·æ‰€æœ‰å¼‚å¸¸
- æ·»åŠ ç©ºç»“æœè€Œä¸æ˜¯å´©æºƒ
- ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡æ¡£

### 3. è¯¦ç»†çš„è¿›åº¦æ—¥å¿—
```
âš™ï¸  === æ‰¹æ¬¡ 6/10 (æ–‡æ¡£ 51-60) ===
ğŸ” æ–‡æ¡£ #51: å¼€å§‹æå–...
   ğŸ”„ æå–å®ä½“ (å°è¯• 1/3)... âœ… æå–åˆ° 5 ä¸ªå®ä½“
   ğŸ”„ æå–å…³ç³» (å°è¯• 1/3)... âœ… æå–åˆ° 3 ä¸ªå…³ç³»
ğŸ“Š æ–‡æ¡£ #51 å®Œæˆ: 5 å®ä½“, 3 å…³ç³»
```

## æ•…éšœæ’é™¤æ­¥éª¤

### æ­¥éª¤ 1: æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
```bash
# æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
ps aux | grep ollama

# æŸ¥çœ‹Ollamaæ—¥å¿—
tail -f ~/.ollama/logs/server.log

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
ollama list
```

### æ­¥éª¤ 2: æ£€æŸ¥ç³»ç»Ÿèµ„æº
```bash
# å†…å­˜ä½¿ç”¨
free -h  # Linux
top      # æŸ¥çœ‹Ollamaè¿›ç¨‹

# åœ¨Colabä¸­
!nvidia-smi  # GPUå†…å­˜
!ps aux | grep ollama
```

### æ­¥éª¤ 3: å‡å°æ‰¹æ¬¡å¤§å°
```python
# åœ¨ main_graphrag.py æˆ–è°ƒç”¨ä»£ç ä¸­
graph = indexer.index_documents(
    documents=doc_splits,
    batch_size=5,  # ä»10é™åˆ°5
    save_path="./knowledge_graph.pkl"
)
```

### æ­¥éª¤ 4: æµ‹è¯•å•ä¸ªæ–‡æ¡£
```python
# æµ‹è¯•æå–å™¨æ˜¯å¦å·¥ä½œ
from entity_extractor import EntityExtractor

extractor = EntityExtractor(timeout=30, max_retries=2)
result = extractor.extract_from_document(
    "æµ‹è¯•æ–‡æœ¬...",
    doc_index=0
)
print(result)
```

### æ­¥éª¤ 5: é‡å¯OllamaæœåŠ¡
```bash
# å®Œå…¨é‡å¯Ollama
pkill -9 ollama
sleep 2
ollama serve &

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 5

# éªŒè¯æœåŠ¡
curl http://localhost:11434/api/tags
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è°ƒæ•´è¶…æ—¶å‚æ•°
```python
# å¯¹äºè¾ƒæ…¢çš„æœºå™¨æˆ–GPU
extractor = EntityExtractor(
    timeout=120,      # å¢åŠ åˆ°2åˆ†é’Ÿ
    max_retries=5     # æ›´å¤šé‡è¯•æ¬¡æ•°
)
```

### 2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
```python
# åœ¨ config.py ä¸­
LOCAL_LLM = "mistral:7b"     # é»˜è®¤
# æ”¹ä¸º
LOCAL_LLM = "llama2:7b"      # æ›´å¿«
# æˆ–
LOCAL_LLM = "phi:latest"     # æœ€å¿«ï¼Œä½†è´¨é‡è¾ƒä½
```

### 3. å¢åŠ æ‰¹æ¬¡é—´å»¶è¿Ÿ
```python
# åœ¨ graph_indexer.py ä¸­ï¼Œæ‰¹æ¬¡å¾ªç¯åæ·»åŠ 
import time
for i in range(0, len(documents), batch_size):
    # ... å¤„ç†æ‰¹æ¬¡ ...
    time.sleep(2)  # ç»™Ollama 2ç§’æ¢å¤æ—¶é—´
```

### 4. é™åˆ¶å¹¶å‘è¯·æ±‚
```python
# ä½¿ç”¨çº¿ç¨‹æ± æ§åˆ¶å¹¶å‘
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=2) as executor:
    futures = [executor.submit(extract, doc) for doc in batch]
    results = [f.result() for f in futures]
```

## åœ¨Google Colabä¸­çš„ç‰¹æ®Šé—®é¢˜

### é—®é¢˜: Colabä¼šè¯è¶…æ—¶
**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨checkpointä¿å­˜è¿›åº¦
```python
# æ¯å¤„ç†Nä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡
if batch_num % 5 == 0:
    checkpoint = {
        'extraction_results': extraction_results,
        'processed_docs': i + len(batch)
    }
    import pickle
    with open(f'/content/drive/MyDrive/checkpoint_{batch_num}.pkl', 'wb') as f:
        pickle.dump(checkpoint, f)
```

### é—®é¢˜: Ollamaå†…å­˜ä¸è¶³
**è§£å†³æ–¹æ¡ˆ**: åœ¨Colabä¸­è®¾ç½®è¾ƒå°çš„ä¸Šä¸‹æ–‡çª—å£
```python
# å¯åŠ¨Ollamaæ—¶
!OLLAMA_NUM_GPU=1 OLLAMA_MAX_LOADED_MODELS=1 ollama serve > /tmp/ollama.log 2>&1 &
```

## ç›‘æ§å’Œè°ƒè¯•

### æ·»åŠ è¯¦ç»†æ—¥å¿—
```python
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('graphrag_debug.log'),
        logging.StreamHandler()
    ]
)
```

### ä½¿ç”¨è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨
```python
import signal
from contextlib import contextmanager

@contextmanager
def timeout(seconds):
    def handler(signum, frame):
        raise TimeoutError()
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)

# ä½¿ç”¨
with timeout(60):
    result = extractor.extract_from_document(text)
```

## å¸¸è§é”™è¯¯ä¿¡æ¯

| é”™è¯¯ä¿¡æ¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|
| `Connection refused` | Ollamaæœªè¿è¡Œ | `ollama serve` |
| `Timeout` | LLMå“åº”æ…¢ | å¢åŠ timeoutå‚æ•° |
| `CUDA out of memory` | GPUå†…å­˜ä¸è¶³ | å‡å°batch_size |
| `JSON decode error` | LLMè¾“å‡ºæ ¼å¼é”™è¯¯ | æ£€æŸ¥promptæ¨¡æ¿ |
| å¡ä½æ— è¾“å‡º | LLMæŒ‚èµ· | é‡å¯Ollamaï¼Œæ·»åŠ è¶…æ—¶ |

## å¿«é€Ÿä¿®å¤æ¸…å•

âœ… **ç«‹å³å°è¯•è¿™äº›æ­¥éª¤**:

1. **é‡å¯Ollama**
   ```bash
   pkill ollama && sleep 2 && ollama serve &
   ```

2. **å‡å°æ‰¹æ¬¡å¤§å°**
   ```python
   batch_size=3  # ä»10æ”¹ä¸º3
   ```

3. **å¢åŠ è¶…æ—¶æ—¶é—´**
   ```python
   EntityExtractor(timeout=120, max_retries=5)
   ```

4. **æ£€æŸ¥ç¬¬6ä¸ªæ–‡æ¡£**
   ```python
   # å•ç‹¬å¤„ç†ç¬¬6ä¸ªæ–‡æ¡£çœ‹æ˜¯å¦æœ‰ç‰¹æ®Šé—®é¢˜
   doc_6 = documents[5]
   print(f"æ–‡æ¡£é•¿åº¦: {len(doc_6.page_content)}")
   print(f"å‰500å­—ç¬¦: {doc_6.page_content[:500]}")
   ```

5. **ä½¿ç”¨æ£€æŸ¥ç‚¹æ¢å¤**
   ```python
   # ä»ç¬¬6æ‰¹æ¬¡é‡æ–°å¼€å§‹
   start_index = 50  # è·³è¿‡å‰5æ‰¹æ¬¡
   documents_remaining = documents[start_index:]
   ```

## é¢„é˜²æªæ–½

1. **å¼€å§‹å‰éªŒè¯ç¯å¢ƒ**
   ```bash
   # æ£€æŸ¥æ‰€æœ‰ä¾èµ–
   python colab_install_deps.py
   
   # æµ‹è¯•Ollama
   ollama list
   ollama run mistral "Hello"
   ```

2. **ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•**
   ```python
   # å…ˆç”¨5ä¸ªæ–‡æ¡£æµ‹è¯•
   test_docs = doc_splits[:5]
   graph = indexer.index_documents(test_docs, batch_size=2)
   ```

3. **ç›‘æ§èµ„æºä½¿ç”¨**
   ```python
   import psutil
   print(f"å†…å­˜ä½¿ç”¨: {psutil.virtual_memory().percent}%")
   ```

## è·å–å¸®åŠ©

å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **ç³»ç»Ÿä¿¡æ¯**
   - OSç‰ˆæœ¬
   - Pythonç‰ˆæœ¬
   - Ollamaç‰ˆæœ¬
   - å¯ç”¨å†…å­˜/GPU

2. **é”™è¯¯æ—¥å¿—**
   - æœ€åä¸€æ¡æˆåŠŸçš„è¾“å‡º
   - å®Œæ•´çš„é”™è¯¯å †æ ˆ
   - Ollamaæ—¥å¿— (`~/.ollama/logs/server.log`)

3. **å¤ç°æ­¥éª¤**
   - æ–‡æ¡£æ•°é‡
   - batch_size
   - åœ¨å“ªä¸ªæ‰¹æ¬¡å¡ä½

## æ€»ç»“

**æœ€å¯èƒ½çš„åŸå› **: LLMè°ƒç”¨è¶…æ—¶æˆ–Ollamaå†…å­˜ç§¯ç´¯

**æœ€å¿«çš„è§£å†³æ–¹æ¡ˆ**:
1. é‡å¯OllamaæœåŠ¡
2. å‡å°batch_sizeåˆ°3-5
3. ä½¿ç”¨æ›´æ–°åçš„å¸¦è¶…æ—¶å’Œé‡è¯•çš„ä»£ç 

ç°åœ¨çš„ä»£ç å·²ç»åŒ…å«äº†æ‰€æœ‰è¿™äº›ä¿æŠ¤æªæ–½ï¼Œåº”è¯¥èƒ½å¤Ÿç¨³å®šè¿è¡Œï¼
