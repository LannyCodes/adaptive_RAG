"""
GraphRAGç´¢å¼•å™¨
è´Ÿè´£æ„å»ºå±‚æ¬¡åŒ–çš„çŸ¥è¯†å›¾è°±ç´¢å¼•ï¼ŒåŒ…æ‹¬å®ä½“æå–ã€å›¾è°±æ„å»ºã€ç¤¾åŒºæ£€æµ‹å’Œæ‘˜è¦ç”Ÿæˆ
"""

from typing import List, Dict, Optional
try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document

from entity_extractor import EntityExtractor, EntityDeduplicator
from knowledge_graph import KnowledgeGraph, CommunitySummarizer


class GraphRAGIndexer:
    """GraphRAGç´¢å¼•å™¨ - å®ç°Microsoft GraphRAGçš„ç´¢å¼•æµç¨‹"""
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–GraphRAGç´¢å¼•å™¨...")
        
        self.entity_extractor = EntityExtractor()
        self.entity_deduplicator = EntityDeduplicator()
        self.knowledge_graph = KnowledgeGraph()
        self.community_summarizer = CommunitySummarizer()
        
        self.indexed = False
        
        print("âœ… GraphRAGç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def index_documents(self, documents: List[Document], 
                       batch_size: int = 10,
                       save_path: Optional[str] = None) -> KnowledgeGraph:
        """
        å¯¹æ–‡æ¡£é›†åˆå»ºç«‹GraphRAGç´¢å¼•
        
        å·¥ä½œæµç¨‹ï¼ˆéµå¾ªMicrosoft GraphRAGï¼‰:
        1. æ–‡æ¡£åˆ†å—ï¼ˆå·²åœ¨document_processorä¸­å®Œæˆï¼‰
        2. å®ä½“å’Œå…³ç³»æå–
        3. å®ä½“å»é‡å’Œåˆå¹¶
        4. æ„å»ºçŸ¥è¯†å›¾è°±
        5. ç¤¾åŒºæ£€æµ‹
        6. ç”Ÿæˆç¤¾åŒºæ‘˜è¦
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            æ„å»ºå¥½çš„çŸ¥è¯†å›¾è°±
        """
        print(f"\n{'='*50}")
        print(f"ğŸ“Š å¼€å§‹GraphRAGç´¢å¼•æµç¨‹")
        print(f"   æ–‡æ¡£æ•°é‡: {len(documents)}")
        print(f"{'='*50}\n")
        
        # æ­¥éª¤1: å®ä½“å’Œå…³ç³»æå–
        print("ğŸ“ æ­¥éª¤ 1/5: å®ä½“å’Œå…³ç³»æå–")
        extraction_results = []
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i+batch_size]
            print(f"   å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}...")
            
            for doc in batch:
                result = self.entity_extractor.extract_from_document(doc.page_content)
                extraction_results.append(result)
        
        # æ­¥éª¤2: å®ä½“å»é‡
        print("\nğŸ“ æ­¥éª¤ 2/5: å®ä½“å»é‡å’Œåˆå¹¶")
        all_entities = []
        all_relations = []
        
        for result in extraction_results:
            all_entities.extend(result.get("entities", []))
            all_relations.extend(result.get("relations", []))
        
        dedup_result = self.entity_deduplicator.deduplicate_entities(all_entities)
        unique_entities = dedup_result["entities"]
        entity_mapping = dedup_result["mapping"]
        
        # æ›´æ–°å…³ç³»ä¸­çš„å®ä½“åç§°
        mapped_relations = []
        for relation in all_relations:
            source = entity_mapping.get(relation["source"], relation["source"])
            target = entity_mapping.get(relation["target"], relation["target"])
            mapped_relations.append({
                **relation,
                "source": source,
                "target": target
            })
        
        # æ­¥éª¤3: æ„å»ºçŸ¥è¯†å›¾è°±
        print("\nğŸ“ æ­¥éª¤ 3/5: æ„å»ºçŸ¥è¯†å›¾è°±")
        cleaned_results = [{
            "entities": unique_entities,
            "relations": mapped_relations
        }]
        self.knowledge_graph.build_from_extractions(cleaned_results)
        
        # æ­¥éª¤4: ç¤¾åŒºæ£€æµ‹
        print("\nğŸ“ æ­¥éª¤ 4/5: ç¤¾åŒºæ£€æµ‹")
        self.knowledge_graph.detect_communities(algorithm="louvain")
        
        # æ­¥éª¤5: ç”Ÿæˆç¤¾åŒºæ‘˜è¦
        print("\nğŸ“ æ­¥éª¤ 5/5: ç”Ÿæˆç¤¾åŒºæ‘˜è¦")
        self.community_summarizer.summarize_all_communities(self.knowledge_graph)
        
        # ä¿å­˜å›¾è°±
        if save_path:
            self.knowledge_graph.save_to_file(save_path)
        
        self.indexed = True
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*50}")
        print("âœ… GraphRAGç´¢å¼•æ„å»ºå®Œæˆ!")
        stats = self.knowledge_graph.get_statistics()
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
        print(f"   - è¾¹æ•°: {stats['num_edges']}")
        print(f"   - ç¤¾åŒºæ•°: {stats['num_communities']}")
        print(f"   - å›¾å¯†åº¦: {stats['density']:.4f}")
        print(f"\n   å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for etype, count in stats['entity_types'].items():
            print(f"     â€¢ {etype}: {count}")
        print(f"{'='*50}\n")
        
        return self.knowledge_graph
    
    def get_graph(self) -> KnowledgeGraph:
        """è·å–çŸ¥è¯†å›¾è°±"""
        if not self.indexed:
            print("âš ï¸ å›¾è°±å°šæœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        return self.knowledge_graph
    
    def load_index(self, filepath: str) -> KnowledgeGraph:
        """åŠ è½½å·²æœ‰çš„å›¾è°±ç´¢å¼•"""
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½å›¾è°±ç´¢å¼•: {filepath}")
        self.knowledge_graph.load_from_file(filepath)
        self.indexed = True
        return self.knowledge_graph


def initialize_graph_indexer():
    """åˆå§‹åŒ–GraphRAGç´¢å¼•å™¨"""
    return GraphRAGIndexer()
