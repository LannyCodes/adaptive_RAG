"""
GraphRAGç´¢å¼•å™¨
è´Ÿè´£æ„å»ºå±‚æ¬¡åŒ–çš„çŸ¥è¯†å›¾è°±ç´¢å¼•ï¼ŒåŒ…æ‹¬å®ä½“æå–ã€å›¾è°±æ„å»ºã€ç¤¾åŒºæ£€æµ‹å’Œæ‘˜è¦ç”Ÿæˆ
"""

from typing import List, Dict, Optional
import asyncio
try:
    from langchain_core.documents import Document
except ImportError:
    try:
    from langchain_core.documents import Document
except ImportError:
    try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from entity_extractor import EntityExtractor, EntityDeduplicator
from knowledge_graph import KnowledgeGraph, CommunitySummarizer


class GraphRAGIndexer:
    """GraphRAGç´¢å¼•å™¨ - å®ç°Microsoft GraphRAGçš„ç´¢å¼•æµç¨‹"""
    
    def __init__(self, enable_async: bool = True, async_batch_size: int = 8):
        """åˆå§‹åŒ–GraphRAGç´¢å¼•å™¨
        
        Args:
            enable_async: æ˜¯å¦å¯ç”¨å¼‚æ­¥å¤„ç†ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
            async_batch_size: å¼‚æ­¥å¹¶å‘æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤5ä¸ªæ–‡æ¡£å¹¶å‘ï¼‰
        """
        print("ğŸš€ åˆå§‹åŒ–GraphRAGç´¢å¼•å™¨...")
        
        self.entity_extractor = EntityExtractor(enable_async=enable_async)
        self.entity_deduplicator = EntityDeduplicator()
        self.knowledge_graph = KnowledgeGraph()
        self.community_summarizer = CommunitySummarizer()
        
        self.enable_async = enable_async
        self.async_batch_size = async_batch_size
        self.indexed = False
        
        mode = "å¼‚æ­¥æ¨¡å¼" if enable_async else "åŒæ­¥æ¨¡å¼"
        print(f"âœ… GraphRAGç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ ({mode}, å¹¶å‘æ•°={async_batch_size})")
    
    def index_documents(self, documents: List[Document], 
                       batch_size: int = 10,
                       save_path: Optional[str] = None) -> KnowledgeGraph:
        """
        å¯¹æ–‡æ¡£é›†åˆå»ºç«‹GraphRAGç´¢å¼•
        
        å·¥ä½œæµç¨‹ï¼ˆéµå¾ªMicrosoft GraphRAGï¼‰:
        1. æ–‡æ¡£åˆ†å—ï¼ˆå·²åœ¨document_processorä¸­å®Œæˆï¼‰
        2. å®ä½“å’Œå…³ç³»æå–
        3. å®ä½“å»é‡å’Œåˆå¹¶
        4. æ„å»ºçŸ¥è¯†å›¾è°±
        5. ç¤¾åŒºæ£€æµ‹
        6. ç”Ÿæˆç¤¾åŒºæ‘˜è¦
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            save_path: ä¿å­˜è·¯å¾„
            
        Returns:
            æ„å»ºå¥½çš„çŸ¥è¯†å›¾è°±
        """
        print(f"\n{'='*50}")
        print(f"ğŸ“Š å¼€å§‹GraphRAGç´¢å¼•æµç¨‹")
        print(f"   æ–‡æ¡£æ•°é‡: {len(documents)}")
        print(f"{'='*50}\n")
        
        # æ­¥éª¤1: å®ä½“å’Œå…³ç³»æå–
        print("ğŸ“ æ­¥éª¤ 1/5: å®ä½“å’Œå…³ç³»æå–")
        extraction_results = []
        
        if self.enable_async:
            # å¼‚æ­¥æ‰¹é‡å¤„ç†æ¨¡å¼
            print(f"ğŸš€ ä½¿ç”¨å¼‚æ­¥å¤„ç†æ¨¡å¼ï¼Œå¹¶å‘æ•°={self.async_batch_size}")
            extraction_results = self._extract_async(documents)
        else:
            # åŒæ­¥å¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            print("ğŸ”„ ä½¿ç”¨åŒæ­¥å¤„ç†æ¨¡å¼")
            total_batches = (len(documents) - 1) // batch_size + 1
            
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i+batch_size]
                batch_num = i // batch_size + 1
                print(f"\nâš™ï¸  === æ‰¹æ¬¡ {batch_num}/{total_batches} (æ–‡æ¡£ {i+1}-{min(i+batch_size, len(documents))}) ===")
                
                for idx, doc in enumerate(batch):
                    doc_global_index = i + idx
                    try:
                        result = self.entity_extractor.extract_from_document(
                            doc.page_content, 
                            doc_index=doc_global_index
                        )
                        extraction_results.append(result)
                    except Exception as e:
                        print(f"   âŒ æ–‡æ¡£ #{doc_global_index + 1} å¤„ç†å¤±è´¥: {e}")
                        # æ·»åŠ ç©ºç»“æœä»¥ä¿æŒç´¢å¼•ä¸€è‡´
                        extraction_results.append({"entities": [], "relations": []})
                
                print(f"âœ… æ‰¹æ¬¡ {batch_num}/{total_batches} å®Œæˆ")
        
        # æ­¥éª¤2: å®ä½“å»é‡
        print("\nğŸ“ æ­¥éª¤ 2/5: å®ä½“å»é‡å’Œåˆå¹¶")
        all_entities = []
        all_relations = []
        
        for result in extraction_results:
            all_entities.extend(result.get("entities", []))
            all_relations.extend(result.get("relations", []))
        
        dedup_result = self.entity_deduplicator.deduplicate_entities(all_entities)
        unique_entities = dedup_result["entities"]
        entity_mapping = dedup_result["mapping"]
        
        # æ›´æ–°å…³ç³»ä¸­çš„å®ä½“åç§°
        mapped_relations = []
        for relation in all_relations:
            source = entity_mapping.get(relation["source"], relation["source"])
            target = entity_mapping.get(relation["target"], relation["target"])
            mapped_relations.append({
                **relation,
                "source": source,
                "target": target
            })
        
        # æ­¥éª¤3: æ„å»ºçŸ¥è¯†å›¾è°±
        print("\nğŸ“ æ­¥éª¤ 3/5: æ„å»ºçŸ¥è¯†å›¾è°±")
        cleaned_results = [{
            "entities": unique_entities,
            "relations": mapped_relations
        }]
        self.knowledge_graph.build_from_extractions(cleaned_results)
        
        # æ­¥éª¤4: ç¤¾åŒºæ£€æµ‹
        print("\nğŸ“ æ­¥éª¤ 4/5: ç¤¾åŒºæ£€æµ‹")
        self.knowledge_graph.detect_communities(algorithm="louvain")
        
        # æ­¥éª¤5: ç”Ÿæˆç¤¾åŒºæ‘˜è¦
        print("\nğŸ“ æ­¥éª¤ 5/5: ç”Ÿæˆç¤¾åŒºæ‘˜è¦")
        self.community_summarizer.summarize_all_communities(self.knowledge_graph)
        
        # ä¿å­˜å›¾è°±
        if save_path:
            self.knowledge_graph.save_to_file(save_path)
        
        self.indexed = True
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*50}")
        print("âœ… GraphRAGç´¢å¼•æ„å»ºå®Œæˆ!")
        stats = self.knowledge_graph.get_statistics()
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
        print(f"   - è¾¹æ•°: {stats['num_edges']}")
        print(f"   - ç¤¾åŒºæ•°: {stats['num_communities']}")
        print(f"   - å›¾å¯†åº¦: {stats['density']:.4f}")
        print(f"\n   å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for etype, count in stats['entity_types'].items():
            print(f"     â€¢ {etype}: {count}")
        print(f"{'='*50}\n")
        
        return self.knowledge_graph
    
    def _extract_async(self, documents: List[Document]) -> List[Dict]:
        """å¼‚æ­¥æ‰¹é‡æå–å®ä½“å’Œå…³ç³»
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æå–ç»“æœåˆ—è¡¨
        """
        total_docs = len(documents)
        extraction_results = []
        
        # å°†æ–‡æ¡£åˆ†æˆå¤šä¸ªå¼‚æ­¥æ‰¹æ¬¡
        for i in range(0, total_docs, self.async_batch_size):
            batch_end = min(i + self.async_batch_size, total_docs)
            batch_num = i // self.async_batch_size + 1
            total_batches = (total_docs - 1) // self.async_batch_size + 1
            
            print(f"\nâš¡ === å¼‚æ­¥æ‰¹æ¬¡ {batch_num}/{total_batches} (æ–‡æ¡£ {i+1}-{batch_end}) ===")
            
            # å‡†å¤‡å¼‚æ­¥æ‰¹æ¬¡æ•°æ®
            async_batch = [
                (documents[idx].page_content, idx) 
                for idx in range(i, batch_end)
            ]
            
            # å¼‚æ­¥æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            try:
                batch_results = asyncio.run(
                    main=self.entity_extractor.extract_batch_async(async_batch)
                )
                extraction_results.extend(batch_results)
                print(f"âœ… å¼‚æ­¥æ‰¹æ¬¡ {batch_num}/{total_batches} å®Œæˆ")
            except Exception as e:
                print(f"âŒ å¼‚æ­¥æ‰¹æ¬¡ {batch_num} å¤±è´¥: {e}")
                # æ·»åŠ ç©ºç»“æœ
                for _ in range(len(async_batch)):
                    extraction_results.append({"entities": [], "relations": []})
        
        return extraction_results
    
    def get_graph(self) -> KnowledgeGraph:
        """è·å–çŸ¥è¯†å›¾è°±"""
        if not self.indexed:
            print("âš ï¸ å›¾è°±å°šæœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ index_documents()")
        return self.knowledge_graph
    
    def load_index(self, filepath: str) -> KnowledgeGraph:
        """åŠ è½½å·²æœ‰çš„å›¾è°±ç´¢å¼•"""
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½å›¾è°±ç´¢å¼•: {filepath}")
        self.knowledge_graph.load_from_file(filepath)
        self.indexed = True
        return self.knowledge_graph


def initialize_graph_indexer():
    """åˆå§‹åŒ–GraphRAGç´¢å¼•å™¨"""
    return GraphRAGIndexer()
