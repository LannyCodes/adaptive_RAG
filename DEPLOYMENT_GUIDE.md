# Linux GPUéƒ¨ç½²æŒ‡å— (RTX 4090)

## ðŸš€ è‡ªé€‚åº”RAGç³»ç»Ÿåœ¨Linux RTX 4090çŽ¯å¢ƒéƒ¨ç½²

æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨é…å¤‡NVIDIA RTX 4090 GPUçš„LinuxæœåŠ¡å™¨ä¸Šéƒ¨ç½²è‡ªé€‚åº”RAGç³»ç»Ÿã€‚

## ðŸ“‹ çŽ¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- NVIDIA RTX 4090 GPU
- è‡³å°‘16GBå†…å­˜ï¼ˆæŽ¨è32GBï¼‰
- 50GB+å¯ç”¨ç£ç›˜ç©ºé—´
- Ubuntu 20.04+ / CentOS 8+ / RHEL 8+

### è½¯ä»¶è¦æ±‚
- Linuxæ“ä½œç³»ç»Ÿï¼ˆæŽ¨èUbuntu 22.04 LTSï¼‰
- NVIDIAé©±åŠ¨ç¨‹åºï¼ˆæŽ¨è535+ï¼‰
- CUDA 12.0+
- Dockerï¼ˆå¯é€‰ä½†æŽ¨èï¼‰
- Python 3.8-3.11

## ðŸ”§ æ­¥éª¤1ï¼šç³»ç»Ÿå‡†å¤‡

### 1.1 æ›´æ–°ç³»ç»Ÿ
```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y curl wget git build-essential python3-pip python3-venv
```

### 1.2 å®‰è£…NVIDIAé©±åŠ¨å’ŒCUDA
```bash
# æ£€æŸ¥GPU
lspci | grep -i nvidia

# æ·»åŠ NVIDIAè½¯ä»¶æº
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt-get update

# å®‰è£…NVIDIAé©±åŠ¨å’ŒCUDA
sudo apt-get install -y nvidia-driver-535 cuda-12-2

# é‡å¯ç³»ç»Ÿ
sudo reboot
```

### 1.3 éªŒè¯GPUå®‰è£…
```bash
# é‡å¯åŽéªŒè¯
nvidia-smi
nvcc --version
```

## ðŸ³ æ­¥éª¤2ï¼šDockerçŽ¯å¢ƒé…ç½®ï¼ˆæŽ¨èï¼‰

### 2.1 å®‰è£…Docker
```bash
# å®‰è£…Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
```

### 2.2 å®‰è£…NVIDIA Container Toolkit
```bash
# æ·»åŠ NVIDIA Dockeræº
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# å®‰è£…nvidia-container-toolkit
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2.3 åˆ›å»ºDockerfile
```dockerfile
# åˆ›å»º Dockerfile
cat > Dockerfile << 'EOF'
FROM nvidia/cuda:12.2-devel-ubuntu22.04

# è®¾ç½®éžäº¤äº’æ¨¡å¼
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# æ›´æ–°ç³»ç»Ÿå¹¶å®‰è£…Python
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-venv \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY requirements.txt .
COPY *.py .
COPY *.md .

# å®‰è£…Pythonä¾èµ–
RUN pip3 install --no-cache-dir -r requirements.txt

# æš´éœ²ç«¯å£ï¼ˆå¦‚æžœéœ€è¦Webç•Œé¢ï¼‰
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "main.py"]
EOF
```

## ðŸ æ­¥éª¤3ï¼šPythonçŽ¯å¢ƒé…ç½®ï¼ˆç›´æŽ¥éƒ¨ç½²ï¼‰

### 3.1 åˆ›å»ºPythonè™šæ‹ŸçŽ¯å¢ƒ
```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url> adaptive_rag
cd adaptive_rag

# åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
python3 -m venv rag_env
source rag_env/bin/activate

# å‡çº§pip
pip install --upgrade pip
```

### 3.2 ä¿®æ”¹requirements.txtä»¥æ”¯æŒGPU
éœ€è¦æ›´æ–°requirements.txtä»¥ä¼˜åŒ–GPUä½¿ç”¨ï¼š

```bash
# åˆ›å»ºGPUä¼˜åŒ–çš„requirementsæ–‡ä»¶
cat > requirements_gpu.txt << 'EOF'
# æ ¸å¿ƒæ¡†æž¶
langchain>=0.1.0
langgraph>=0.0.40
langchain-community>=0.0.20
langchain-core>=0.1.0

# LLMé›†æˆ
langchain-ollama>=0.1.0

# å‘é‡æ•°æ®åº“å’ŒåµŒå…¥ï¼ˆGPUä¼˜åŒ–ç‰ˆæœ¬ï¼‰
chromadb>=0.4.0
sentence-transformers>=2.2.0
torch>=2.0.0+cu118 --index-url https://download.pytorch.org/whl/cu118
torchvision>=0.15.0+cu118 --index-url https://download.pytorch.org/whl/cu118
transformers>=4.30.0
accelerate>=0.20.0

# æ–‡æ¡£å¤„ç†
tiktoken>=0.5.0
beautifulsoup4>=4.12.0
requests>=2.31.0

# ç½‘ç»œæœç´¢
tavily-python>=0.3.0

# æ•°æ®å¤„ç†
numpy>=1.24.0,<2.0
pandas>=2.0.0

# å·¥å…·åº“
python-dotenv>=1.0.0
pydantic>=2.0.0
typing-extensions>=4.0.0

# GPUåŠ é€Ÿåº“
cupy-cuda12x>=12.0.0
faiss-gpu>=1.7.4
EOF
```

### 3.3 å®‰è£…ä¾èµ–
```bash
# å®‰è£…GPUä¼˜åŒ–ä¾èµ–
pip install -r requirements_gpu.txt
```

## ðŸ› ï¸ æ­¥éª¤4ï¼šä¿®æ”¹é…ç½®ä»¥ä¼˜åŒ–GPUä½¿ç”¨

### 4.1 æ›´æ–°document_processor.pyä»¥ä½¿ç”¨GPU
éœ€è¦ä¿®æ”¹åµŒå…¥æ¨¡åž‹é…ç½®ï¼š

```python
# åœ¨document_processor.pyä¸­ä¿®æ”¹
self.embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={'device': 'cuda'},  # ä½¿ç”¨GPU
    encode_kwargs={'normalize_embeddings': True}
)
```

### 4.2 åˆ›å»ºGPUä¼˜åŒ–é…ç½®
```python
# åˆ›å»º gpu_config.py
cat > gpu_config.py << 'EOF'
import torch
import os

# GPUé…ç½®
if torch.cuda.is_available():
    DEVICE = "cuda"
    GPU_COUNT = torch.cuda.device_count()
    GPU_NAME = torch.cuda.get_device_name(0)
    print(f"å‘çŽ° {GPU_COUNT} ä¸ªGPU: {GPU_NAME}")
    
    # è®¾ç½®CUDAä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # è®¾ç½®GPUå†…å­˜ç®¡ç†
    torch.cuda.empty_cache()
else:
    DEVICE = "cpu"
    print("æœªå‘çŽ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")

# ä¼˜åŒ–è®¾ç½®
EMBEDDING_BATCH_SIZE = 32 if DEVICE == "cuda" else 8
MAX_WORKERS = 4 if DEVICE == "cuda" else 2
EOF
```

## ðŸ¤– æ­¥éª¤5ï¼šå®‰è£…å’Œé…ç½®Ollama

### 5.1 å®‰è£…Ollama
```bash
# ä¸‹è½½å¹¶å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# æˆ–è€…ä½¿ç”¨Docker
# docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

### 5.2 ä¸‹è½½æ¨¡åž‹
```bash
# ä¸‹è½½Mistralæ¨¡åž‹
ollama pull mistral

# æˆ–è€…ä¸‹è½½æ›´å¤§çš„æ¨¡åž‹ï¼ˆå¦‚æžœGPUå†…å­˜è¶³å¤Ÿï¼‰
ollama pull llama2:13b
ollama pull codellama:34b
```

### 5.3 å¯åŠ¨OllamaæœåŠ¡
```bash
# å¯åŠ¨OllamaæœåŠ¡
ollama serve &

# éªŒè¯æœåŠ¡
curl http://localhost:11434/api/version
```

## ðŸ” æ­¥éª¤6ï¼šçŽ¯å¢ƒå˜é‡é…ç½®

### 6.1 åˆ›å»º.envæ–‡ä»¶
```bash
cat > .env << 'EOF'
# APIå¯†é’¥
TAVILY_API_KEY=your_tavily_api_key_here

# GPUé…ç½®
CUDA_VISIBLE_DEVICES=0
TORCH_CUDA_ARCH_LIST="8.9"  # RTX 4090æž¶æž„

# æ¨¡åž‹é…ç½®
HF_HOME=/app/models
TRANSFORMERS_CACHE=/app/models

# æ€§èƒ½ä¼˜åŒ–
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8
EOF
```

## ðŸš€ æ­¥éª¤7ï¼šéƒ¨ç½²å’Œå¯åŠ¨

### 7.1 ä½¿ç”¨Dockeréƒ¨ç½²
```bash
# æž„å»ºé•œåƒ
docker build -t adaptive-rag:gpu .

# è¿è¡Œå®¹å™¨
docker run -d \
  --gpus all \
  --name adaptive-rag \
  --env-file .env \
  -p 8000:8000 \
  -v $(pwd)/data:/app/data \
  adaptive-rag:gpu
```

### 7.2 ç›´æŽ¥Pythonéƒ¨ç½²
```bash
# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source rag_env/bin/activate

# å¯åŠ¨ç³»ç»Ÿ
python main.py
```

## ðŸ“Š æ­¥éª¤8ï¼šæ€§èƒ½ç›‘æŽ§

### 8.1 åˆ›å»ºç›‘æŽ§è„šæœ¬
```bash
cat > monitor_gpu.py << 'EOF'
import psutil
import GPUtil
import time

def monitor_system():
    while True:
        # GPUç›‘æŽ§
        gpus = GPUtil.getGPUs()
        for gpu in gpus:
            print(f"GPU {gpu.id}: {gpu.load*100}% | å†…å­˜: {gpu.memoryUsed}MB/{gpu.memoryTotal}MB")
        
        # CPUå’Œå†…å­˜ç›‘æŽ§
        print(f"CPU: {psutil.cpu_percent()}% | å†…å­˜: {psutil.virtual_memory().percent}%")
        print("-" * 50)
        time.sleep(5)

if __name__ == "__main__":
    monitor_system()
EOF

pip install gputil
python monitor_gpu.py
```

## ðŸ”§ æ­¥éª¤9ï¼šæ€§èƒ½ä¼˜åŒ–é…ç½®

### 9.1 åˆ›å»ºä¼˜åŒ–å¯åŠ¨è„šæœ¬
```bash
cat > start_optimized.sh << 'EOF'
#!/bin/bash

# è®¾ç½®GPUä¼˜åŒ–çŽ¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export TOKENIZERS_PARALLELISM=false

# å¯åŠ¨ç³»ç»Ÿ
source rag_env/bin/activate
python main.py
EOF

chmod +x start_optimized.sh
```

### 9.2 åˆ›å»ºç³»ç»ŸæœåŠ¡
```bash
# åˆ›å»ºsystemdæœåŠ¡
sudo tee /etc/systemd/system/adaptive-rag.service > /dev/null << 'EOF'
[Unit]
Description=Adaptive RAG System
After=network.target

[Service]
Type=simple
User=your_username
WorkingDirectory=/path/to/adaptive_rag
Environment=PATH=/path/to/adaptive_rag/rag_env/bin
ExecStart=/path/to/adaptive_rag/rag_env/bin/python main.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# å¯ç”¨æœåŠ¡
sudo systemctl daemon-reload
sudo systemctl enable adaptive-rag
sudo systemctl start adaptive-rag
```

## ðŸ› æ­¥éª¤10ï¼šæ•…éšœæŽ’é™¤

### 10.1 å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
```bash
# å‡å°‘æ‰¹å¤„ç†å¤§å°
export EMBEDDING_BATCH_SIZE=16
# æˆ–è€…å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
```

2. **Ollamaè¿žæŽ¥é—®é¢˜**
```bash
# æ£€æŸ¥OllamaçŠ¶æ€
sudo systemctl status ollama
# é‡å¯Ollama
sudo systemctl restart ollama
```

3. **æƒé™é—®é¢˜**
```bash
# æ·»åŠ ç”¨æˆ·åˆ°dockerç»„
sudo usermod -aG docker $USER
# é‡æ–°ç™»å½•
```

### 10.2 æ€§èƒ½è°ƒä¼˜

```bash
# GPUæ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1
sudo nvidia-smi -ac 9251,2100

# ç³»ç»Ÿä¼˜åŒ–
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

## ðŸ“ˆ é¢„æœŸæ€§èƒ½

åœ¨RTX 4090çŽ¯å¢ƒä¸‹çš„é¢„æœŸæ€§èƒ½ï¼š
- **æ–‡æ¡£åµŒå…¥**: ~1000 documents/second
- **æŸ¥è¯¢å“åº”**: ~2-5 seconds per query
- **GPUåˆ©ç”¨çŽ‡**: 60-80%
- **å†…å­˜ä½¿ç”¨**: 8-12GB GPU memory

## ðŸŽ¯ éªŒè¯éƒ¨ç½²

```bash
# æµ‹è¯•GPUå¯ç”¨æ€§
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}'); print(f'GPUæ•°é‡: {torch.cuda.device_count()}')"

# æµ‹è¯•ç³»ç»Ÿ
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "ä»€ä¹ˆæ˜¯LLMæ™ºèƒ½ä½“ï¼Ÿ"}'
```

è¿™ä¸ªéƒ¨ç½²æŒ‡å—æä¾›äº†å®Œæ•´çš„Linux GPUçŽ¯å¢ƒé…ç½®ï¼Œç¡®ä¿æ‚¨çš„è‡ªé€‚åº”RAGç³»ç»Ÿèƒ½å¤Ÿå……åˆ†åˆ©ç”¨RTX 4090çš„è®¡ç®—èƒ½åŠ›ã€‚