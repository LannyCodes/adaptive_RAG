# GraphRAG Colab å®Œæ•´è¿è¡ŒæŒ‡å—

## ğŸ¯ åœ¨Colabä¸­è¿è¡ŒOllamaçš„3ç§æ–¹æ³•

### æ–¹æ³•1: åå°è¿è¡ŒOllamaï¼ˆæ¨èï¼‰â­â­â­â­â­

åœ¨Colabä¸­ï¼Œæ‚¨å¯ä»¥åœ¨å•ä¸ªå•å…ƒæ ¼ä¸­åå°å¯åŠ¨Ollamaï¼Œç„¶ååœ¨å¦ä¸€ä¸ªå•å…ƒæ ¼è¿è¡ŒGraphRAGã€‚

#### æ­¥éª¤1: å®‰è£…Ollama

```bash
# å•å…ƒæ ¼1: å®‰è£…Ollama
!curl -fsSL https://ollama.com/install.sh | sh
```

#### æ­¥éª¤2: åå°å¯åŠ¨OllamaæœåŠ¡

```python
# å•å…ƒæ ¼2: åå°å¯åŠ¨Ollama
import subprocess
import time
import os

# å¯åŠ¨OllamaæœåŠ¡ï¼ˆåå°ï¼‰
ollama_process = subprocess.Popen(
    ["ollama", "serve"],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    preexec_fn=os.setpgrp
)

print("â³ ç­‰å¾…OllamaæœåŠ¡å¯åŠ¨...")
time.sleep(5)

# éªŒè¯æœåŠ¡æ˜¯å¦å¯åŠ¨
!curl -s http://localhost:11434/api/tags | head -5

print(f"âœ… OllamaæœåŠ¡å·²å¯åŠ¨ (PID: {ollama_process.pid})")
```

#### æ­¥éª¤3: ä¸‹è½½Mistralæ¨¡å‹

```bash
# å•å…ƒæ ¼3: ä¸‹è½½æ¨¡å‹
!ollama pull mistral
```

#### æ­¥éª¤4: å®‰è£…Pythonä¾èµ–

```bash
# å•å…ƒæ ¼4: å®‰è£…ä¾èµ–
!pip install -q langchain langchain-community langchain-core langgraph
!pip install -q chromadb sentence-transformers tiktoken
!pip install -q tavily-python python-dotenv networkx python-louvain
```

#### æ­¥éª¤5: é…ç½®APIå¯†é’¥

```python
# å•å…ƒæ ¼5: é…ç½®ç¯å¢ƒ
import os
from getpass import getpass

os.environ['TAVILY_API_KEY'] = getpass('è¾“å…¥TAVILY_API_KEY: ')
print("âœ… APIå¯†é’¥å·²è®¾ç½®")
```

#### æ­¥éª¤6: è¿è¡ŒGraphRAG

```python
# å•å…ƒæ ¼6: è¿è¡ŒGraphRAG
!python main_graphrag.py
```

#### æ­¥éª¤7: ä¸‹è½½ç»“æœï¼ˆå¯é€‰ï¼‰

```python
# å•å…ƒæ ¼7: ä¸‹è½½ç”Ÿæˆçš„å›¾è°±
from google.colab import files
files.download('data/knowledge_graph.json')
```

---

### æ–¹æ³•2: ä½¿ç”¨tmuxï¼ˆé«˜çº§ï¼‰â­â­â­â­

```bash
# å•å…ƒæ ¼1: å®‰è£…tmux
!apt-get install -y tmux

# å•å…ƒæ ¼2: åœ¨tmuxä¼šè¯ä¸­å¯åŠ¨Ollama
!tmux new-session -d -s ollama 'ollama serve'

# å•å…ƒæ ¼3: æ£€æŸ¥ä¼šè¯
!tmux ls

# å•å…ƒæ ¼4: ä¸‹è½½æ¨¡å‹
!ollama pull mistral

# å•å…ƒæ ¼5: è¿è¡ŒGraphRAG
!python main_graphrag.py

# å•å…ƒæ ¼6: åœæ­¢tmuxä¼šè¯ï¼ˆæ¸…ç†ï¼‰
!tmux kill-session -t ollama
```

---

### æ–¹æ³•3: ä½¿ç”¨nohupï¼ˆç®€å•ï¼‰â­â­â­

```bash
# å•å…ƒæ ¼1: åå°å¯åŠ¨Ollama
!nohup ollama serve > /tmp/ollama.log 2>&1 &

# å•å…ƒæ ¼2: ç­‰å¾…å¯åŠ¨
import time
time.sleep(5)

# å•å…ƒæ ¼3: æ£€æŸ¥æ—¥å¿—
!tail -20 /tmp/ollama.log

# å•å…ƒæ ¼4: ä¸‹è½½æ¨¡å‹
!ollama pull mistral

# å•å…ƒæ ¼5: è¿è¡ŒGraphRAG
!python main_graphrag.py

# å•å…ƒæ ¼6: åœæ­¢Ollamaï¼ˆæ¸…ç†ï¼‰
!pkill -f 'ollama serve'
```

---

## ğŸš€ ä¸€é”®è¿è¡Œè„šæœ¬ï¼ˆæœ€ç®€å•ï¼‰â­â­â­â­â­

æˆ‘å·²ç»ä¸ºæ‚¨åˆ›å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è„šæœ¬ `colab_setup_and_run.py`ï¼Œå®ƒä¼šï¼š
1. âœ… è‡ªåŠ¨å®‰è£…Ollama
2. âœ… åå°å¯åŠ¨æœåŠ¡
3. âœ… ä¸‹è½½Mistralæ¨¡å‹
4. âœ… å®‰è£…Pythonä¾èµ–
5. âœ… é…ç½®ç¯å¢ƒå˜é‡
6. âœ… è¿è¡ŒGraphRAG

### ä½¿ç”¨æ–¹æ³•:

```bash
# æ–¹æ³•A: ç›´æ¥è¿è¡Œè„šæœ¬
!python colab_setup_and_run.py

# æ–¹æ³•B: æˆ–è€…åœ¨Pythonä¸­
import subprocess
subprocess.run(["python", "colab_setup_and_run.py"])
```

---

## ğŸ“Š å®Œæ•´çš„Colab Notebookç¤ºä¾‹

åˆ›å»ºä¸€ä¸ªæ–°çš„Colabç¬”è®°æœ¬ï¼ŒæŒ‰é¡ºåºè¿è¡Œä»¥ä¸‹å•å…ƒæ ¼ï¼š

### å•å…ƒæ ¼1: ç¯å¢ƒå‡†å¤‡

```python
# æ£€æµ‹GPU
import torch
print(f"GPUå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
```

### å•å…ƒæ ¼2: å®‰è£…Ollama

```bash
%%bash
curl -fsSL https://ollama.com/install.sh | sh
echo "âœ… Ollamaå®‰è£…å®Œæˆ"
```

### å•å…ƒæ ¼3: åå°å¯åŠ¨Ollama

```python
import subprocess
import time
import os

print("ğŸ”„ å¯åŠ¨OllamaæœåŠ¡...")

# åå°å¯åŠ¨
process = subprocess.Popen(
    ["ollama", "serve"],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    preexec_fn=os.setpgrp
)

# ç­‰å¾…å¯åŠ¨
time.sleep(5)

# éªŒè¯
import requests
try:
    response = requests.get("http://localhost:11434/api/tags", timeout=3)
    if response.status_code == 200:
        print(f"âœ… OllamaæœåŠ¡è¿è¡Œæ­£å¸¸ (PID: {process.pid})")
    else:
        print("âš ï¸  æœåŠ¡å“åº”å¼‚å¸¸")
except:
    print("âš ï¸  æ— æ³•è¿æ¥æœåŠ¡ï¼Œä½†è¿›ç¨‹å·²å¯åŠ¨")

# ä¿å­˜è¿›ç¨‹IDï¼ˆé‡è¦ï¼ï¼‰
ollama_pid = process.pid
print(f"ğŸ“ ä¿å­˜çš„PID: {ollama_pid}")
```

### å•å…ƒæ ¼4: ä¸‹è½½æ¨¡å‹

```bash
%%bash
echo "ğŸ“¥ ä¸‹è½½Mistralæ¨¡å‹..."
ollama pull mistral
echo "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ"
ollama list
```

### å•å…ƒæ ¼5: ä¸Šä¼ é¡¹ç›®æ–‡ä»¶

```python
# æ–¹å¼A: ä»Google Drive
from google.colab import drive
drive.mount('/content/drive')

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
!cp -r /content/drive/MyDrive/adaptive_RAG /content/
%cd /content/adaptive_RAG

# æ–¹å¼B: æ‰‹åŠ¨ä¸Šä¼ 
# from google.colab import files
# uploaded = files.upload()
```

### å•å…ƒæ ¼6: å®‰è£…ä¾èµ–

```bash
%%bash
pip install -q -r requirements.txt
pip install -q -r requirements_graphrag.txt
echo "âœ… ä¾èµ–å®‰è£…å®Œæˆ"
```

### å•å…ƒæ ¼7: é…ç½®ç¯å¢ƒ

```python
import os
from getpass import getpass

# è®¾ç½®APIå¯†é’¥
if not os.path.exists('.env'):
    api_key = getpass('è¾“å…¥TAVILY_API_KEY: ')
    with open('.env', 'w') as f:
        f.write(f'TAVILY_API_KEY={api_key}\n')
    print("âœ… .envæ–‡ä»¶å·²åˆ›å»º")
else:
    print("âœ… ä½¿ç”¨ç°æœ‰.envæ–‡ä»¶")
```

### å•å…ƒæ ¼8: è¿è¡ŒGraphRAG

```python
# æ–¹å¼A: ç›´æ¥è¿è¡Œ
!python main_graphrag.py

# æ–¹å¼B: åœ¨Pythonä¸­è¿è¡Œï¼ˆå¯ä»¥æ•è·è¾“å‡ºï¼‰
import subprocess

result = subprocess.run(
    ["python", "main_graphrag.py"],
    capture_output=True,
    text=True
)

print(result.stdout)
if result.returncode != 0:
    print("é”™è¯¯ä¿¡æ¯:")
    print(result.stderr)
```

### å•å…ƒæ ¼9: ä¸‹è½½ç»“æœ

```python
# ä¸‹è½½ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±
from google.colab import files

if os.path.exists('data/knowledge_graph.json'):
    files.download('data/knowledge_graph.json')
    print("âœ… æ–‡ä»¶å·²ä¸‹è½½")
else:
    print("âŒ æœªæ‰¾åˆ°å›¾è°±æ–‡ä»¶")

# ä¿å­˜åˆ°Google Drive
import shutil
shutil.copy(
    'data/knowledge_graph.json',
    '/content/drive/MyDrive/graphrag_backup.json'
)
print("âœ… å·²å¤‡ä»½åˆ°Google Drive")
```

### å•å…ƒæ ¼10: æ¸…ç†ï¼ˆå¯é€‰ï¼‰

```python
# åœæ­¢OllamaæœåŠ¡
import os
import signal

try:
    os.kill(ollama_pid, signal.SIGTERM)
    print(f"âœ… OllamaæœåŠ¡å·²åœæ­¢ (PID: {ollama_pid})")
except:
    print("âš ï¸  åœæ­¢æœåŠ¡å¤±è´¥ï¼Œæ‰‹åŠ¨åœæ­¢:")
    !pkill -f 'ollama serve'
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: OllamaæœåŠ¡å¯åŠ¨åç«‹å³é€€å‡º

**A**: ä½¿ç”¨ `subprocess.Popen` è€Œä¸æ˜¯ `subprocess.run`:

```python
# âŒ é”™è¯¯æ–¹å¼
!ollama serve &  # ä¼šç«‹å³é€€å‡º

# âœ… æ­£ç¡®æ–¹å¼
import subprocess
process = subprocess.Popen(["ollama", "serve"])
```

### Q2: è¿æ¥è¢«æ‹’ç» (Connection refused)

**A**: ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨:

```python
import time
time.sleep(10)  # å¢åŠ ç­‰å¾…æ—¶é—´
```

### Q3: è¿›ç¨‹ç®¡ç†å›°éš¾

**A**: ä½¿ç”¨PIDæ–‡ä»¶:

```python
# ä¿å­˜PID
with open('/tmp/ollama.pid', 'w') as f:
    f.write(str(process.pid))

# åç»­åœæ­¢
with open('/tmp/ollama.pid', 'r') as f:
    pid = int(f.read())
os.kill(pid, signal.SIGTERM)
```

### Q4: ä¼šè¯è¶…æ—¶å¯¼è‡´æœåŠ¡åœæ­¢

**A**: å®šæœŸæ‰§è¡Œä»£ç ä¿æŒæ´»è·ƒ:

```python
import time
while True:
    print("Keep alive...")
    time.sleep(300)  # æ¯5åˆ†é’Ÿ
```

---

## ğŸ“š æ¨èçš„å®Œæ•´æµç¨‹

1. âœ… **è¿è¡Œè‡ªåŠ¨åŒ–è„šæœ¬** - `!python colab_setup_and_run.py`
2. âœ… **æˆ–æŒ‰ç…§Notebookç¤ºä¾‹** - é€æ­¥æ‰§è¡Œæ¯ä¸ªå•å…ƒæ ¼
3. âœ… **å®šæœŸä¿å­˜ç»“æœ** - åˆ°Google Drive

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å§‹ç»ˆä¿å­˜Ollamaçš„PID**: æ–¹ä¾¿åç»­ç®¡ç†
2. **ä½¿ç”¨try-finally**: ç¡®ä¿æ¸…ç†åå°è¿›ç¨‹
3. **å®šæœŸå¤‡ä»½**: ä¿å­˜ä¸­é—´ç»“æœåˆ°Drive
4. **ç›‘æ§æ˜¾å­˜**: é¿å…OOMé”™è¯¯

```python
# æœ€ä½³å®è·µç¤ºä¾‹
import subprocess
import atexit
import signal

# å¯åŠ¨Ollama
ollama_process = subprocess.Popen(["ollama", "serve"])

# æ³¨å†Œæ¸…ç†å‡½æ•°
def cleanup():
    try:
        ollama_process.terminate()
        print("âœ… Ollamaå·²åœæ­¢")
    except:
        pass

atexit.register(cleanup)

# è¿è¡Œæ‚¨çš„ä»£ç 
try:
    # ... æ‚¨çš„GraphRAGä»£ç  ...
    pass
finally:
    cleanup()
```

---

**æ¨è**: ç›´æ¥ä½¿ç”¨ `colab_setup_and_run.py` è„šæœ¬ï¼Œå®ƒå·²ç»å¤„ç†äº†æ‰€æœ‰è¿™äº›ç»†èŠ‚ï¼ğŸš€
