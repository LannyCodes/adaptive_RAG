"""
BERT Encoder 12å±‚è¯¦ç»†è§£æ
å±•ç¤º Vectara HHEM ä¸­ BERT ç¼–ç å™¨çš„æ¯ä¸€å±‚å¤„ç†è¿‡ç¨‹
ä½¿ç”¨çœŸå®è®­ç»ƒæ ·æœ¬æ¼”ç¤ºæ•°æ®æµè½¬
"""

import numpy as np

print("=" * 80)
print("BERT Encoder 12å±‚å®Œæ•´è§£æ - è”åˆç¼–ç å¹»è§‰æ£€æµ‹")
print("=" * 80)

# ============================================================================
# Part 1: è®­ç»ƒæ ·æœ¬å‡†å¤‡
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“š Part 1: è®­ç»ƒæ ·æœ¬")
print("=" * 80)

print("""
è®­ç»ƒæ ·æœ¬ï¼ˆå¹»è§‰æ£€æµ‹ï¼‰ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Documents (æ–‡æ¡£):
  "AlphaCodium æ˜¯ä¸€ç§ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›æå‡æ€§èƒ½ã€‚"

Generation (LLMç”Ÿæˆ):
  "AlphaCodium æ˜¯ Google åœ¨ 2024 å¹´å‘å¸ƒçš„ä»£ç ç”Ÿæˆå·¥å…·ã€‚"
  
Label (æ ‡ç­¾):
  Hallucinated âŒ
  
åŸå› :
  - "Google" åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰ â†’ å¹»è§‰
  - "2024 å¹´" åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰ â†’ å¹»è§‰
  - "å·¥å…·" vs "æ–¹æ³•" â†’ è¯è¯­ä¸ç²¾ç¡®
""")


# ============================================================================
# Part 2: Tokenization å’Œåˆå§‹ Embeddings
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ”§ Part 2: è¾“å…¥å‡†å¤‡ - Tokenization")
print("=" * 80)

print("""
Step 1: æ–‡æœ¬æ‹¼æ¥
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å…¥æ ¼å¼:
  [CLS] Documents [SEP] Generation [SEP]

å®é™…æ‹¼æ¥å:
  [CLS] AlphaCodium æ˜¯ä¸€ç§ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›æå‡æ€§èƒ½ã€‚
  [SEP] AlphaCodium æ˜¯ Google åœ¨ 2024 å¹´å‘å¸ƒçš„ä»£ç ç”Ÿæˆå·¥å…·ã€‚
  [SEP]


Step 2: Tokenization (BERT WordPiece åˆ†è¯)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

åˆ†è¯ç»“æœï¼ˆç®€åŒ–ï¼Œå®é™…ä¼šæ›´ç»†ï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä½ç½®   Token              Token ID    Segment ID
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0      [CLS]              101         0
1      Alpha              2945        0
2      ##Codium           3421        0
3      æ˜¯                 2003        0
4      ä¸€ç§               1037        0
5      ä»£ç                4521        0
6      ç”Ÿæˆ               3156        0
7      æ–¹æ³•               2567        0
8      ï¼Œ                110         0
9      é€šè¿‡               2134        0
10     è¿­ä»£               3789        0
11     æ”¹è¿›               2891        0
12     æå‡               4123        0
13     æ€§èƒ½               3456        0
14     ã€‚                119         0
15     [SEP]              102         0  â† ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦
16     Alpha              2945        1  â† Segment ID å˜ä¸º 1
17     ##Codium           3421        1
18     æ˜¯                 2003        1
19     Google             5678        1
20     åœ¨                 2156        1
21     2024               4532        1
22     å¹´                 3267        1
23     å‘å¸ƒ               2789        1
24     çš„                 1998        1
25     ä»£ç                4521        1
26     ç”Ÿæˆ               3156        1
27     å·¥å…·               3890        1
28     ã€‚                119         1
29     [SEP]              102         1  â† ç¬¬äºŒä¸ªåˆ†éš”ç¬¦

æ€»å…±: 30 ä¸ª tokens


Step 3: åˆå§‹ Embeddings
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BERT çš„è¾“å…¥ = Token Embedding + Segment Embedding + Position Embedding

å¯¹äºæ¯ä¸ª tokenï¼Œè·å–ä¸‰ä¸ª embedding å¹¶ç›¸åŠ ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä»¥ Token 0 "[CLS]" ä¸ºä¾‹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Token Embedding (è¯åµŒå…¥è¡¨æŸ¥è¯¢)
   Token ID: 101
   â†’ Embedding Table[101] = [0.12, -0.34, 0.56, ..., 0.78]  (768ç»´)

2. Segment Embedding (æ®µè½åµŒå…¥)
   Segment ID: 0 (å±äº Documents éƒ¨åˆ†)
   â†’ Segment Table[0] = [0.05, 0.02, -0.01, ..., 0.03]  (768ç»´)

3. Position Embedding (ä½ç½®åµŒå…¥)
   Position: 0 (ç¬¬ä¸€ä¸ªä½ç½®)
   â†’ Position Table[0] = [0.08, -0.12, 0.15, ..., -0.05]  (768ç»´)

4. ç›¸åŠ å¾—åˆ°åˆå§‹å‘é‡
   Initial Embedding[0] = Token + Segment + Position
                        = [0.12, -0.34, 0.56, ..., 0.78]
                        + [0.05,  0.02, -0.01, ..., 0.03]
                        + [0.08, -0.12, 0.15, ..., -0.05]
                        = [0.25, -0.44, 0.70, ..., 0.76]  (768ç»´)


æ‰€æœ‰ tokens çš„åˆå§‹å‘é‡çŸ©é˜µ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Hâ° = [
    [0.25, -0.44, 0.70, ..., 0.76],  â† Token 0: [CLS]
    [0.15,  0.32, -0.23, ..., 0.45],  â† Token 1: Alpha
    [0.28, -0.15,  0.41, ..., 0.52],  â† Token 2: ##Codium
    ...
    [0.19,  0.27, -0.38, ..., 0.61]   â† Token 29: [SEP]
]

å½¢çŠ¶: (30, 768)
      â†‘    â†‘
   30ä¸ªtokens  æ¯ä¸ª768ç»´
""")


# ============================================================================
# Part 3: BERT Encoder Layer è¯¦ç»†ç»“æ„
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ—ï¸  Part 3: BERT Encoder Layer ç»“æ„ï¼ˆå•å±‚è¯¦è§£ï¼‰")
print("=" * 80)

print("""
æ¯ä¸€å±‚ BERT Encoder çš„ç»“æ„:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å…¥: H^(l-1)  (ä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œå½¢çŠ¶: 30Ã—768)
è¾“å‡º: H^l      (æœ¬å±‚çš„è¾“å‡ºï¼Œå½¢çŠ¶: 30Ã—768)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BERT Encoder Layer                   â”‚
â”‚                                                        â”‚
â”‚  è¾“å…¥: H^(l-1) (30, 768)                              â”‚
â”‚    â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Sub-Layer 1: Multi-Head Self-Attention           â”‚ â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 1.1 è®¡ç®— Q, K, V                                 â”‚ â”‚
â”‚  â”‚     Q = H^(l-1) Ã— W^Q  (30Ã—768 Ã— 768Ã—768)      â”‚ â”‚
â”‚  â”‚     K = H^(l-1) Ã— W^K  (30Ã—768 Ã— 768Ã—768)      â”‚ â”‚
â”‚  â”‚     V = H^(l-1) Ã— W^V  (30Ã—768 Ã— 768Ã—768)      â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 1.2 åˆ†æˆ 12 ä¸ª Head                              â”‚ â”‚
â”‚  â”‚     æ¯ä¸ª Head: 768 / 12 = 64 ç»´                 â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 1.3 æ¯ä¸ª Head è®¡ç®— Attention                     â”‚ â”‚
â”‚  â”‚     Attention = softmax(QÃ—K^T / âˆš64) Ã— V        â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 1.4 Concat æ‰€æœ‰ Heads                           â”‚ â”‚
â”‚  â”‚     Output = Concat(headâ‚, ..., headâ‚â‚‚)        â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 1.5 çº¿æ€§å˜æ¢                                     â”‚ â”‚
â”‚  â”‚     Output = Output Ã— W^O                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Add & Norm 1                                     â”‚ â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚ H_att = LayerNorm(H^(l-1) + Attention_Output)   â”‚ â”‚
â”‚  â”‚         â†‘ æ®‹å·®è¿æ¥  â†‘ Attention è¾“å‡º             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Sub-Layer 2: Feed Forward Network                â”‚ â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 2.1 ç¬¬ä¸€å±‚å…¨è¿æ¥ + ReLU                          â”‚ â”‚
â”‚  â”‚     FFNâ‚ = ReLU(H_att Ã— Wâ‚ + bâ‚)                â”‚ â”‚
â”‚  â”‚     (30Ã—768 Ã— 768Ã—3072 = 30Ã—3072)               â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚ 2.2 ç¬¬äºŒå±‚å…¨è¿æ¥                                 â”‚ â”‚
â”‚  â”‚     FFNâ‚‚ = FFNâ‚ Ã— Wâ‚‚ + bâ‚‚                       â”‚ â”‚
â”‚  â”‚     (30Ã—3072 Ã— 3072Ã—768 = 30Ã—768)               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Add & Norm 2                                     â”‚ â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚ H^l = LayerNorm(H_att + FFNâ‚‚)                   â”‚ â”‚
â”‚  â”‚       â†‘ æ®‹å·®è¿æ¥  â†‘ FFN è¾“å‡º                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                   â”‚
â”‚  è¾“å‡º: H^l (30, 768)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®å‚æ•°:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- Hidden Size: 768
- Attention Heads: 12
- Head Dimension: 768 / 12 = 64
- Intermediate Size (FFN): 3072
- Dropout: 0.1
""")


# ============================================================================
# Part 4: Multi-Head Self-Attention è¯¦ç»†è®¡ç®—
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ” Part 4: Multi-Head Self-Attention è¯¦ç»†è®¡ç®—è¿‡ç¨‹")
print("=" * 80)

print("""
ä»¥ Layer 1 ä¸ºä¾‹ï¼Œè¯¦ç»†å±•ç¤º Attention è®¡ç®—:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å…¥: Hâ° (30, 768)  # åˆå§‹ embeddings

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 1: è®¡ç®— Q, K, V
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q = Hâ° Ã— W^Q
  = (30, 768) Ã— (768, 768)
  = (30, 768)

K = Hâ° Ã— W^K
  = (30, 768) Ã— (768, 768)
  = (30, 768)

V = Hâ° Ã— W^V
  = (30, 768) Ã— (768, 768)
  = (30, 768)

å®é™…æ•°å€¼ç¤ºä¾‹ï¼ˆåªå±•ç¤ºå‰3ä¸ªtokençš„å‰8ç»´ï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q = [
  [0.15, -0.23, 0.34, 0.12, -0.45, 0.67, 0.89, -0.12, ...],  â† [CLS]
  [0.22,  0.18, -0.31, 0.45, 0.23, -0.56, 0.34, 0.78, ...],  â† Alpha
  [0.34, -0.12, 0.45, -0.23, 0.67, 0.12, -0.89, 0.45, ...],  â† ##Codium
  ...
]

K å’Œ V ç±»ä¼¼ç»“æ„


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 2: åˆ†æˆ 12 ä¸ª Head
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å°† 768 ç»´åˆ†æˆ 12 ä»½ï¼Œæ¯ä»½ 64 ç»´ï¼š

Head 0: Q[:, 0:64],   K[:, 0:64],   V[:, 0:64]
Head 1: Q[:, 64:128], K[:, 64:128], V[:, 64:128]
...
Head 11: Q[:, 704:768], K[:, 704:768], V[:, 704:768]

ä»¥ Head 0 ä¸ºä¾‹:
  Qâ‚€ = Q[:, 0:64]  # (30, 64)
  Kâ‚€ = K[:, 0:64]  # (30, 64)
  Vâ‚€ = V[:, 0:64]  # (30, 64)


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 3: è®¡ç®— Attention Scores (Head 0)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Scores = Qâ‚€ Ã— Kâ‚€^T / âˆš64
       = (30, 64) Ã— (64, 30) / 8
       = (30, 30) / 8

ç»“æœçŸ©é˜µ Scores (30, 30):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ¯ä¸ªå…ƒç´  Scores[i][j] è¡¨ç¤º token i å¯¹ token j çš„æ³¨æ„åŠ›åˆ†æ•°

ç¤ºä¾‹ï¼ˆå‰5x5ï¼‰:
                        â†“ Key tokens
         [CLS]  Alpha  ##Cod  æ˜¯    ä¸€ç§
[CLS]    [2.3   1.5    1.8    0.9   0.7  ...]  â† Query: [CLS]
Alpha    [1.2   3.1    2.9    1.1   0.8  ...]  â† Query: Alpha
##Cod    [1.0   2.8    3.5    1.3   0.9  ...]  â† Query: ##Codium
æ˜¯       [0.8   1.2    1.4    2.1   1.5  ...]  â† Query: æ˜¯
ä¸€ç§     [0.6   0.9    1.0    1.6   2.3  ...]  â† Query: ä¸€ç§
...

è§£é‡Š:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Scores[0][0] = 2.3  â†’ [CLS] å¯¹è‡ªå·±çš„æ³¨æ„åŠ›
Scores[1][2] = 2.9  â†’ "Alpha" å¯¹ "##Codium" çš„æ³¨æ„åŠ›ï¼ˆå¾ˆé«˜ï¼Œå› ä¸ºæ˜¯åŒä¸€ä¸ªè¯ï¼‰
Scores[19][1] = 1.8 â†’ "Google"(pos 19) å¯¹ "Alpha"(pos 1) çš„æ³¨æ„åŠ›


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 4: Softmax å½’ä¸€åŒ–
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Attention_Weights = softmax(Scores, dim=-1)

å¯¹æ¯ä¸€è¡Œåš softmaxï¼ˆå’Œä¸º1ï¼‰:

ç¤ºä¾‹ï¼ˆå‰5x5ï¼Œå½’ä¸€åŒ–åï¼‰:
                        â†“ Key tokens
         [CLS]  Alpha  ##Cod  æ˜¯    ä¸€ç§   ...
[CLS]    [0.35  0.15   0.20   0.08  0.05  ...]  â† æ€»å’Œ=1.0
Alpha    [0.10  0.40   0.35   0.08  0.04  ...]  â† æ€»å’Œ=1.0
##Cod    [0.08  0.28   0.45   0.10  0.06  ...]  â† æ€»å’Œ=1.0
æ˜¯       [0.12  0.18   0.20   0.30  0.15  ...]  â† æ€»å’Œ=1.0
ä¸€ç§     [0.10  0.14   0.16   0.22  0.32  ...]  â† æ€»å’Œ=1.0
...

å…³é”®è§‚å¯Ÿ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- "Alpha" å¯¹ "##Codium" çš„æƒé‡ = 0.35ï¼ˆé«˜ï¼ï¼‰
  â†’ è¯´æ˜æ¨¡å‹å­¦ä¼šäº†å®ƒä»¬æ˜¯åŒä¸€ä¸ªè¯

- "Google" (pos 19) å¯¹ Documents ä¸­çš„ tokens æƒé‡è¾ƒä½
  â†’ å› ä¸º Documents ä¸­æ²¡æœ‰ "Google"
  â†’ è¿™ä¸ªä¿¡æ¯ä¼šè¢«ç”¨äºåˆ¤æ–­å¹»è§‰ï¼


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 5: åŠ æƒæ±‚å’Œ V
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Outputâ‚€ = Attention_Weights Ã— Vâ‚€
        = (30, 30) Ã— (30, 64)
        = (30, 64)

å¯¹äºæ¯ä¸ª token i:
  Outputâ‚€[i] = Î£â±¼ Attention_Weights[i][j] Ã— Vâ‚€[j]

ç¤ºä¾‹ï¼ˆtoken 0 "[CLS]" çš„è¾“å‡ºï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Outputâ‚€[0] = 0.35 Ã— Vâ‚€[0]  ([CLS] çš„ value)
           + 0.15 Ã— Vâ‚€[1]  (Alpha çš„ value)
           + 0.20 Ã— Vâ‚€[2]  (##Codium çš„ value)
           + 0.08 Ã— Vâ‚€[3]  (æ˜¯ çš„ value)
           + ...
           + 0.02 Ã— Vâ‚€[19] (Google çš„ value)  â† æƒé‡å¾ˆå°ï¼
           + ...

ç»“æœ: [0.23, -0.15, 0.34, ..., 0.67]  (64ç»´)

[CLS] çš„å‘é‡ç°åœ¨åŒ…å«äº†:
- ä¸»è¦: è‡ªå·±ã€Alphaã€##Codium çš„ä¿¡æ¯ï¼ˆæƒé‡å¤§ï¼‰
- å°‘é‡: Googleã€2024 çš„ä¿¡æ¯ï¼ˆæƒé‡å°ï¼‰
- è¿™ä¸ªå·®å¼‚ä¼šè¢«åç»­å±‚æ”¾å¤§ï¼Œç”¨äºæ£€æµ‹å¹»è§‰ï¼


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 6: Concat æ‰€æœ‰ 12 ä¸ª Heads
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Multi_Head_Output = Concat(Outputâ‚€, Outputâ‚, ..., Outputâ‚â‚)
                  = Concat((30,64), (30,64), ..., (30,64))
                  = (30, 768)

æ¯ä¸ª Head æ•æ‰ä¸åŒçš„æ¨¡å¼:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Head 0: è¯å†…å…³ç³» ("Alpha" â†” "##Codium")
Head 1: è¯­æ³•å…³ç³» ("æ˜¯" â†” "æ–¹æ³•")
Head 2: é•¿è·ç¦»ä¾èµ– ("AlphaCodium" â†” "æ€§èƒ½")
Head 3: æ£€æµ‹æ·»åŠ ä¿¡æ¯ ("Google" åœ¨ Documents ä¸­çš„å¯¹åº”)
...


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 7: çº¿æ€§å˜æ¢
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Attention_Output = Multi_Head_Output Ã— W^O + b^O
                 = (30, 768) Ã— (768, 768) + (768,)
                 = (30, 768)
""")


# ============================================================================
# Part 5: 12å±‚é€å±‚å¤„ç†
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ”¢ Part 5: BERT 12å±‚é€å±‚å¤„ç†è¿‡ç¨‹")
print("=" * 80)

print("""
å®Œæ•´çš„ 12 å±‚å¤„ç†æµç¨‹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¾“å…¥: Hâ° (30, 768)  # åˆå§‹ embeddings

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1                                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ è¾“å…¥: Hâ°                                                 â”‚
â”‚   â†“                                                      â”‚
â”‚ Multi-Head Self-Attention                               â”‚
â”‚   - "Alpha" attendåˆ° "##Codium" (0.35)                 â”‚
â”‚   - "Google" attendåˆ° Documents tokens (0.1-0.2)       â”‚
â”‚   â†“                                                      â”‚
â”‚ Add & Norm: H_attÂ¹ = LayerNorm(Hâ° + Attention)         â”‚
â”‚   â†“                                                      â”‚
â”‚ Feed Forward: FFN(H_attÂ¹)                               â”‚
â”‚   â†“                                                      â”‚
â”‚ Add & Norm: HÂ¹ = LayerNorm(H_attÂ¹ + FFN)               â”‚
â”‚                                                          â”‚
â”‚ è¾“å‡º: HÂ¹ (30, 768)                                      â”‚
â”‚                                                          â”‚
â”‚ å­¦åˆ°çš„æ¨¡å¼:                                              â”‚
â”‚ âœ“ åŸºæœ¬è¯è¯­å…³ç³»                                          â”‚
â”‚ âœ“ "AlphaCodium" åœ¨ä¸¤æ®µä¸­éƒ½å‡ºç°                          â”‚
â”‚ âœ“ "Google" åªåœ¨ Generation ä¸­å‡ºç° âš ï¸                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2                                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ è¾“å…¥: HÂ¹                                                 â”‚
â”‚   â†“                                                      â”‚
â”‚ Multi-Head Self-Attention                               â”‚
â”‚   - å¼€å§‹å»ºç«‹è¯­æ³•å…³ç³»                                     â”‚
â”‚   - "æ˜¯" attendåˆ° "æ–¹æ³•" å’Œ "å·¥å…·"                      â”‚
â”‚   â†“                                                      â”‚
â”‚ FFN + Residual                                          â”‚
â”‚   â†“                                                      â”‚
â”‚ è¾“å‡º: HÂ² (30, 768)                                      â”‚
â”‚                                                          â”‚
â”‚ å­¦åˆ°çš„æ¨¡å¼:                                              â”‚
â”‚ âœ“ "æ–¹æ³•" vs "å·¥å…·" çš„è¯­ä¹‰å·®å¼‚                           â”‚
â”‚ âœ“ æ—¶é—´ä¿¡æ¯: "2024 å¹´"                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3                                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ è¾“å…¥: HÂ²                                                 â”‚
â”‚   â†“                                                      â”‚
â”‚ Multi-Head Self-Attention                               â”‚
â”‚   - é•¿è·ç¦»ä¾èµ–å¼€å§‹å»ºç«‹                                   â”‚
â”‚   - [CLS] attendåˆ°å…³é”®è¯: "Google", "2024"             â”‚
â”‚   â†“                                                      â”‚
â”‚ FFN + Residual                                          â”‚
â”‚   â†“                                                      â”‚
â”‚ è¾“å‡º: HÂ³ (30, 768)                                      â”‚
â”‚                                                          â”‚
â”‚ å­¦åˆ°çš„æ¨¡å¼:                                              â”‚
â”‚ âœ“ Documents: "è¿­ä»£æ”¹è¿›" vs Generation: æ— æ­¤ä¿¡æ¯        â”‚
â”‚ âœ“ Generation: "Google" vs Documents: æ— æ­¤ä¿¡æ¯ âš ï¸       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4-6: ä¸­é—´å±‚                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ è¾“å…¥: HÂ³                                                 â”‚
â”‚   â†“                                                      â”‚
â”‚ å¤šå±‚ Self-Attention + FFN                               â”‚
â”‚   â†“                                                      â”‚
â”‚ è¾“å‡º: Hâ¶ (30, 768)                                      â”‚
â”‚                                                          â”‚
â”‚ å­¦åˆ°çš„æ¨¡å¼:                                              â”‚
â”‚ âœ“ å¤æ‚çš„è¯­ä¹‰å…³ç³»                                        â”‚
â”‚ âœ“ Documents å’Œ Generation çš„å¯¹æ¯”                        â”‚
â”‚ âœ“ è¯†åˆ«ä¸ä¸€è‡´çš„åœ°æ–¹:                                     â”‚
â”‚   - "æ–¹æ³•" vs "å·¥å…·"                                    â”‚
â”‚   - ç¼ºå¤± "Google" å’Œ "2024" çš„æ¥æº                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 7-9: æ·±å±‚æŠ½è±¡                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ è¾“å…¥: Hâ¶                                                 â”‚
â”‚   â†“                                                      â”‚
â”‚ å¤šå±‚ Self-Attention + FFN                               â”‚
â”‚   â†“                                                      â”‚
â”‚ è¾“å‡º: Hâ¹ (30, 768)                                      â”‚
â”‚                                                          â”‚
â”‚ å­¦åˆ°çš„æ¨¡å¼:                                              â”‚
â”‚ âœ“ é«˜å±‚è¯­ä¹‰ç†è§£                                          â”‚
â”‚ âœ“ [CLS] å‘é‡å¼€å§‹èšåˆåˆ¤æ–­ä¿¡æ¯:                           â”‚
â”‚   - Documents è¯´: "ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œè¿­ä»£æ”¹è¿›"              â”‚
â”‚   - Generation è¯´: "Google å‘å¸ƒçš„å·¥å…·"                  â”‚
â”‚   â†’ å‘ç°ä¸åŒ¹é…ï¼âš ï¸                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 10-12: æœ€ç»ˆå±‚ï¼ˆå†³ç­–å±‚ï¼‰                            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                          â”‚
â”‚ è¾“å…¥: Hâ¹                                                 â”‚
â”‚   â†“                                                      â”‚
â”‚ å¤šå±‚ Self-Attention + FFN                               â”‚
â”‚   â†“                                                      â”‚
â”‚ è¾“å‡º: HÂ¹Â² (30, 768)                                     â”‚
â”‚                                                          â”‚
â”‚ [CLS] å‘é‡çš„ä¿¡æ¯ï¼ˆæœ€å…³é”®ï¼‰:                              â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚
â”‚                                                          â”‚
â”‚ HÂ¹Â²[0] = [0.234, -0.567, 0.890, ..., 0.123]  (768ç»´)   â”‚
â”‚          â†‘ [CLS] token çš„æœ€ç»ˆå‘é‡                       â”‚
â”‚                                                          â”‚
â”‚ è¿™ä¸ªå‘é‡ç¼–ç äº†:                                          â”‚
â”‚ âœ“ Documents çš„å®Œæ•´ä¿¡æ¯                                  â”‚
â”‚ âœ“ Generation çš„å®Œæ•´ä¿¡æ¯                                 â”‚
â”‚ âœ“ ä¸¤è€…çš„å…³ç³»:                                           â”‚
â”‚   - æœ‰å“ªäº›ä¿¡æ¯ä¸€è‡´                                      â”‚
â”‚   - æœ‰å“ªäº›ä¿¡æ¯çŸ›ç›¾                                      â”‚
â”‚   - Generation æ·»åŠ äº†å“ªäº› Documents ä¸­æ²¡æœ‰çš„ä¿¡æ¯       â”‚
â”‚                                                          â”‚
â”‚ å…·ä½“è¯†åˆ«åˆ°çš„é—®é¢˜:                                        â”‚
â”‚ âŒ "Google" åœ¨ Documents ä¸­æ‰¾ä¸åˆ°å¯¹åº”                   â”‚
â”‚ âŒ "2024" åœ¨ Documents ä¸­æ‰¾ä¸åˆ°å¯¹åº”                     â”‚
â”‚ âš ï¸  "å·¥å…·" vs "æ–¹æ³•" è¯­ä¹‰å·®å¼‚                           â”‚
â”‚                                                          â”‚
â”‚ â†’ å‡†å¤‡è¾“å‡ºåˆ°åˆ†ç±»å¤´ï¼Œåˆ¤æ–­ä¸º "Hallucinated"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æœ€ç»ˆè¾“å‡º: HÂ¹Â² (30, 768)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

åªä½¿ç”¨ HÂ¹Â²[0]ï¼ˆ[CLS] çš„å‘é‡ï¼‰é€å…¥åˆ†ç±»å¤´:

[CLS] Vector = HÂ¹Â²[0] = [0.234, -0.567, 0.890, ..., 0.123]
                        â†“
                  åˆ†ç±»å¤´ (768 â†’ 2)
                        â†“
                Logits: [0.8, 4.2]
                         â†‘    â†‘
                    Factual Hallucinated
                        â†“
                    Softmax
                        â†“
            Probs: [0.03, 0.97]
                    â†‘     â†‘
                3%äº‹å®  97%å¹»è§‰

åˆ¤æ–­: Hallucinated âŒ (ç½®ä¿¡åº¦ 97%)
""")


# ============================================================================
# Part 6: å…³é”® Attention æ¨¡å¼å¯è§†åŒ–
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ‘ï¸  Part 6: å…³é”® Attention æ¨¡å¼å¯è§†åŒ–")
print("=" * 80)

print("""
Layer 12 çš„ Attention æƒé‡çŸ©é˜µï¼ˆç®€åŒ–ï¼Œåªæ˜¾ç¤ºå…³é”® tokensï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Query â†“        Key Tokens â†’
Tokens     [CLS] AlphaÂ¹ æ–¹æ³•  [SEP] AlphaÂ² Google 2024 å·¥å…·  [SEP]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[CLS]      [0.15  0.08  0.12  0.05  0.07  0.18   0.16  0.10  0.05]
           â†‘ä½   â†‘ä½   â†‘ä¸­   â†‘ä½   â†‘ä½   â†‘é«˜âš ï¸  â†‘é«˜âš ï¸ â†‘ä¸­   â†‘ä½
           
AlphaÂ¹     [0.05  0.30  0.08  0.03  0.25  0.04   0.03  0.05  0.02]
           â†‘ä½   â†‘é«˜âœ“  â†‘ä½   â†‘ä½   â†‘é«˜âœ“  â†‘ä½    â†‘ä½   â†‘ä½   â†‘ä½
           
æ–¹æ³•       [0.08  0.10  0.25  0.05  0.08  0.06   0.05  0.20  0.03]
           â†‘ä½   â†‘ä½   â†‘é«˜âœ“  â†‘ä½   â†‘ä½   â†‘ä½    â†‘ä½   â†‘ä¸­âš ï¸ â†‘ä½
           
Google     [0.10  0.05  0.03  0.02  0.06  0.40   0.15  0.08  0.02]
           â†‘ä¸­âš ï¸ â†‘ä½   â†‘ä½   â†‘ä½   â†‘ä½   â†‘é«˜âœ“   â†‘ä¸­   â†‘ä½   â†‘ä½
           
2024       [0.12  0.04  0.02  0.01  0.05  0.18   0.35  0.07  0.01]
           â†‘ä¸­âš ï¸ â†‘ä½   â†‘ä½   â†‘ä½   â†‘ä½   â†‘ä¸­    â†‘é«˜âœ“  â†‘ä½   â†‘ä½

å·¥å…·       [0.09  0.08  0.15  0.03  0.09  0.07   0.06  0.30  0.02]
           â†‘ä½   â†‘ä½   â†‘ä¸­âš ï¸ â†‘ä½   â†‘ä½   â†‘ä½    â†‘ä½   â†‘é«˜âœ“  â†‘ä½

å…³é”®è§‚å¯Ÿ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ æ­£å¸¸æ¨¡å¼:
  - "AlphaÂ¹" attendåˆ° "AlphaÂ²" (0.25) â† åŒä¸€å®ä½“
  - "æ–¹æ³•" attendåˆ°è‡ªå·± (0.25) â† è‡ªæ³¨æ„åŠ›
  
âš ï¸ å¹»è§‰æŒ‡ç¤º:
  - "Google" ä¸»è¦ attendåˆ°è‡ªå·± (0.40)
    â†’ åœ¨ Documents ä¸­æ‰¾ä¸åˆ°å¼ºå…³è”ï¼
    
  - "2024" ä¸»è¦ attendåˆ°è‡ªå·± (0.35)
    â†’ åœ¨ Documents ä¸­æ‰¾ä¸åˆ°å¼ºå…³è”ï¼
    
  - [CLS] attendåˆ° "Google" (0.18) å’Œ "2024" (0.16)
    â†’ [CLS] æ³¨æ„åˆ°è¿™äº›å¼‚å¸¸è¯ï¼
    
  - "å·¥å…·" å¯¹ "æ–¹æ³•" çš„ attention (0.15)
    â†’ è¯­ä¹‰ç›¸ä¼¼ä½†ä¸å®Œå…¨ä¸€è‡´

è¿™äº›æ¨¡å¼è¢«åˆ†ç±»å¤´å­¦ä¹ å¹¶ç”¨äºåˆ¤æ–­å¹»è§‰ï¼
""")


# ============================================================================
# Part 7: å‚æ•°ç»Ÿè®¡
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“Š Part 7: BERT Encoder å‚æ•°ç»Ÿè®¡")
print("=" * 80)

print("""
BERT-base å‚æ•°è¯¦ç»†ç»Ÿè®¡:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Embedding å±‚:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   - Token Embedding: 30,522 Ã— 768 = 23,440,896
   - Segment Embedding: 2 Ã— 768 = 1,536
   - Position Embedding: 512 Ã— 768 = 393,216
   å°è®¡: 23,835,648 å‚æ•°


2. æ¯ä¸ª Encoder Layer:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Multi-Head Attention:
     - W^Q: 768 Ã— 768 = 589,824
     - W^K: 768 Ã— 768 = 589,824
     - W^V: 768 Ã— 768 = 589,824
     - W^O: 768 Ã— 768 = 589,824
     - Biases: 768 Ã— 4 = 3,072
     å°è®¡: 2,362,368 å‚æ•°
   
   Feed Forward Network:
     - Wâ‚: 768 Ã— 3,072 = 2,359,296
     - bâ‚: 3,072
     - Wâ‚‚: 3,072 Ã— 768 = 2,359,296
     - bâ‚‚: 768
     å°è®¡: 4,722,432 å‚æ•°
   
   Layer Normalization (Ã—2):
     - Î³, Î²: 768 Ã— 2 Ã— 2 = 3,072
   
   æ¯å±‚æ€»è®¡: 2,362,368 + 4,722,432 + 3,072 = 7,087,872 å‚æ•°


3. 12 å±‚ Encoder:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   7,087,872 Ã— 12 = 85,054,464 å‚æ•°


4. åˆ†ç±»å¤´ï¼ˆHHEM ç‰¹æœ‰ï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   - W: 768 Ã— 2 = 1,536
   - b: 2
   å°è®¡: 1,538 å‚æ•°


æ€»å‚æ•°é‡:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
23,835,648 (Embeddings)
+ 85,054,464 (12 Layers)
+ 1,538 (Classification Head)
= 108,891,650 å‚æ•°

çº¦ 109M (ç™¾ä¸‡) å‚æ•°
æ¨¡å‹å¤§å°: 109M Ã— 4 bytes = 436 MB


å†…å­˜å ç”¨ï¼ˆæ¨ç†æ—¶ï¼‰:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- æ¨¡å‹å‚æ•°: 436 MB
- æ¿€æ´»å€¼ (batch_size=1, seq_len=30):
  æ¯å±‚: 30 Ã— 768 Ã— 4 bytes Ã— 2 (residual) = 184 KB
  12 å±‚: 184 KB Ã— 12 = 2.2 MB
- æ€»è®¡: ~438 MB (FP32)
        ~220 MB (FP16ï¼Œä½¿ç”¨åŠç²¾åº¦)
""")


# ============================================================================
# Part 8: æ€»ç»“
# ============================================================================
print("\n" + "=" * 80)
print("ğŸ“š Part 8: æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
print("=" * 80)

print("""
BERT Encoder 12å±‚è”åˆç¼–ç æ ¸å¿ƒè¦ç‚¹:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. è¾“å…¥å‡†å¤‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   [CLS] Documents [SEP] Generation [SEP]
   â†’ Tokenization (30 tokens)
   â†’ Token + Segment + Position Embeddings
   â†’ Hâ° (30, 768)

2. æ¯å±‚ç»“æ„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   H^(l-1) 
      â†“
   Multi-Head Self-Attention (12 heads)
      â†“
   Add & Norm
      â†“
   Feed Forward Network
      â†“
   Add & Norm
      â†“
   H^l

3. Multi-Head Attention å…³é”®
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Q, K, V = H Ã— W^Q, H Ã— W^K, H Ã— W^V
      â†“
   åˆ†æˆ 12 ä¸ª Head (æ¯ä¸ª 64 ç»´)
      â†“
   Attention = softmax(QÃ—K^T / âˆš64) Ã— V
      â†“
   Concat æ‰€æœ‰ Heads â†’ (768 ç»´)

4. 12å±‚é€å±‚å­¦ä¹ 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Layer 1-3:   åŸºæœ¬è¯­æ³•ã€è¯è¯­å…³ç³»
   Layer 4-6:   å¤æ‚è¯­ä¹‰ã€é•¿è·ç¦»ä¾èµ–
   Layer 7-9:   é«˜å±‚æŠ½è±¡ã€ä¸ä¸€è‡´æ£€æµ‹
   Layer 10-12: æœ€ç»ˆåˆ¤æ–­ã€ä¿¡æ¯èšåˆåˆ° [CLS]

5. å¹»è§‰æ£€æµ‹æœºåˆ¶
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   é€šè¿‡ Attention æƒé‡å‘ç°:
   âœ“ "Google" åœ¨ Documents ä¸­æ— å¼ºå…³è”
   âœ“ "2024" åœ¨ Documents ä¸­æ— å¼ºå…³è”
   âœ“ [CLS] èšåˆè¿™äº›ä¿¡æ¯
      â†“
   HÂ¹Â²[0] (768ç»´) â†’ åˆ†ç±»å¤´ (768â†’2)
      â†“
   [Factual: 0.03, Hallucinated: 0.97]
      â†“
   åˆ¤æ–­: Hallucinated âŒ

6. å…³é”®å‚æ•°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   - Hidden Size: 768
   - Layers: 12
   - Attention Heads: 12
   - Head Dimension: 64
   - FFN Size: 3072
   - Total Parameters: 109M
   - Model Size: 436 MB (FP32)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è”åˆç¼–ç çš„ä¼˜åŠ¿:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Documents å’Œ Generation å¯ä»¥äº’ç›¸ attend
âœ… æ¨¡å‹èƒ½æ•æ‰ä¸¤è€…ä¹‹é—´çš„ä¸€è‡´æ€§/çŸ›ç›¾
âœ… [CLS] å‘é‡èšåˆäº†å…¨å±€åˆ¤æ–­ä¿¡æ¯
âœ… 12 å±‚é€å±‚æ·±åŒ–ç†è§£ï¼Œæœ€ç»ˆå‡†ç¡®åˆ¤æ–­å¹»è§‰

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ BERT Cross-Encoder åœ¨å¹»è§‰æ£€æµ‹ä¸Šè¡¨ç°ä¼˜ç§€ï¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")

print("\n" + "=" * 80)
print("âœ… BERT Encoder 12å±‚è¯¦ç»†è§£æå®Œæ¯•ï¼")
print("=" * 80)
print()
